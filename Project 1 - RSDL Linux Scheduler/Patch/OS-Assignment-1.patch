From b8dc1c8a387729bd6c5d37f5ae6c16d84ed514a7 Mon Sep 17 00:00:00 2001
From: Milin <milinbhade@iisc.ac.in>
Date: Sun, 29 Jan 2023 23:13:30 +0530
Subject: [PATCH 1/3] OS Assignment-0

Signed-off-by: Milin <milinbhade@iisc.ac.in>
---
 Makefile                               |  3 ++-
 arch/x86/entry/syscalls/syscall_64.tbl |  1 +
 include/linux/syscalls.h               |  3 +++
 isolate/Makefile                       |  1 +
 isolate/isolate.c                      | 30 ++++++++++++++++++++++++++
 5 files changed, 37 insertions(+), 1 deletion(-)
 create mode 100755 isolate/Makefile
 create mode 100755 isolate/isolate.c

diff --git a/Makefile b/Makefile
index b978809a1..55e46b1ce 100644
--- a/Makefile
+++ b/Makefile
@@ -1101,10 +1101,11 @@ export MODORDER := $(extmod_prefix)modules.order
 export MODULES_NSDEPS := $(extmod_prefix)modules.nsdeps
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y			+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/
+core-y			+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ isolate/
 core-$(CONFIG_BLOCK)	+= block/
 core-$(CONFIG_IO_URING)	+= io_uring/
 
+
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
 		     $(libs-y) $(libs-m)))
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index c84d12608..ddd3a654e 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -372,6 +372,7 @@
 448	common	process_mrelease	sys_process_mrelease
 449	common	futex_waitv		sys_futex_waitv
 450	common	set_mempolicy_home_node	sys_set_mempolicy_home_node
+451	common	isolate			sys_isolate
 
 #
 # Due to a historical design error, certain syscalls are numbered differently
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index a34b0f9a9..527187309 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1385,4 +1385,7 @@ int __sys_getsockopt(int fd, int level, int optname, char __user *optval,
 		int __user *optlen);
 int __sys_setsockopt(int fd, int level, int optname, char __user *optval,
 		int optlen);
+
+asmlinkage long sys_isolate(void);
+		
 #endif
diff --git a/isolate/Makefile b/isolate/Makefile
new file mode 100755
index 000000000..1416a2ae1
--- /dev/null
+++ b/isolate/Makefile
@@ -0,0 +1 @@
+obj-y := isolate.o
diff --git a/isolate/isolate.c b/isolate/isolate.c
new file mode 100755
index 000000000..71247f8b9
--- /dev/null
+++ b/isolate/isolate.c
@@ -0,0 +1,30 @@
+#define _GNU_SOURCE
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+
+SYSCALL_DEFINE0(isolate){
+	struct task_struct *p;
+	int count = 0;
+	struct cpumask new_mask;
+	int result=0;
+	
+	cpumask_clear(&new_mask);
+	cpumask_set_cpu(0, &new_mask);
+	cpumask_set_cpu(2, &new_mask);
+	cpumask_set_cpu(3, &new_mask);
+	// CPU core 1 is chosen to isolate because it has less kernel noise than CPU core 0
+	
+	for_each_process(p) {
+		count+=1;
+		result = sched_setaffinity(task_pid_nr(p), &new_mask);
+		if(result == -1){
+    			printk("Task %s (pid = %d) : Affinity not set\n",p->comm, task_pid_nr(p));
+		}else{
+			printk("Task %s (pid = %d) : Affinity set\n", p->comm, task_pid_nr(p));
+		}
+	}
+	printk("No of processes in taskList: %d\n", count);
+	return 0;
+}
-- 
2.34.1


From e944b6d5284619600f6d51662c49ebec3b3c0504 Mon Sep 17 00:00:00 2001
From: Milin <milinbhade@iisc.ac.in>
Date: Sun, 12 Feb 2023 20:12:56 +0530
Subject: [PATCH 2/3] Commit Before Function Declarations

Signed-off-by: Milin <milinbhade@iisc.ac.in>
---
 Documentation/Changes                         |    1 -
 arch/arm/boot/dts/sun8i-a23-ippo-q8h-v1.2.dts |    1 -
 arch/arm/boot/dts/sun8i-a23-ippo-q8h-v5.dts   |    1 -
 arch/arm/boot/dts/sun8i-a33-et-q8-v1.6.dts    |    1 -
 arch/arm/boot/dts/sun8i-a33-ippo-q8h-v1.2.dts |    1 -
 arch/arm64/boot/dts/arm/vexpress-v2m-rs1.dtsi |    1 -
 include/asm-generic/vmlinux.lds.h             |    1 +
 .../dt-bindings/clock/qcom,dispcc-sm8150.h    |    1 -
 .../dt-bindings/clock/qcom,dispcc-sm8350.h    |    1 -
 include/dt-bindings/input/linux-event-codes.h |    1 -
 include/linux/sched.h                         |   14 +
 include/linux/sched/prio.h                    |    1 +
 include/linux/sched/rsdl.h                    |   31 +
 include/uapi/linux/sched.h                    |    2 +
 kernel/sched/build_policy.c                   |    1 +
 kernel/sched/core.c                           |  983 ++++++++------
 kernel/sched/rsdl.c                           |  214 +++
 kernel/sched/sched.h                          | 1153 +++++++++--------
 scripts/dtc/include-prefixes/arc              |    1 -
 scripts/dtc/include-prefixes/arm              |    1 -
 scripts/dtc/include-prefixes/arm64            |    1 -
 scripts/dtc/include-prefixes/dt-bindings      |    1 -
 scripts/dtc/include-prefixes/microblaze       |    1 -
 scripts/dtc/include-prefixes/mips             |    1 -
 scripts/dtc/include-prefixes/nios2            |    1 -
 scripts/dtc/include-prefixes/openrisc         |    1 -
 scripts/dtc/include-prefixes/powerpc          |    1 -
 scripts/dtc/include-prefixes/sh               |    1 -
 scripts/dtc/include-prefixes/xtensa           |    1 -
 scripts/dummy-tools/nm                        |    1 -
 scripts/dummy-tools/objcopy                   |    1 -
 .../drivers/net/bonding/net_forwarding_lib.sh |    1 -
 .../drivers/net/dsa/bridge_locked_port.sh     |    1 -
 .../selftests/drivers/net/dsa/bridge_mdb.sh   |    1 -
 .../selftests/drivers/net/dsa/bridge_mld.sh   |    1 -
 .../drivers/net/dsa/bridge_vlan_aware.sh      |    1 -
 .../drivers/net/dsa/bridge_vlan_mcast.sh      |    1 -
 .../drivers/net/dsa/bridge_vlan_unaware.sh    |    1 -
 .../testing/selftests/drivers/net/dsa/lib.sh  |    1 -
 .../drivers/net/dsa/local_termination.sh      |    1 -
 .../drivers/net/dsa/no_forwarding.sh          |    1 -
 .../net/mlxsw/spectrum-2/rif_counter_scale.sh |    1 -
 .../selftests/drivers/net/team/lag_lib.sh     |    1 -
 .../drivers/net/team/net_forwarding_lib.sh    |    1 -
 .../selftests/powerpc/copyloops/copy_mc_64.S  |    1 -
 .../selftests/powerpc/copyloops/copyuser_64.S |    1 -
 .../powerpc/copyloops/copyuser_power7.S       |    1 -
 .../selftests/powerpc/copyloops/mem_64.S      |    1 -
 .../selftests/powerpc/copyloops/memcpy_64.S   |    1 -
 .../powerpc/copyloops/memcpy_power7.S         |    1 -
 tools/testing/selftests/powerpc/mce/vas-api.h |    1 -
 .../powerpc/nx-gzip/include/vas-api.h         |    1 -
 .../powerpc/primitives/asm/asm-compat.h       |    1 -
 .../powerpc/primitives/asm/asm-const.h        |    1 -
 .../powerpc/primitives/asm/extable.h          |    1 -
 .../powerpc/primitives/asm/feature-fixups.h   |    1 -
 .../powerpc/primitives/asm/ppc_asm.h          |    1 -
 .../powerpc/primitives/word-at-a-time.h       |    1 -
 .../selftests/powerpc/stringloops/memcmp_32.S |    1 -
 .../selftests/powerpc/stringloops/memcmp_64.S |    1 -
 .../selftests/powerpc/stringloops/strlen_32.S |    1 -
 .../selftests/powerpc/vphn/asm/lppaca.h       |    1 -
 tools/testing/selftests/powerpc/vphn/vphn.c   |    1 -
 63 files changed, 1431 insertions(+), 1023 deletions(-)
 delete mode 120000 Documentation/Changes
 delete mode 120000 arch/arm/boot/dts/sun8i-a23-ippo-q8h-v1.2.dts
 delete mode 120000 arch/arm/boot/dts/sun8i-a23-ippo-q8h-v5.dts
 delete mode 120000 arch/arm/boot/dts/sun8i-a33-et-q8-v1.6.dts
 delete mode 120000 arch/arm/boot/dts/sun8i-a33-ippo-q8h-v1.2.dts
 delete mode 120000 arch/arm64/boot/dts/arm/vexpress-v2m-rs1.dtsi
 delete mode 120000 include/dt-bindings/clock/qcom,dispcc-sm8150.h
 delete mode 120000 include/dt-bindings/clock/qcom,dispcc-sm8350.h
 delete mode 120000 include/dt-bindings/input/linux-event-codes.h
 create mode 100644 include/linux/sched/rsdl.h
 create mode 100644 kernel/sched/rsdl.c
 delete mode 120000 scripts/dtc/include-prefixes/arc
 delete mode 120000 scripts/dtc/include-prefixes/arm
 delete mode 120000 scripts/dtc/include-prefixes/arm64
 delete mode 120000 scripts/dtc/include-prefixes/dt-bindings
 delete mode 120000 scripts/dtc/include-prefixes/microblaze
 delete mode 120000 scripts/dtc/include-prefixes/mips
 delete mode 120000 scripts/dtc/include-prefixes/nios2
 delete mode 120000 scripts/dtc/include-prefixes/openrisc
 delete mode 120000 scripts/dtc/include-prefixes/powerpc
 delete mode 120000 scripts/dtc/include-prefixes/sh
 delete mode 120000 scripts/dtc/include-prefixes/xtensa
 delete mode 120000 scripts/dummy-tools/nm
 delete mode 120000 scripts/dummy-tools/objcopy
 delete mode 120000 tools/testing/selftests/drivers/net/bonding/net_forwarding_lib.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_locked_port.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_mdb.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_mld.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_vlan_aware.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_vlan_mcast.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/bridge_vlan_unaware.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/lib.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/local_termination.sh
 delete mode 120000 tools/testing/selftests/drivers/net/dsa/no_forwarding.sh
 delete mode 120000 tools/testing/selftests/drivers/net/mlxsw/spectrum-2/rif_counter_scale.sh
 delete mode 120000 tools/testing/selftests/drivers/net/team/lag_lib.sh
 delete mode 120000 tools/testing/selftests/drivers/net/team/net_forwarding_lib.sh
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/copy_mc_64.S
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/copyuser_64.S
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/copyuser_power7.S
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/mem_64.S
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/memcpy_64.S
 delete mode 120000 tools/testing/selftests/powerpc/copyloops/memcpy_power7.S
 delete mode 120000 tools/testing/selftests/powerpc/mce/vas-api.h
 delete mode 120000 tools/testing/selftests/powerpc/nx-gzip/include/vas-api.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/asm/asm-compat.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/asm/asm-const.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/asm/extable.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/asm/feature-fixups.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/asm/ppc_asm.h
 delete mode 120000 tools/testing/selftests/powerpc/primitives/word-at-a-time.h
 delete mode 120000 tools/testing/selftests/powerpc/stringloops/memcmp_32.S
 delete mode 120000 tools/testing/selftests/powerpc/stringloops/memcmp_64.S
 delete mode 120000 tools/testing/selftests/powerpc/stringloops/strlen_32.S
 delete mode 120000 tools/testing/selftests/powerpc/vphn/asm/lppaca.h
 delete mode 120000 tools/testing/selftests/powerpc/vphn/vphn.c

diff --git a/Documentation/Changes b/Documentation/Changes
deleted file mode 120000
index 7564ae168..000000000
--- a/Documentation/Changes
+++ /dev/null
@@ -1 +0,0 @@
-process/changes.rst
\ No newline at end of file
diff --git a/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v1.2.dts b/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v1.2.dts
deleted file mode 120000
index c2f22fc33..000000000
--- a/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v1.2.dts
+++ /dev/null
@@ -1 +0,0 @@
-sun8i-a23-q8-tablet.dts
\ No newline at end of file
diff --git a/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v5.dts b/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v5.dts
deleted file mode 120000
index c2f22fc33..000000000
--- a/arch/arm/boot/dts/sun8i-a23-ippo-q8h-v5.dts
+++ /dev/null
@@ -1 +0,0 @@
-sun8i-a23-q8-tablet.dts
\ No newline at end of file
diff --git a/arch/arm/boot/dts/sun8i-a33-et-q8-v1.6.dts b/arch/arm/boot/dts/sun8i-a33-et-q8-v1.6.dts
deleted file mode 120000
index 4519fd791..000000000
--- a/arch/arm/boot/dts/sun8i-a33-et-q8-v1.6.dts
+++ /dev/null
@@ -1 +0,0 @@
-sun8i-a33-q8-tablet.dts
\ No newline at end of file
diff --git a/arch/arm/boot/dts/sun8i-a33-ippo-q8h-v1.2.dts b/arch/arm/boot/dts/sun8i-a33-ippo-q8h-v1.2.dts
deleted file mode 120000
index 4519fd791..000000000
--- a/arch/arm/boot/dts/sun8i-a33-ippo-q8h-v1.2.dts
+++ /dev/null
@@ -1 +0,0 @@
-sun8i-a33-q8-tablet.dts
\ No newline at end of file
diff --git a/arch/arm64/boot/dts/arm/vexpress-v2m-rs1.dtsi b/arch/arm64/boot/dts/arm/vexpress-v2m-rs1.dtsi
deleted file mode 120000
index 68fd0f8f1..000000000
--- a/arch/arm64/boot/dts/arm/vexpress-v2m-rs1.dtsi
+++ /dev/null
@@ -1 +0,0 @@
-../../../../arm/boot/dts/vexpress-v2m-rs1.dtsi
\ No newline at end of file
diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 594422890..f1d72b364 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -130,6 +130,7 @@
 	*(__stop_sched_class)			\
 	*(__dl_sched_class)			\
 	*(__rt_sched_class)			\
+	*(__rsdl_sched_class)			\
 	*(__fair_sched_class)			\
 	*(__idle_sched_class)			\
 	__sched_class_lowest = .;
diff --git a/include/dt-bindings/clock/qcom,dispcc-sm8150.h b/include/dt-bindings/clock/qcom,dispcc-sm8150.h
deleted file mode 120000
index 0312b4544..000000000
--- a/include/dt-bindings/clock/qcom,dispcc-sm8150.h
+++ /dev/null
@@ -1 +0,0 @@
-qcom,dispcc-sm8250.h
\ No newline at end of file
diff --git a/include/dt-bindings/clock/qcom,dispcc-sm8350.h b/include/dt-bindings/clock/qcom,dispcc-sm8350.h
deleted file mode 120000
index 0312b4544..000000000
--- a/include/dt-bindings/clock/qcom,dispcc-sm8350.h
+++ /dev/null
@@ -1 +0,0 @@
-qcom,dispcc-sm8250.h
\ No newline at end of file
diff --git a/include/dt-bindings/input/linux-event-codes.h b/include/dt-bindings/input/linux-event-codes.h
deleted file mode 120000
index 693bbcd26..000000000
--- a/include/dt-bindings/input/linux-event-codes.h
+++ /dev/null
@@ -1 +0,0 @@
-../../uapi/linux/input-event-codes.h
\ No newline at end of file
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8d82d6d32..820f1a4ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -46,6 +46,7 @@ struct bpf_local_storage;
 struct bpf_run_ctx;
 struct capture_control;
 struct cfs_rq;
+struct rsdl_rq;
 struct fs_struct;
 struct futex_pi_state;
 struct io_context;
@@ -588,6 +589,18 @@ struct sched_rt_entity {
 #endif
 } __randomize_layout;
 
+////////// Sched_rsdl_entity added /////////
+
+struct sched_rsdl_entity {
+	struct list_head		run_list;
+	unsigned short			on_rq;
+	unsigned short			on_list;
+
+} __randomize_layout;
+
+
+
+////////////////////////////////////////////
 struct sched_dl_entity {
 	struct rb_node			rb_node;
 
@@ -778,6 +791,7 @@ struct task_struct {
 	struct sched_entity		se;
 	struct sched_rt_entity		rt;
 	struct sched_dl_entity		dl;
+	struct sched_rsdl_entity		rsdl;
 	const struct sched_class	*sched_class;
 
 #ifdef CONFIG_SCHED_CORE
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index ab83d85e1..a16996aa7 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -14,6 +14,7 @@
  */
 
 #define MAX_RT_PRIO		100
+#define MAX_RSDL_PRIO		40
 
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
diff --git a/include/linux/sched/rsdl.h b/include/linux/sched/rsdl.h
new file mode 100644
index 000000000..837f11870
--- /dev/null
+++ b/include/linux/sched/rsdl.h
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_SCHED_RSDL_H
+#define _LINUX_SCHED_RSDL_H
+
+#include <linux/sched.h>
+
+struct task_struct;
+
+static inline int rsdl_prio(int prio)
+{
+	if (unlikely(prio < MAX_PRIO))
+		return 1;
+	return 0;
+}
+
+static inline int rsdl_task(struct task_struct *p)
+{
+	return rsdl_prio(p->prio);
+}
+
+static inline bool task_is_rsdl(struct task_struct *tsk)
+{
+	int policy = tsk->policy;
+
+	if (policy == SCHED_RSDL)
+		return true;
+	return false;
+}
+
+
+#endif /* _LINUX_SCHED_RSDL_H */
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index 3bac0a8ce..894f68ee9 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -118,6 +118,8 @@ struct clone_args {
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
+#define SCHED_RSDL		7
+
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/kernel/sched/build_policy.c b/kernel/sched/build_policy.c
index d9dc9ab37..292db85b2 100644
--- a/kernel/sched/build_policy.c
+++ b/kernel/sched/build_policy.c
@@ -43,6 +43,7 @@
 #include "idle.c"
 
 #include "rt.c"
+#include "rsdl.c"
 
 #ifdef CONFIG_SMP
 # include "cpudeadline.c"
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index cb9d8ae7c..b382acf23 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -36,6 +36,7 @@
 #include <linux/sched/nohz.h>
 #include <linux/sched/rseq_api.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/rsdl.h>
 
 #include <linux/blkdev.h>
 #include <linux/context_tracking.h>
@@ -66,9 +67,9 @@
 #include <linux/workqueue_api.h>
 
 #ifdef CONFIG_PREEMPT_DYNAMIC
-# ifdef CONFIG_GENERIC_ENTRY
-#  include <linux/entry-common.h>
-# endif
+#ifdef CONFIG_GENERIC_ENTRY
+#include <linux/entry-common.h>
+#endif
 #endif
 
 #include <uapi/linux/sched/types.h>
@@ -120,8 +121,7 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
  * sysctl_sched_features, defined in sched.h, to allow constants propagation
  * at compile time and compiler optimization based on features default.
  */
-#define SCHED_FEAT(name, enabled)	\
-	(1UL << __SCHED_FEAT_##name) * enabled |
+#define SCHED_FEAT(name, enabled) (1UL << __SCHED_FEAT_##name) * enabled |
 const_debug unsigned int sysctl_sched_features =
 #include "features.h"
 	0;
@@ -177,9 +177,9 @@ static inline int __task_prio(struct task_struct *p)
  */
 
 /* real prio, less is less */
-static inline bool prio_less(struct task_struct *a, struct task_struct *b, bool in_fi)
+static inline bool prio_less(struct task_struct *a, struct task_struct *b,
+			     bool in_fi)
 {
-
 	int pa = __task_prio(a), pb = __task_prio(b);
 
 	if (-pa < -pb)
@@ -191,13 +191,14 @@ static inline bool prio_less(struct task_struct *a, struct task_struct *b, bool
 	if (pa == -1) /* dl_prio() doesn't work because of stop_class above */
 		return !dl_time_before(a->dl.deadline, b->dl.deadline);
 
-	if (pa == MAX_RT_PRIO + MAX_NICE)	/* fair */
+	if (pa == MAX_RT_PRIO + MAX_NICE) /* fair */
 		return cfs_prio_less(a, b, in_fi);
 
 	return false;
 }
 
-static inline bool __sched_core_less(struct task_struct *a, struct task_struct *b)
+static inline bool __sched_core_less(struct task_struct *a,
+				     struct task_struct *b)
 {
 	if (a->core_cookie < b->core_cookie)
 		return true;
@@ -214,7 +215,8 @@ static inline bool __sched_core_less(struct task_struct *a, struct task_struct *
 
 #define __node_2_sc(node) rb_entry((node), struct task_struct, core_node)
 
-static inline bool rb_sched_core_less(struct rb_node *a, const struct rb_node *b)
+static inline bool rb_sched_core_less(struct rb_node *a,
+				      const struct rb_node *b)
 {
 	return __sched_core_less(__node_2_sc(a), __node_2_sc(b));
 }
@@ -279,7 +281,8 @@ static struct task_struct *sched_core_find(struct rq *rq, unsigned long cookie)
 	return __node_2_sc(node);
 }
 
-static struct task_struct *sched_core_next(struct task_struct *p, unsigned long cookie)
+static struct task_struct *sched_core_next(struct task_struct *p,
+					   unsigned long cookie)
 {
 	struct rb_node *node = &p->core_node;
 
@@ -435,9 +438,13 @@ void sched_core_put(void)
 
 #else /* !CONFIG_SCHED_CORE */
 
-static inline void sched_core_enqueue(struct rq *rq, struct task_struct *p) { }
-static inline void
-sched_core_dequeue(struct rq *rq, struct task_struct *p, int flags) { }
+static inline void sched_core_enqueue(struct rq *rq, struct task_struct *p)
+{
+}
+static inline void sched_core_dequeue(struct rq *rq, struct task_struct *p,
+				      int flags)
+{
+}
 
 #endif /* CONFIG_SCHED_CORE */
 
@@ -635,8 +642,7 @@ struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
  * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.
  */
 struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
-	__acquires(p->pi_lock)
-	__acquires(rq->lock)
+	__acquires(p->pi_lock) __acquires(rq->lock)
 {
 	struct rq *rq;
 
@@ -679,7 +685,7 @@ struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 
 static void update_rq_clock_task(struct rq *rq, s64 delta)
 {
-/*
+	/*
  * In theory, the compile should just see 0 here, and optimize out the call
  * to sched_rt_avg_update. But I don't trust it...
  */
@@ -856,7 +862,7 @@ static void hrtick_rq_init(struct rq *rq)
 	hrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
 	rq->hrtick_timer.function = hrtick;
 }
-#else	/* CONFIG_SCHED_HRTICK */
+#else /* CONFIG_SCHED_HRTICK */
 static inline void hrtick_clear(struct rq *rq)
 {
 }
@@ -864,21 +870,21 @@ static inline void hrtick_clear(struct rq *rq)
 static inline void hrtick_rq_init(struct rq *rq)
 {
 }
-#endif	/* CONFIG_SCHED_HRTICK */
+#endif /* CONFIG_SCHED_HRTICK */
 
 /*
  * cmpxchg based fetch_or, macro so it works for different integer types
  */
-#define fetch_or(ptr, mask)						\
-	({								\
-		typeof(ptr) _ptr = (ptr);				\
-		typeof(mask) _mask = (mask);				\
-		typeof(*_ptr) _val = *_ptr;				\
-									\
-		do {							\
-		} while (!try_cmpxchg(_ptr, &_val, _val | _mask));	\
-	_val;								\
-})
+#define fetch_or(ptr, mask)                                        \
+	({                                                         \
+		typeof(ptr) _ptr = (ptr);                          \
+		typeof(mask) _mask = (mask);                       \
+		typeof(*_ptr) _val = *_ptr;                        \
+                                                                   \
+		do {                                               \
+		} while (!try_cmpxchg(_ptr, &_val, _val | _mask)); \
+		_val;                                              \
+	})
 
 #if defined(CONFIG_SMP) && defined(TIF_POLLING_NRFLAG)
 /*
@@ -1082,7 +1088,8 @@ int get_nohz_timer_target(void)
 	hk_mask = housekeeping_cpumask(HK_TYPE_TIMER);
 
 	rcu_read_lock();
-	for_each_domain(cpu, sd) {
+	for_each_domain(cpu, sd)
+	{
 		for_each_cpu_and(i, sched_domain_span(sd), hk_mask) {
 			if (cpu == i)
 				continue;
@@ -1134,10 +1141,9 @@ static bool wake_up_full_nohz_cpu(int cpu)
 	 * empty IRQ.
 	 */
 	if (cpu_is_offline(cpu))
-		return true;  /* Don't try to wake offline CPUs. */
+		return true; /* Don't try to wake offline CPUs. */
 	if (tick_nohz_full_cpu(cpu)) {
-		if (cpu != smp_processor_id() ||
-		    tick_nohz_tick_stopped())
+		if (cpu != smp_processor_id() || tick_nohz_tick_stopped())
 			tick_nohz_full_kick_cpu(cpu);
 		return true;
 	}
@@ -1165,7 +1171,8 @@ static void nohz_csd_func(void *info)
 	/*
 	 * Release the rq::nohz_csd.
 	 */
-	flags = atomic_fetch_andnot(NOHZ_KICK_MASK | NOHZ_NEWILB_KICK, nohz_flags(cpu));
+	flags = atomic_fetch_andnot(NOHZ_KICK_MASK | NOHZ_NEWILB_KICK,
+				    nohz_flags(cpu));
 	WARN_ON(!(flags & NOHZ_KICK_MASK));
 
 	rq->idle_balance = idle_cpu(cpu);
@@ -1218,16 +1225,17 @@ bool sched_can_stop_tick(struct rq *rq)
 #endif /* CONFIG_NO_HZ_FULL */
 #endif /* CONFIG_SMP */
 
-#if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) && \
-			(defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH)))
+#if defined(CONFIG_RT_GROUP_SCHED) ||        \
+	(defined(CONFIG_FAIR_GROUP_SCHED) && \
+	 (defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH)))
 /*
  * Iterate task_group tree rooted at *from, calling @down when first entering a
  * node and @up when leaving it for the final time.
  *
  * Caller must hold rcu_lock or sufficient equivalent.
  */
-int walk_tg_tree_from(struct task_group *from,
-			     tg_visitor down, tg_visitor up, void *data)
+int walk_tg_tree_from(struct task_group *from, tg_visitor down, tg_visitor up,
+		      void *data)
 {
 	struct task_group *parent, *child;
 	int ret;
@@ -1303,10 +1311,12 @@ static void set_load_weight(struct task_struct *p, bool update_load)
 static DEFINE_MUTEX(uclamp_mutex);
 
 /* Max allowed minimum utilization */
-static unsigned int __maybe_unused sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
+static unsigned int __maybe_unused sysctl_sched_uclamp_util_min =
+	SCHED_CAPACITY_SCALE;
 
 /* Max allowed maximum utilization */
-static unsigned int __maybe_unused sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
+static unsigned int __maybe_unused sysctl_sched_uclamp_util_max =
+	SCHED_CAPACITY_SCALE;
 
 /*
  * By default RT tasks run at the maximum performance point/capacity of the
@@ -1323,7 +1333,8 @@ static unsigned int __maybe_unused sysctl_sched_uclamp_util_max = SCHED_CAPACITY
  * This knob will not override the system default sched_util_clamp_min defined
  * above.
  */
-static unsigned int sysctl_sched_uclamp_util_min_rt_default = SCHED_CAPACITY_SCALE;
+static unsigned int sysctl_sched_uclamp_util_min_rt_default =
+	SCHED_CAPACITY_SCALE;
 
 /* All clamps are required to be less or equal than these values */
 static struct uclamp_se uclamp_default[UCLAMP_CNT];
@@ -1349,14 +1360,16 @@ static struct uclamp_se uclamp_default[UCLAMP_CNT];
 DEFINE_STATIC_KEY_FALSE(sched_uclamp_used);
 
 /* Integer rounded range for each bucket */
-#define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
+#define UCLAMP_BUCKET_DELTA \
+	DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
 
 #define for_each_clamp_id(clamp_id) \
 	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
 
 static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
 {
-	return min_t(unsigned int, clamp_value / UCLAMP_BUCKET_DELTA, UCLAMP_BUCKETS - 1);
+	return min_t(unsigned int, clamp_value / UCLAMP_BUCKET_DELTA,
+		     UCLAMP_BUCKETS - 1);
 }
 
 static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
@@ -1366,17 +1379,17 @@ static inline unsigned int uclamp_none(enum uclamp_id clamp_id)
 	return SCHED_CAPACITY_SCALE;
 }
 
-static inline void uclamp_se_set(struct uclamp_se *uc_se,
-				 unsigned int value, bool user_defined)
+static inline void uclamp_se_set(struct uclamp_se *uc_se, unsigned int value,
+				 bool user_defined)
 {
 	uc_se->value = value;
 	uc_se->bucket_id = uclamp_bucket_id(value);
 	uc_se->user_defined = user_defined;
 }
 
-static inline unsigned int
-uclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,
-		  unsigned int clamp_value)
+static inline unsigned int uclamp_idle_value(struct rq *rq,
+					     enum uclamp_id clamp_id,
+					     unsigned int clamp_value)
 {
 	/*
 	 * Avoid blocked utilization pushing up the frequency when we go
@@ -1401,9 +1414,9 @@ static inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,
 	uclamp_rq_set(rq, clamp_id, clamp_value);
 }
 
-static inline
-unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
-				   unsigned int clamp_value)
+static inline unsigned int uclamp_rq_max_value(struct rq *rq,
+					       enum uclamp_id clamp_id,
+					       unsigned int clamp_value)
 {
 	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
 	int bucket_id = UCLAMP_BUCKETS - 1;
@@ -1412,7 +1425,7 @@ unsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,
 	 * Since both min and max clamps are max aggregated, find the
 	 * top most bucket with tasks in.
 	 */
-	for ( ; bucket_id >= 0; bucket_id--) {
+	for (; bucket_id >= 0; bucket_id--) {
 		if (!bucket[bucket_id].tasks)
 			continue;
 		return bucket[bucket_id].value;
@@ -1453,8 +1466,8 @@ static void uclamp_update_util_min_rt_default(struct task_struct *p)
 	task_rq_unlock(rq, p, &rf);
 }
 
-static inline struct uclamp_se
-uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
+static inline struct uclamp_se uclamp_tg_restrict(struct task_struct *p,
+						  enum uclamp_id clamp_id)
 {
 	/* Copy by value as we could modify it */
 	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
@@ -1488,8 +1501,8 @@ uclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)
  *   group or in an autogroup
  * - the system default clamp value, defined by the sysadmin
  */
-static inline struct uclamp_se
-uclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)
+static inline struct uclamp_se uclamp_eff_get(struct task_struct *p,
+					      enum uclamp_id clamp_id)
 {
 	struct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);
 	struct uclamp_se uc_max = uclamp_default[clamp_id];
@@ -1644,8 +1657,7 @@ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
 	if (unlikely(!p->sched_class->uclamp_enabled))
 		return;
 
-	for_each_clamp_id(clamp_id)
-		uclamp_rq_inc_id(rq, p, clamp_id);
+	for_each_clamp_id(clamp_id) uclamp_rq_inc_id(rq, p, clamp_id);
 
 	/* Reset clamp idle holding when there is one RUNNABLE task */
 	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
@@ -1668,8 +1680,7 @@ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
 	if (unlikely(!p->sched_class->uclamp_enabled))
 		return;
 
-	for_each_clamp_id(clamp_id)
-		uclamp_rq_dec_id(rq, p, clamp_id);
+	for_each_clamp_id(clamp_id) uclamp_rq_dec_id(rq, p, clamp_id);
 }
 
 static inline void uclamp_rq_reinc_id(struct rq *rq, struct task_struct *p,
@@ -1689,8 +1700,7 @@ static inline void uclamp_rq_reinc_id(struct rq *rq, struct task_struct *p,
 		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
 }
 
-static inline void
-uclamp_update_active(struct task_struct *p)
+static inline void uclamp_update_active(struct task_struct *p)
 {
 	enum uclamp_id clamp_id;
 	struct rq_flags rf;
@@ -1712,15 +1722,13 @@ uclamp_update_active(struct task_struct *p)
 	 * affecting a valid clamp bucket, the next time it's enqueued,
 	 * it will already see the updated clamp bucket value.
 	 */
-	for_each_clamp_id(clamp_id)
-		uclamp_rq_reinc_id(rq, p, clamp_id);
+	for_each_clamp_id(clamp_id) uclamp_rq_reinc_id(rq, p, clamp_id);
 
 	task_rq_unlock(rq, p, &rf);
 }
 
 #ifdef CONFIG_UCLAMP_TASK_GROUP
-static inline void
-uclamp_update_active_tasks(struct cgroup_subsys_state *css)
+static inline void uclamp_update_active_tasks(struct cgroup_subsys_state *css)
 {
 	struct css_task_iter it;
 	struct task_struct *p;
@@ -1741,17 +1749,19 @@ static void uclamp_update_root_tg(void)
 {
 	struct task_group *tg = &root_task_group;
 
-	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],
-		      sysctl_sched_uclamp_util_min, false);
-	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],
-		      sysctl_sched_uclamp_util_max, false);
+	uclamp_se_set(&tg->uclamp_req[UCLAMP_MIN], sysctl_sched_uclamp_util_min,
+		      false);
+	uclamp_se_set(&tg->uclamp_req[UCLAMP_MAX], sysctl_sched_uclamp_util_max,
+		      false);
 
 	rcu_read_lock();
 	cpu_util_update_eff(&root_task_group.css);
 	rcu_read_unlock();
 }
 #else
-static void uclamp_update_root_tg(void) { }
+static void uclamp_update_root_tg(void)
+{
+}
 #endif
 
 static void uclamp_sync_util_min_rt_default(void)
@@ -1782,7 +1792,7 @@ static void uclamp_sync_util_min_rt_default(void)
 }
 
 static int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
-				void *buffer, size_t *lenp, loff_t *ppos)
+				       void *buffer, size_t *lenp, loff_t *ppos)
 {
 	bool update_root_tg = false;
 	int old_min, old_max, old_min_rt;
@@ -1800,9 +1810,8 @@ static int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
 		goto done;
 
 	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
-	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE	||
+	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE ||
 	    sysctl_sched_uclamp_util_min_rt_default > SCHED_CAPACITY_SCALE) {
-
 		result = -EINVAL;
 		goto undo;
 	}
@@ -1848,8 +1857,7 @@ static int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
 #endif
 #endif
 
-static int uclamp_validate(struct task_struct *p,
-			   const struct sched_attr *attr)
+static int uclamp_validate(struct task_struct *p, const struct sched_attr *attr)
 {
 	int util_min = p->uclamp_req[UCLAMP_MIN].value;
 	int util_max = p->uclamp_req[UCLAMP_MAX].value;
@@ -1883,8 +1891,7 @@ static int uclamp_validate(struct task_struct *p,
 	return 0;
 }
 
-static bool uclamp_reset(const struct sched_attr *attr,
-			 enum uclamp_id clamp_id,
+static bool uclamp_reset(const struct sched_attr *attr, enum uclamp_id clamp_id,
 			 struct uclamp_se *uc_se)
 {
 	/* Reset on sched class change for a non user-defined clamp value. */
@@ -1913,7 +1920,8 @@ static void __setscheduler_uclamp(struct task_struct *p,
 {
 	enum uclamp_id clamp_id;
 
-	for_each_clamp_id(clamp_id) {
+	for_each_clamp_id(clamp_id)
+	{
 		struct uclamp_se *uc_se = &p->uclamp_req[clamp_id];
 		unsigned int value;
 
@@ -1930,7 +1938,6 @@ static void __setscheduler_uclamp(struct task_struct *p,
 			value = uclamp_none(clamp_id);
 
 		uclamp_se_set(uc_se, value, false);
-
 	}
 
 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
@@ -1938,14 +1945,14 @@ static void __setscheduler_uclamp(struct task_struct *p,
 
 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&
 	    attr->sched_util_min != -1) {
-		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
-			      attr->sched_util_min, true);
+		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN], attr->sched_util_min,
+			      true);
 	}
 
 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&
 	    attr->sched_util_max != -1) {
-		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
-			      attr->sched_util_max, true);
+		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX], attr->sched_util_max,
+			      true);
 	}
 }
 
@@ -1957,15 +1964,15 @@ static void uclamp_fork(struct task_struct *p)
 	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here
 	 * as the task is still at its early fork stages.
 	 */
-	for_each_clamp_id(clamp_id)
-		p->uclamp[clamp_id].active = false;
+	for_each_clamp_id(clamp_id) p->uclamp[clamp_id].active = false;
 
 	if (likely(!p->sched_reset_on_fork))
 		return;
 
-	for_each_clamp_id(clamp_id) {
-		uclamp_se_set(&p->uclamp_req[clamp_id],
-			      uclamp_none(clamp_id), false);
+	for_each_clamp_id(clamp_id)
+	{
+		uclamp_se_set(&p->uclamp_req[clamp_id], uclamp_none(clamp_id),
+			      false);
 	}
 }
 
@@ -1979,10 +1986,10 @@ static void __init init_uclamp_rq(struct rq *rq)
 	enum uclamp_id clamp_id;
 	struct uclamp_rq *uc_rq = rq->uclamp;
 
-	for_each_clamp_id(clamp_id) {
-		uc_rq[clamp_id] = (struct uclamp_rq) {
-			.value = uclamp_none(clamp_id)
-		};
+	for_each_clamp_id(clamp_id)
+	{
+		uc_rq[clamp_id] =
+			(struct uclamp_rq){ .value = uclamp_none(clamp_id) };
 	}
 
 	rq->uclamp_flags = UCLAMP_FLAG_IDLE;
@@ -1997,14 +2004,16 @@ static void __init init_uclamp(void)
 	for_each_possible_cpu(cpu)
 		init_uclamp_rq(cpu_rq(cpu));
 
-	for_each_clamp_id(clamp_id) {
+	for_each_clamp_id(clamp_id)
+	{
 		uclamp_se_set(&init_task.uclamp_req[clamp_id],
 			      uclamp_none(clamp_id), false);
 	}
 
 	/* System defaults allow max clamp values for both indexes */
 	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
-	for_each_clamp_id(clamp_id) {
+	for_each_clamp_id(clamp_id)
+	{
 		uclamp_default[clamp_id] = uc_max;
 #ifdef CONFIG_UCLAMP_TASK_GROUP
 		root_task_group.uclamp_req[clamp_id] = uc_max;
@@ -2014,18 +2023,30 @@ static void __init init_uclamp(void)
 }
 
 #else /* CONFIG_UCLAMP_TASK */
-static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
-static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
+static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
+{
+}
+static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
+{
+}
 static inline int uclamp_validate(struct task_struct *p,
 				  const struct sched_attr *attr)
 {
 	return -EOPNOTSUPP;
 }
 static void __setscheduler_uclamp(struct task_struct *p,
-				  const struct sched_attr *attr) { }
-static inline void uclamp_fork(struct task_struct *p) { }
-static inline void uclamp_post_fork(struct task_struct *p) { }
-static inline void init_uclamp(void) { }
+				  const struct sched_attr *attr)
+{
+}
+static inline void uclamp_fork(struct task_struct *p)
+{
+}
+static inline void uclamp_post_fork(struct task_struct *p)
+{
+}
+static inline void init_uclamp(void)
+{
+}
 #endif /* CONFIG_UCLAMP_TASK */
 
 bool sched_task_on_rq(struct task_struct *p)
@@ -2103,11 +2124,12 @@ void deactivate_task(struct rq *rq, struct task_struct *p, int flags)
 static inline int __normal_prio(int policy, int rt_prio, int nice)
 {
 	int prio;
-
 	if (dl_policy(policy))
 		prio = MAX_DL_PRIO - 1;
 	else if (rt_policy(policy))
 		prio = MAX_RT_PRIO - 1 - rt_prio;
+	else if(rsdl_policy(policy))
+		prio = NICE_TO_PRIO(nice); 
 	else
 		prio = NICE_TO_PRIO(nice);
 
@@ -2123,7 +2145,8 @@ static inline int __normal_prio(int policy, int rt_prio, int nice)
  */
 static inline int normal_prio(struct task_struct *p)
 {
-	return __normal_prio(p->policy, p->rt_priority, PRIO_TO_NICE(p->static_prio));
+	return __normal_prio(p->policy, p->rt_priority,
+			     PRIO_TO_NICE(p->static_prio));
 }
 
 /*
@@ -2194,12 +2217,11 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 
 #ifdef CONFIG_SMP
 
-static void
-__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+static void __do_set_cpus_allowed(struct task_struct *p,
+				  const struct cpumask *new_mask, u32 flags);
 
 static int __set_cpus_allowed_ptr(struct task_struct *p,
-				  const struct cpumask *new_mask,
-				  u32 flags);
+				  const struct cpumask *new_mask, u32 flags);
 
 static void migrate_disable_switch(struct rq *rq, struct task_struct *p)
 {
@@ -2336,9 +2358,9 @@ static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,
 }
 
 struct migration_arg {
-	struct task_struct		*task;
-	int				dest_cpu;
-	struct set_affinity_pending	*pending;
+	struct task_struct *task;
+	int dest_cpu;
+	struct set_affinity_pending *pending;
 };
 
 /*
@@ -2346,11 +2368,11 @@ struct migration_arg {
  * @stop_pending: is @stop_work in use
  */
 struct set_affinity_pending {
-	refcount_t		refs;
-	unsigned int		stop_pending;
-	struct completion	done;
-	struct cpu_stop_work	stop_work;
-	struct migration_arg	arg;
+	refcount_t refs;
+	unsigned int stop_pending;
+	struct completion done;
+	struct cpu_stop_work stop_work;
+	struct migration_arg arg;
 };
 
 /*
@@ -2529,7 +2551,8 @@ int push_cpu_stop(void *arg)
  * sched_class::set_cpus_allowed must do the below, but is not required to
  * actually call this function.
  */
-void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
+void set_cpus_allowed_common(struct task_struct *p,
+			     const struct cpumask *new_mask, u32 flags)
 {
 	if (flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
 		p->cpus_ptr = new_mask;
@@ -2540,8 +2563,8 @@ void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_ma
 	p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
-static void
-__do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
+static void __do_set_cpus_allowed(struct task_struct *p,
+				  const struct cpumask *new_mask, u32 flags)
 {
 	struct rq *rq = task_rq(p);
 	bool queued, running;
@@ -2694,10 +2717,11 @@ void release_user_cpus_ptr(struct task_struct *p)
  * pending affinity completion is preceded by an uninstallation of
  * p->migration_pending done with p->pi_lock held.
  */
-static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,
-			    int dest_cpu, unsigned int flags)
+static int affine_move_task(struct rq *rq, struct task_struct *p,
+			    struct rq_flags *rf, int dest_cpu,
+			    unsigned int flags)
 {
-	struct set_affinity_pending my_pending = { }, *pending = NULL;
+	struct set_affinity_pending my_pending = {}, *pending = NULL;
 	bool stop_pending, complete = false;
 
 	/* Can the task run on the task's current CPU? If so, we're done */
@@ -2723,8 +2747,8 @@ static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flag
 		task_rq_unlock(rq, p, rf);
 
 		if (push_task) {
-			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
-					    p, &rq->push_work);
+			stop_one_cpu_nowait(rq->cpu, push_cpu_stop, p,
+					    &rq->push_work);
 		}
 
 		if (complete)
@@ -2739,7 +2763,7 @@ static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flag
 			/* Install the request */
 			refcount_set(&my_pending.refs, 1);
 			init_completion(&my_pending.done);
-			my_pending.arg = (struct migration_arg) {
+			my_pending.arg = (struct migration_arg){
 				.task = p,
 				.dest_cpu = dest_cpu,
 				.pending = &my_pending,
@@ -2801,7 +2825,6 @@ static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flag
 		if (flags & SCA_MIGRATE_ENABLE)
 			return 0;
 	} else {
-
 		if (!is_migration_disabled(p)) {
 			if (task_on_rq_queued(p))
 				rq = move_queued_task(rq, rf, p, dest_cpu);
@@ -2839,11 +2862,9 @@ static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flag
  */
 static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 					 const struct cpumask *new_mask,
-					 u32 flags,
-					 struct rq *rq,
+					 u32 flags, struct rq *rq,
 					 struct rq_flags *rf)
-	__releases(rq->lock)
-	__releases(p->pi_lock)
+	__releases(rq->lock) __releases(p->pi_lock)
 {
 	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
@@ -2886,8 +2907,7 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 		if (cpumask_equal(&p->cpus_mask, new_mask))
 			goto out;
 
-		if (WARN_ON_ONCE(p == current &&
-				 is_migration_disabled(p) &&
+		if (WARN_ON_ONCE(p == current && is_migration_disabled(p) &&
 				 !cpumask_test_cpu(task_cpu(p), new_mask))) {
 			ret = -EBUSY;
 			goto out;
@@ -3037,9 +3057,10 @@ void force_compatible_cpus_allowed_ptr(struct task_struct *p)
 
 out_set_mask:
 	if (printk_ratelimit()) {
-		printk_deferred("Overriding affinity for process %d (%s) to CPUs %*pbl\n",
-				task_pid_nr(p), p->comm,
-				cpumask_pr_args(override_mask));
+		printk_deferred(
+			"Overriding affinity for process %d (%s) to CPUs %*pbl\n",
+			task_pid_nr(p), p->comm,
+			cpumask_pr_args(override_mask));
 	}
 
 	WARN_ON(set_cpus_allowed_ptr(p, override_mask));
@@ -3048,8 +3069,8 @@ void force_compatible_cpus_allowed_ptr(struct task_struct *p)
 	free_cpumask_var(new_mask);
 }
 
-static int
-__sched_setaffinity(struct task_struct *p, const struct cpumask *mask);
+static int __sched_setaffinity(struct task_struct *p,
+			       const struct cpumask *mask);
 
 /*
  * Restore the affinity of a task @p which was previously restricted by a
@@ -3088,7 +3109,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	 * We should never call set_task_cpu() on a blocked task,
 	 * ttwu() will sort out the placement.
 	 */
-	WARN_ON_ONCE(state != TASK_RUNNING && state != TASK_WAKING && !p->on_rq);
+	WARN_ON_ONCE(state != TASK_RUNNING && state != TASK_WAKING &&
+		     !p->on_rq);
 
 	/*
 	 * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,
@@ -3182,8 +3204,7 @@ static int migrate_swap_stop(void *data)
 	src_rq = cpu_rq(arg->src_cpu);
 	dst_rq = cpu_rq(arg->dst_cpu);
 
-	double_raw_lock(&arg->src_task->pi_lock,
-			&arg->dst_task->pi_lock);
+	double_raw_lock(&arg->src_task->pi_lock, &arg->dst_task->pi_lock);
 	double_rq_lock(src_rq, dst_rq);
 
 	if (task_cpu(arg->dst_task) != arg->dst_cpu)
@@ -3214,8 +3235,8 @@ static int migrate_swap_stop(void *data)
 /*
  * Cross migrate two tasks
  */
-int migrate_swap(struct task_struct *cur, struct task_struct *p,
-		int target_cpu, int curr_cpu)
+int migrate_swap(struct task_struct *cur, struct task_struct *p, int target_cpu,
+		 int curr_cpu)
 {
 	struct migration_swap_arg arg;
 	int ret = -EINVAL;
@@ -3267,7 +3288,8 @@ int migrate_swap(struct task_struct *cur, struct task_struct *p,
  * smp_call_function() if an IPI is sent by the same process we are
  * waiting to become inactive.
  */
-unsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state)
+unsigned long wait_task_inactive(struct task_struct *p,
+				 unsigned int match_state)
 {
 	int running, queued;
 	struct rq_flags rf;
@@ -3295,7 +3317,8 @@ unsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state
 		 * is actually now running somewhere else!
 		 */
 		while (task_running(rq, p)) {
-			if (match_state && unlikely(READ_ONCE(p->__state) != match_state))
+			if (match_state &&
+			    unlikely(READ_ONCE(p->__state) != match_state))
 				return 0;
 			cpu_relax();
 		}
@@ -3469,8 +3492,9 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 		 * leave kernel.
 		 */
 		if (p->mm && printk_ratelimit()) {
-			printk_deferred("process %d (%s) no longer affine to cpu%d\n",
-					task_pid_nr(p), p->comm, cpu);
+			printk_deferred(
+				"process %d (%s) no longer affine to cpu%d\n",
+				task_pid_nr(p), p->comm, cpu);
 		}
 	}
 
@@ -3480,8 +3504,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 /*
  * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable.
  */
-static inline
-int select_task_rq(struct task_struct *p, int cpu, int wake_flags)
+static inline int select_task_rq(struct task_struct *p, int cpu, int wake_flags)
 {
 	lockdep_assert_held(&p->pi_lock);
 
@@ -3560,7 +3583,9 @@ static inline int __set_cpus_allowed_ptr(struct task_struct *p,
 	return set_cpus_allowed_ptr(p, new_mask);
 }
 
-static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
+static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p)
+{
+}
 
 static inline bool rq_has_pinned_tasks(struct rq *rq)
 {
@@ -3569,8 +3594,7 @@ static inline bool rq_has_pinned_tasks(struct rq *rq)
 
 #endif /* !CONFIG_SMP */
 
-static void
-ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
+static void ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 {
 	struct rq *rq;
 
@@ -3588,7 +3612,8 @@ ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 
 		__schedstat_inc(p->stats.nr_wakeups_remote);
 		rcu_read_lock();
-		for_each_domain(rq->cpu, sd) {
+		for_each_domain(rq->cpu, sd)
+		{
 			if (cpumask_test_cpu(cpu, sched_domain_span(sd))) {
 				__schedstat_inc(sd->ttwu_wake_remote);
 				break;
@@ -3631,7 +3656,7 @@ static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
 
 	if (rq->idle_stamp) {
 		u64 delta = rq_clock(rq) - rq->idle_stamp;
-		u64 max = 2*rq->max_idle_balance_cost;
+		u64 max = 2 * rq->max_idle_balance_cost;
 
 		update_avg(&rq->avg_idle, delta);
 
@@ -3646,9 +3671,8 @@ static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
 #endif
 }
 
-static void
-ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
-		 struct rq_flags *rf)
+static void ttwu_do_activate(struct rq *rq, struct task_struct *p,
+			     int wake_flags, struct rq_flags *rf)
 {
 	int en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;
 
@@ -3662,7 +3686,7 @@ ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
 		en_flags |= ENQUEUE_MIGRATED;
 	else
 #endif
-	if (p->in_iowait) {
+		if (p->in_iowait) {
 		delayacct_blkio_end(p);
 		atomic_dec(&task_rq(p)->nr_iowait);
 	}
@@ -3742,7 +3766,8 @@ void sched_ttwu_pending(void *arg)
 		if (WARN_ON_ONCE(task_cpu(p) != cpu_of(rq)))
 			set_task_cpu(p, cpu_of(rq));
 
-		ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
+		ttwu_do_activate(rq, p,
+				 p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
 	}
 
 	rq_unlock_irqrestore(rq, &rf);
@@ -3764,7 +3789,8 @@ void send_call_function_single_ipi(int cpu)
  * via sched_ttwu_wakeup() for activation so the wakee incurs the cost
  * of the wakeup instead of the waker.
  */
-static void __ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
+static void __ttwu_queue_wakelist(struct task_struct *p, int cpu,
+				  int wake_flags)
 {
 	struct rq *rq = cpu_rq(cpu);
 
@@ -3855,7 +3881,8 @@ static bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
 
 #else /* !CONFIG_SMP */
 
-static inline bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
+static inline bool ttwu_queue_wakelist(struct task_struct *p, int cpu,
+				       int wake_flags)
 {
 	return false;
 }
@@ -3890,8 +3917,8 @@ static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)
  *   The lock wait and lock wakeups happen via TASK_RTLOCK_WAIT. No other
  *   bits set. This allows to distinguish all wakeup scenarios.
  */
-static __always_inline
-bool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)
+static __always_inline bool ttwu_state_match(struct task_struct *p,
+					     unsigned int state, int *success)
 {
 	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)) {
 		WARN_ON_ONCE((state & TASK_RTLOCK_WAIT) &&
@@ -4045,8 +4072,8 @@ bool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)
  * Return: %true if @p->state changes (an actual wakeup was done),
  *	   %false otherwise.
  */
-static int
-try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
+static int try_to_wake_up(struct task_struct *p, unsigned int state,
+			  int wake_flags)
 {
 	unsigned long flags;
 	int cpu, success = 0;
@@ -4328,18 +4355,18 @@ int wake_up_state(struct task_struct *p, unsigned int state)
  */
 static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 {
-	p->on_rq			= 0;
+	p->on_rq = 0;
 
-	p->se.on_rq			= 0;
-	p->se.exec_start		= 0;
-	p->se.sum_exec_runtime		= 0;
-	p->se.prev_sum_exec_runtime	= 0;
-	p->se.nr_migrations		= 0;
-	p->se.vruntime			= 0;
+	p->se.on_rq = 0;
+	p->se.exec_start = 0;
+	p->se.sum_exec_runtime = 0;
+	p->se.prev_sum_exec_runtime = 0;
+	p->se.nr_migrations = 0;
+	p->se.vruntime = 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	p->se.cfs_rq			= NULL;
+	p->se.cfs_rq = NULL;
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
@@ -4353,10 +4380,14 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	__dl_clear_params(p);
 
 	INIT_LIST_HEAD(&p->rt.run_list);
-	p->rt.timeout		= 0;
-	p->rt.time_slice	= sched_rr_timeslice;
-	p->rt.on_rq		= 0;
-	p->rt.on_list		= 0;
+	p->rt.timeout = 0;
+	p->rt.time_slice = sched_rr_timeslice;
+	p->rt.on_rq = 0;
+	p->rt.on_list = 0;
+
+	//////// Add initialization related to rsdl entity p->rsdl
+
+	///////////////////////////////////////////////////////////
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
@@ -4396,8 +4427,8 @@ void set_numabalancing_state(bool enabled)
 }
 
 #ifdef CONFIG_PROC_SYSCTL
-int sysctl_numa_balancing(struct ctl_table *table, int write,
-			  void *buffer, size_t *lenp, loff_t *ppos)
+int sysctl_numa_balancing(struct ctl_table *table, int write, void *buffer,
+			  size_t *lenp, loff_t *ppos)
 {
 	struct ctl_table t;
 	int err;
@@ -4463,7 +4494,7 @@ __setup("schedstats=", setup_schedstats);
 
 #ifdef CONFIG_PROC_SYSCTL
 static int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
-		size_t *lenp, loff_t *ppos)
+			     size_t *lenp, loff_t *ppos)
 {
 	struct ctl_table t;
 	int err;
@@ -4488,36 +4519,36 @@ static int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
 static struct ctl_table sched_core_sysctls[] = {
 #ifdef CONFIG_SCHEDSTATS
 	{
-		.procname       = "sched_schedstats",
-		.data           = NULL,
-		.maxlen         = sizeof(unsigned int),
-		.mode           = 0644,
-		.proc_handler   = sysctl_schedstats,
-		.extra1         = SYSCTL_ZERO,
-		.extra2         = SYSCTL_ONE,
+		.procname = "sched_schedstats",
+		.data = NULL,
+		.maxlen = sizeof(unsigned int),
+		.mode = 0644,
+		.proc_handler = sysctl_schedstats,
+		.extra1 = SYSCTL_ZERO,
+		.extra2 = SYSCTL_ONE,
 	},
 #endif /* CONFIG_SCHEDSTATS */
 #ifdef CONFIG_UCLAMP_TASK
 	{
-		.procname       = "sched_util_clamp_min",
-		.data           = &sysctl_sched_uclamp_util_min,
-		.maxlen         = sizeof(unsigned int),
-		.mode           = 0644,
-		.proc_handler   = sysctl_sched_uclamp_handler,
+		.procname = "sched_util_clamp_min",
+		.data = &sysctl_sched_uclamp_util_min,
+		.maxlen = sizeof(unsigned int),
+		.mode = 0644,
+		.proc_handler = sysctl_sched_uclamp_handler,
 	},
 	{
-		.procname       = "sched_util_clamp_max",
-		.data           = &sysctl_sched_uclamp_util_max,
-		.maxlen         = sizeof(unsigned int),
-		.mode           = 0644,
-		.proc_handler   = sysctl_sched_uclamp_handler,
+		.procname = "sched_util_clamp_max",
+		.data = &sysctl_sched_uclamp_util_max,
+		.maxlen = sizeof(unsigned int),
+		.mode = 0644,
+		.proc_handler = sysctl_sched_uclamp_handler,
 	},
 	{
-		.procname       = "sched_util_clamp_min_rt_default",
-		.data           = &sysctl_sched_uclamp_util_min_rt_default,
-		.maxlen         = sizeof(unsigned int),
-		.mode           = 0644,
-		.proc_handler   = sysctl_sched_uclamp_handler,
+		.procname = "sched_util_clamp_min_rt_default",
+		.data = &sysctl_sched_uclamp_util_min_rt_default,
+		.maxlen = sizeof(unsigned int),
+		.mode = 0644,
+		.proc_handler = sysctl_sched_uclamp_handler,
 	},
 #endif /* CONFIG_UCLAMP_TASK */
 	{}
@@ -4575,12 +4606,13 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		return -EAGAIN;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	// else if (rsdl_prio(p->prio) && rsdl_policy(p->policy))
+	// 	p->sched_class = &rsdl_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 
 	init_entity_runnable_average(&p->se);
 
-
 #ifdef CONFIG_SCHED_INFO
 	if (likely(sched_info_on()))
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
@@ -4717,7 +4749,8 @@ EXPORT_SYMBOL_GPL(preempt_notifier_dec);
 void preempt_notifier_register(struct preempt_notifier *notifier)
 {
 	if (!static_branch_unlikely(&preempt_notifier_key))
-		WARN(1, "registering preempt_notifier while notifiers disabled\n");
+		WARN(1,
+		     "registering preempt_notifier while notifiers disabled\n");
 
 	hlist_add_head(&notifier->link, &current->preempt_notifiers);
 }
@@ -4743,15 +4776,15 @@ static void __fire_sched_in_preempt_notifiers(struct task_struct *curr)
 		notifier->ops->sched_in(notifier, raw_smp_processor_id());
 }
 
-static __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
+static __always_inline void
+fire_sched_in_preempt_notifiers(struct task_struct *curr)
 {
 	if (static_branch_unlikely(&preempt_notifier_key))
 		__fire_sched_in_preempt_notifiers(curr);
 }
 
-static void
-__fire_sched_out_preempt_notifiers(struct task_struct *curr,
-				   struct task_struct *next)
+static void __fire_sched_out_preempt_notifiers(struct task_struct *curr,
+					       struct task_struct *next)
 {
 	struct preempt_notifier *notifier;
 
@@ -4773,9 +4806,8 @@ static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)
 {
 }
 
-static inline void
-fire_sched_out_preempt_notifiers(struct task_struct *curr,
-				 struct task_struct *next)
+static inline void fire_sched_out_preempt_notifiers(struct task_struct *curr,
+						    struct task_struct *next)
 {
 }
 
@@ -4817,7 +4849,7 @@ static inline void finish_task(struct task_struct *prev)
 
 static void do_balance_callbacks(struct rq *rq, struct callback_head *head)
 {
-	void (*func)(struct rq *rq);
+	void (*func)(struct rq * rq);
 	struct callback_head *next;
 
 	lockdep_assert_rq_held(rq);
@@ -4850,8 +4882,8 @@ struct callback_head balance_push_callback = {
 	.func = (void (*)(struct callback_head *))balance_push,
 };
 
-static inline struct callback_head *
-__splice_balance_callbacks(struct rq *rq, bool split)
+static inline struct callback_head *__splice_balance_callbacks(struct rq *rq,
+							       bool split)
 {
 	struct callback_head *head = rq->balance_callback;
 
@@ -4913,8 +4945,8 @@ static inline void balance_callbacks(struct rq *rq, struct callback_head *head)
 
 #endif
 
-static inline void
-prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf)
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next,
+				       struct rq_flags *rf)
 {
 	/*
 	 * Since the runqueue lock will be released by the next
@@ -4947,11 +4979,15 @@ static inline void finish_lock_switch(struct rq *rq)
  */
 
 #ifndef prepare_arch_switch
-# define prepare_arch_switch(next)	do { } while (0)
+#define prepare_arch_switch(next) \
+	do {                      \
+	} while (0)
 #endif
 
 #ifndef finish_arch_post_lock_switch
-# define finish_arch_post_lock_switch()	do { } while (0)
+#define finish_arch_post_lock_switch() \
+	do {                           \
+	} while (0)
 #endif
 
 static inline void kmap_local_sched_out(void)
@@ -4983,9 +5019,8 @@ static inline void kmap_local_sched_in(void)
  * prepare_task_switch sets up locking and calls architecture specific
  * hooks.
  */
-static inline void
-prepare_task_switch(struct rq *rq, struct task_struct *prev,
-		    struct task_struct *next)
+static inline void prepare_task_switch(struct rq *rq, struct task_struct *prev,
+				       struct task_struct *next)
 {
 	kcov_prepare_switch(prev);
 	sched_info_switch(rq, prev, next);
@@ -5034,9 +5069,9 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	 *
 	 * Also, see FORK_PREEMPT_COUNT.
 	 */
-	if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
-		      "corrupted preempt_count: %s/%d/0x%x\n",
-		      current->comm, current->pid, preempt_count()))
+	if (WARN_ONCE(preempt_count() != 2 * PREEMPT_DISABLE_OFFSET,
+		      "corrupted preempt_count: %s/%d/0x%x\n", current->comm,
+		      current->pid, preempt_count()))
 		preempt_count_set(FORK_PREEMPT_COUNT);
 
 	rq->prev_mm = NULL;
@@ -5127,9 +5162,10 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
-static __always_inline struct rq *
-context_switch(struct rq *rq, struct task_struct *prev,
-	       struct task_struct *next, struct rq_flags *rf)
+static __always_inline struct rq *context_switch(struct rq *rq,
+						 struct task_struct *prev,
+						 struct task_struct *next,
+						 struct rq_flags *rf)
 {
 	prepare_task_switch(rq, prev, next);
 
@@ -5147,15 +5183,15 @@ context_switch(struct rq *rq, struct task_struct *prev,
 	 * kernel ->   user   switch + mmdrop() active
 	 *   user ->   user   switch
 	 */
-	if (!next->mm) {                                // to kernel
+	if (!next->mm) { // to kernel
 		enter_lazy_tlb(prev->active_mm, next);
 
 		next->active_mm = prev->active_mm;
-		if (prev->mm)                           // from user
+		if (prev->mm) // from user
 			mmgrab(prev->active_mm);
 		else
 			prev->active_mm = NULL;
-	} else {                                        // to user
+	} else { // to user
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
 		 * sys_membarrier() requires an smp_mb() between setting
@@ -5167,14 +5203,14 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		 */
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 
-		if (!prev->mm) {                        // from kernel
+		if (!prev->mm) { // from kernel
 			/* will mmdrop() in finish_task_switch(). */
 			rq->prev_mm = prev->active_mm;
 			prev->active_mm = NULL;
 		}
 	}
 
-	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
+	rq->clock_update_flags &= ~(RQCF_ACT_SKIP | RQCF_REQ_SKIP);
 
 	prepare_lock_switch(rq, next, rf);
 
@@ -5426,7 +5462,10 @@ static int __init setup_resched_latency_warn_ms(char *str)
 }
 __setup("resched_latency_warn_ms=", setup_resched_latency_warn_ms);
 #else
-static inline u64 cpu_resched_latency(struct rq *rq) { return 0; }
+static inline u64 cpu_resched_latency(struct rq *rq)
+{
+	return 0;
+}
 #endif /* CONFIG_SCHED_DEBUG */
 
 /*
@@ -5472,14 +5511,14 @@ void scheduler_tick(void)
 #ifdef CONFIG_NO_HZ_FULL
 
 struct tick_work {
-	int			cpu;
-	atomic_t		state;
-	struct delayed_work	work;
+	int cpu;
+	atomic_t state;
+	struct delayed_work work;
 };
 /* Values for ->state, see diagram below. */
-#define TICK_SCHED_REMOTE_OFFLINE	0
-#define TICK_SCHED_REMOTE_OFFLINING	1
-#define TICK_SCHED_REMOTE_RUNNING	2
+#define TICK_SCHED_REMOTE_OFFLINE 0
+#define TICK_SCHED_REMOTE_OFFLINING 1
+#define TICK_SCHED_REMOTE_RUNNING 2
 
 /*
  * State diagram for ->state:
@@ -5555,7 +5594,8 @@ static void sched_tick_remote(struct work_struct *work)
 	 * to keep scheduler internal stats reasonably up to date.  But
 	 * first update state to reflect hotplug activity if required.
 	 */
-	os = atomic_fetch_add_unless(&twork->state, -1, TICK_SCHED_REMOTE_RUNNING);
+	os = atomic_fetch_add_unless(&twork->state, -1,
+				     TICK_SCHED_REMOTE_RUNNING);
 	WARN_ON_ONCE(os == TICK_SCHED_REMOTE_OFFLINE);
 	if (os == TICK_SCHED_REMOTE_RUNNING)
 		queue_delayed_work(system_unbound_wq, dwork, HZ);
@@ -5608,12 +5648,16 @@ int __init sched_tick_offload_init(void)
 }
 
 #else /* !CONFIG_NO_HZ_FULL */
-static inline void sched_tick_start(int cpu) { }
-static inline void sched_tick_stop(int cpu) { }
+static inline void sched_tick_start(int cpu)
+{
+}
+static inline void sched_tick_stop(int cpu)
+{
+}
 #endif
 
 #if defined(CONFIG_PREEMPTION) && (defined(CONFIG_DEBUG_PREEMPT) || \
-				defined(CONFIG_TRACE_PREEMPT_TOGGLE))
+				   defined(CONFIG_TRACE_PREEMPT_TOGGLE))
 /*
  * If the value passed in is equal to the current preempt count
  * then we just disabled preemption. Start timing the latency.
@@ -5644,7 +5688,7 @@ void preempt_count_add(int val)
 	 * Spinlock count overflowing soon?
 	 */
 	DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=
-				PREEMPT_MASK - 10);
+			    PREEMPT_MASK - 10);
 #endif
 	preempt_latency_start(val);
 }
@@ -5673,7 +5717,7 @@ void preempt_count_sub(int val)
 	 * Is the spinlock portion underflowing?
 	 */
 	if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
-			!(preempt_count() & PREEMPT_MASK)))
+				!(preempt_count() & PREEMPT_MASK)))
 		return;
 #endif
 
@@ -5684,8 +5728,12 @@ EXPORT_SYMBOL(preempt_count_sub);
 NOKPROBE_SYMBOL(preempt_count_sub);
 
 #else
-static inline void preempt_latency_start(int val) { }
-static inline void preempt_latency_stop(int val) { }
+static inline void preempt_latency_start(int val)
+{
+}
+static inline void preempt_latency_stop(int val)
+{
+}
 #endif
 
 static inline unsigned long get_preempt_disable_ip(struct task_struct *p)
@@ -5709,14 +5757,13 @@ static noinline void __schedule_bug(struct task_struct *prev)
 		return;
 
 	printk(KERN_ERR "BUG: scheduling while atomic: %s/%d/0x%08x\n",
-		prev->comm, prev->pid, preempt_count());
+	       prev->comm, prev->pid, preempt_count());
 
 	debug_show_held_locks(prev);
 	print_modules();
 	if (irqs_disabled())
 		print_irqtrace_events(prev);
-	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)
-	    && in_atomic_preempt_off()) {
+	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT) && in_atomic_preempt_off()) {
 		pr_err("Preemption disabled at:");
 		print_ip_sym(KERN_ERR, preempt_disable_ip);
 	}
@@ -5742,8 +5789,9 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	if (!preempt && READ_ONCE(prev->__state) && prev->non_block_count) {
-		printk(KERN_ERR "BUG: scheduling in a non-blocking section: %s/%d/%i\n",
-			prev->comm, prev->pid, prev->non_block_count);
+		printk(KERN_ERR
+		       "BUG: scheduling in a non-blocking section: %s/%d/%i\n",
+		       prev->comm, prev->pid, prev->non_block_count);
 		dump_stack();
 		add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
 	}
@@ -5774,7 +5822,8 @@ static void put_prev_task_balance(struct rq *rq, struct task_struct *prev,
 	 * We can terminate the balance pass as soon as we know there is
 	 * a runnable task of @class priority or higher.
 	 */
-	for_class_range(class, prev->sched_class, &idle_sched_class) {
+	for_class_range(class, prev->sched_class, &idle_sched_class)
+	{
 		if (class->balance(rq, prev, rf))
 			break;
 	}
@@ -5800,7 +5849,6 @@ __pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	 */
 	if (likely(!sched_class_above(prev->sched_class, &fair_sched_class) &&
 		   rq->nr_running == rq->cfs.h_nr_running)) {
-
 		p = pick_next_task_fair(rq, prev, rf);
 		if (unlikely(p == RETRY_TASK))
 			goto restart;
@@ -5817,7 +5865,8 @@ __pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 restart:
 	put_prev_task_balance(rq, prev, rf);
 
-	for_each_class(class) {
+	for_each_class(class)
+	{
 		p = class->pick_next_task(rq);
 		if (p)
 			return p;
@@ -5850,7 +5899,8 @@ static inline struct task_struct *pick_task(struct rq *rq)
 	const struct sched_class *class;
 	struct task_struct *p;
 
-	for_each_class(class) {
+	for_each_class(class)
+	{
 		p = class->pick_task(rq);
 		if (p)
 			return p;
@@ -5859,7 +5909,8 @@ static inline struct task_struct *pick_task(struct rq *rq)
 	BUG(); /* The idle class should always have a runnable task. */
 }
 
-extern void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi);
+extern void task_vruntime_update(struct rq *rq, struct task_struct *p,
+				 bool in_fi);
 
 static void queue_core_balance(struct rq *rq);
 
@@ -5901,8 +5952,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	 * selection. In this case, do a core-wide selection.
 	 */
 	if (rq->core->core_pick_seq == rq->core->core_task_seq &&
-	    rq->core->core_pick_seq != rq->core_sched_seq &&
-	    rq->core_pick) {
+	    rq->core->core_pick_seq != rq->core_sched_seq && rq->core_pick) {
 		WRITE_ONCE(rq->core_sched_seq, rq->core->core_pick_seq);
 
 		next = rq->core_pick;
@@ -6061,7 +6111,8 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 		 *  1            1       0
 		 */
 		if (!(fi_before && rq->core->core_forceidle_count))
-			task_vruntime_update(rq_i, rq_i->core_pick, !!rq->core->core_forceidle_count);
+			task_vruntime_update(rq_i, rq_i->core_pick,
+					     !!rq->core->core_forceidle_count);
 
 		rq_i->core_pick->core_occupation = occ;
 
@@ -6167,7 +6218,8 @@ static void sched_core_balance(struct rq *rq)
 	preempt_disable();
 	rcu_read_lock();
 	raw_spin_rq_unlock_irq(rq);
-	for_each_domain(cpu, sd) {
+	for_each_domain(cpu, sd)
+	{
 		if (need_resched())
 			break;
 
@@ -6192,7 +6244,8 @@ static void queue_core_balance(struct rq *rq)
 	if (!rq->nr_running) /* not forced idle */
 		return;
 
-	queue_balance_callback(rq, &per_cpu(core_balance_head, rq->cpu), sched_core_balance);
+	queue_balance_callback(rq, &per_cpu(core_balance_head, rq->cpu),
+			       sched_core_balance);
 }
 
 static void sched_core_cpu_starting(unsigned int cpu)
@@ -6269,11 +6322,11 @@ static void sched_core_cpu_deactivate(unsigned int cpu)
 		goto unlock;
 
 	/* copy the shared state to the new leader */
-	core_rq->core_task_seq             = rq->core_task_seq;
-	core_rq->core_pick_seq             = rq->core_pick_seq;
-	core_rq->core_cookie               = rq->core_cookie;
-	core_rq->core_forceidle_count      = rq->core_forceidle_count;
-	core_rq->core_forceidle_seq        = rq->core_forceidle_seq;
+	core_rq->core_task_seq = rq->core_task_seq;
+	core_rq->core_pick_seq = rq->core_pick_seq;
+	core_rq->core_cookie = rq->core_cookie;
+	core_rq->core_forceidle_count = rq->core_forceidle_count;
+	core_rq->core_forceidle_seq = rq->core_forceidle_seq;
 	core_rq->core_forceidle_occupation = rq->core_forceidle_occupation;
 
 	/*
@@ -6303,9 +6356,15 @@ static inline void sched_core_cpu_dying(unsigned int cpu)
 
 #else /* !CONFIG_SCHED_CORE */
 
-static inline void sched_core_cpu_starting(unsigned int cpu) {}
-static inline void sched_core_cpu_deactivate(unsigned int cpu) {}
-static inline void sched_core_cpu_dying(unsigned int cpu) {}
+static inline void sched_core_cpu_starting(unsigned int cpu)
+{
+}
+static inline void sched_core_cpu_deactivate(unsigned int cpu)
+{
+}
+static inline void sched_core_cpu_dying(unsigned int cpu)
+{
+}
 
 static struct task_struct *
 pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
@@ -6323,14 +6382,14 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  * SM_MASK_PREEMPT for !RT has all bits set, which allows the compiler to
  * optimize the AND operation out and just check for zero.
  */
-#define SM_NONE			0x0
-#define SM_PREEMPT		0x1
-#define SM_RTLOCK_WAIT		0x2
+#define SM_NONE 0x0
+#define SM_PREEMPT 0x1
+#define SM_RTLOCK_WAIT 0x2
 
 #ifndef CONFIG_PREEMPT_RT
-# define SM_MASK_PREEMPT	(~0U)
+#define SM_MASK_PREEMPT (~0U)
 #else
-# define SM_MASK_PREEMPT	SM_PREEMPT
+#define SM_MASK_PREEMPT SM_PREEMPT
 #endif
 
 /*
@@ -6445,7 +6504,8 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 			 *
 			 * After this, schedule() must not care about p->state any more.
 			 */
-			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
+			deactivate_task(rq, prev,
+					DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
 
 			if (prev->in_iowait) {
 				atomic_inc(&rq->nr_iowait);
@@ -6488,12 +6548,13 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 		migrate_disable_switch(rq, prev);
 		psi_sched_switch(prev, next, !task_on_rq_queued(prev));
 
-		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next, prev_state);
+		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next,
+				   prev_state);
 
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
-		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
+		rq->clock_update_flags &= ~(RQCF_ACT_SKIP | RQCF_REQ_SKIP);
 
 		rq_unpin_lock(rq, &rf);
 		__balance_callbacks(rq);
@@ -6599,7 +6660,8 @@ void __sched schedule_idle(void)
 	} while (need_resched());
 }
 
-#if defined(CONFIG_CONTEXT_TRACKING_USER) && !defined(CONFIG_HAVE_CONTEXT_TRACKING_USER_OFFSTACK)
+#if defined(CONFIG_CONTEXT_TRACKING_USER) && \
+	!defined(CONFIG_HAVE_CONTEXT_TRACKING_USER_OFFSTACK)
 asmlinkage __visible void __sched schedule_user(void)
 {
 	/*
@@ -6692,8 +6754,8 @@ EXPORT_SYMBOL(preempt_schedule);
 #ifdef CONFIG_PREEMPT_DYNAMIC
 #if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)
 #ifndef preempt_schedule_dynamic_enabled
-#define preempt_schedule_dynamic_enabled	preempt_schedule
-#define preempt_schedule_dynamic_disabled	NULL
+#define preempt_schedule_dynamic_enabled preempt_schedule
+#define preempt_schedule_dynamic_disabled NULL
 #endif
 DEFINE_STATIC_CALL(preempt_schedule, preempt_schedule_dynamic_enabled);
 EXPORT_STATIC_CALL_TRAMP(preempt_schedule);
@@ -6765,10 +6827,11 @@ EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
 #ifdef CONFIG_PREEMPT_DYNAMIC
 #if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)
 #ifndef preempt_schedule_notrace_dynamic_enabled
-#define preempt_schedule_notrace_dynamic_enabled	preempt_schedule_notrace
-#define preempt_schedule_notrace_dynamic_disabled	NULL
+#define preempt_schedule_notrace_dynamic_enabled preempt_schedule_notrace
+#define preempt_schedule_notrace_dynamic_disabled NULL
 #endif
-DEFINE_STATIC_CALL(preempt_schedule_notrace, preempt_schedule_notrace_dynamic_enabled);
+DEFINE_STATIC_CALL(preempt_schedule_notrace,
+		   preempt_schedule_notrace_dynamic_enabled);
 EXPORT_STATIC_CALL_TRAMP(preempt_schedule_notrace);
 #elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)
 static DEFINE_STATIC_KEY_TRUE(sk_dynamic_preempt_schedule_notrace);
@@ -6811,8 +6874,8 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 	exception_exit(prev_state);
 }
 
-int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags,
-			  void *key)
+int default_wake_function(wait_queue_entry_t *curr, unsigned mode,
+			  int wake_flags, void *key)
 {
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_SCHED_DEBUG) && wake_flags & ~WF_SYNC);
 	return try_to_wake_up(curr->private, mode, wake_flags);
@@ -6825,10 +6888,14 @@ static void __setscheduler_prio(struct task_struct *p, int prio)
 		p->sched_class = &dl_sched_class;
 	else if (rt_prio(prio))
 		p->sched_class = &rt_sched_class;
+	else if(rsdl_prio(prio) && rsdl_policy(p->policy))
+		p->sched_class = &rsdl_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 
 	p->prio = prio;
+	// printk("Priority Set to : %d", p->prio);
+	
 }
 
 #ifdef CONFIG_RT_MUTEXES
@@ -6861,8 +6928,8 @@ static inline int rt_effective_prio(struct task_struct *p, int prio)
  */
 void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 {
-	int prio, oldprio, queued, running, queue_flag =
-		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
+	int prio, oldprio, queued, running,
+		queue_flag = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 	const struct sched_class *prev_class;
 	struct rq_flags rf;
 	struct rq *rq;
@@ -7189,16 +7256,15 @@ struct task_struct *idle_task(int cpu)
  * required to meet deadlines.
  */
 unsigned long effective_cpu_util(int cpu, unsigned long util_cfs,
-				 enum cpu_util_type type,
-				 struct task_struct *p)
+				 enum cpu_util_type type, struct task_struct *p)
 {
 	unsigned long dl_util, util, irq, max;
 	struct rq *rq = cpu_rq(cpu);
 
 	max = arch_scale_cpu_capacity(cpu);
 
-	if (!uclamp_is_used() &&
-	    type == FREQUENCY_UTIL && rt_rq_is_runnable(&rq->rt)) {
+	if (!uclamp_is_used() && type == FREQUENCY_UTIL &&
+	    rt_rq_is_runnable(&rq->rt)) {
 		return max;
 	}
 
@@ -7297,10 +7363,10 @@ static struct task_struct *find_process_by_pid(pid_t pid)
  * sched_setparam() passes in -1 for its policy, to let the functions
  * it calls know not to change it.
  */
-#define SETPARAM_POLICY	-1
+#define SETPARAM_POLICY -1
 
 static void __setscheduler_params(struct task_struct *p,
-		const struct sched_attr *attr)
+				  const struct sched_attr *attr)
 {
 	int policy = attr->sched_policy;
 
@@ -7319,9 +7385,12 @@ static void __setscheduler_params(struct task_struct *p,
 	 * !rt_policy. Always setting this ensures that things like
 	 * getparam()/getattr() don't report silly values for !rt tasks.
 	 */
+
 	p->rt_priority = attr->sched_priority;
 	p->normal_prio = normal_prio(p);
 	set_load_weight(p, true);
+	// printk("Policy Set to : %d" , p->policy);
+
 }
 
 /*
@@ -7404,8 +7473,8 @@ static int user_check_sched_setscheduler(struct task_struct *p,
 }
 
 static int __sched_setscheduler(struct task_struct *p,
-				const struct sched_attr *attr,
-				bool user, bool pi)
+				const struct sched_attr *attr, bool user,
+				bool pi)
 {
 	int oldpolicy = -1, policy = attr->sched_policy;
 	int retval, oldprio, newprio, queued, running;
@@ -7415,7 +7484,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 	struct rq *rq;
-
+	// printk("Parameters Passed : %d \t", attr->sched_policy);
 	/* The pi code expects interrupts enabled */
 	BUG_ON(pi && in_interrupt());
 recheck:
@@ -7424,7 +7493,8 @@ static int __sched_setscheduler(struct task_struct *p,
 		reset_on_fork = p->sched_reset_on_fork;
 		policy = oldpolicy = p->policy;
 	} else {
-		reset_on_fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);
+		reset_on_fork =
+			!!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);
 
 		if (!valid_policy(policy))
 			return -EINVAL;
@@ -7438,14 +7508,15 @@ static int __sched_setscheduler(struct task_struct *p,
 	 * 1..MAX_RT_PRIO-1, valid priority for SCHED_NORMAL,
 	 * SCHED_BATCH and SCHED_IDLE is 0.
 	 */
-	if (attr->sched_priority > MAX_RT_PRIO-1)
+	if (attr->sched_priority > MAX_RT_PRIO - 1)
 		return -EINVAL;
 	if ((dl_policy(policy) && !__checkparam_dl(attr)) ||
 	    (rt_policy(policy) != (attr->sched_priority != 0)))
 		return -EINVAL;
 
 	if (user) {
-		retval = user_check_sched_setscheduler(p, attr, policy, reset_on_fork);
+		retval = user_check_sched_setscheduler(p, attr, policy,
+						       reset_on_fork);
 		if (retval)
 			return retval;
 
@@ -7512,15 +7583,15 @@ static int __sched_setscheduler(struct task_struct *p,
 		 * assigned.
 		 */
 		if (rt_bandwidth_enabled() && rt_policy(policy) &&
-				task_group(p)->rt_bandwidth.rt_runtime == 0 &&
-				!task_group_is_autogroup(task_group(p))) {
+		    task_group(p)->rt_bandwidth.rt_runtime == 0 &&
+		    !task_group_is_autogroup(task_group(p))) {
 			retval = -EPERM;
 			goto unlock;
 		}
 #endif
 #ifdef CONFIG_SMP
 		if (dl_bandwidth_enabled() && dl_policy(policy) &&
-				!(attr->sched_flags & SCHED_FLAG_SUGOV)) {
+		    !(attr->sched_flags & SCHED_FLAG_SUGOV)) {
 			cpumask_t *span = rq->rd->span;
 
 			/*
@@ -7551,7 +7622,8 @@ static int __sched_setscheduler(struct task_struct *p,
 	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth
 	 * is available.
 	 */
-	if ((dl_policy(policy) || dl_task(p)) && sched_dl_overflow(p, policy, attr)) {
+	if ((dl_policy(policy) || dl_task(p)) &&
+	    sched_dl_overflow(p, policy, attr)) {
 		retval = -EBUSY;
 		goto unlock;
 	}
@@ -7559,7 +7631,10 @@ static int __sched_setscheduler(struct task_struct *p,
 	p->sched_reset_on_fork = reset_on_fork;
 	oldprio = p->prio;
 
+	// printk("Priority before normal prio : %d\n", newprio);
 	newprio = __normal_prio(policy, attr->sched_priority, attr->sched_nice);
+	// printk("Priority after normal prio : %d\n", newprio);
+
 	if (pi) {
 		/*
 		 * Take priority boosted tasks into account. If the new
@@ -7572,6 +7647,7 @@ static int __sched_setscheduler(struct task_struct *p,
 		if (newprio == oldprio)
 			queue_flags &= ~DEQUEUE_MOVE;
 	}
+	// printk("Priority after pi : %d\n", newprio);
 
 	queued = task_on_rq_queued(p);
 	running = task_current(rq, p);
@@ -7581,11 +7657,14 @@ static int __sched_setscheduler(struct task_struct *p,
 		put_prev_task(rq, p);
 
 	prev_class = p->sched_class;
+	
 
 	if (!(attr->sched_flags & SCHED_FLAG_KEEP_PARAMS)) {
 		__setscheduler_params(p, attr);
 		__setscheduler_prio(p, newprio);
 	}
+	// printk("Priority after everything set : %d", p->prio);
+
 	__setscheduler_uclamp(p, attr);
 
 	if (queued) {
@@ -7630,9 +7709,9 @@ static int _sched_setscheduler(struct task_struct *p, int policy,
 			       const struct sched_param *param, bool check)
 {
 	struct sched_attr attr = {
-		.sched_policy   = policy,
+		.sched_policy = policy,
 		.sched_priority = param->sched_priority,
-		.sched_nice	= PRIO_TO_NICE(p->static_prio),
+		.sched_nice = PRIO_TO_NICE(p->static_prio),
 	};
 
 	/* Fixup the legacy SCHED_RESET_ON_FORK hack. */
@@ -7737,8 +7816,8 @@ void sched_set_normal(struct task_struct *p, int nice)
 }
 EXPORT_SYMBOL_GPL(sched_set_normal);
 
-static int
-do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
+static int do_sched_setscheduler(pid_t pid, int policy,
+				 struct sched_param __user *param)
 {
 	struct sched_param lparam;
 	struct task_struct *p;
@@ -7767,7 +7846,8 @@ do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
 /*
  * Mimics kernel/events/core.c perf_copy_attr().
  */
-static int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *attr)
+static int sched_copy_attr(struct sched_attr __user *uattr,
+			   struct sched_attr *attr)
 {
 	u32 size;
 	int ret;
@@ -7827,7 +7907,8 @@ static void get_params(struct task_struct *p, struct sched_attr *attr)
  *
  * Return: 0 on success. An error code otherwise.
  */
-SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param)
+SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,
+		struct sched_param __user *, param)
 {
 	if (policy < 0)
 		return -EINVAL;
@@ -7854,7 +7935,7 @@ SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)
  * @flags: for future extension.
  */
 SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,
-			       unsigned int, flags)
+		unsigned int, flags)
 {
 	struct sched_attr attr;
 	struct task_struct *p;
@@ -7910,8 +7991,9 @@ SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)
 	if (p) {
 		retval = security_task_getscheduler(p);
 		if (!retval)
-			retval = p->policy
-				| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);
+			retval = p->policy |
+				 (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK :
+							   0);
 	}
 	rcu_read_unlock();
 	return retval;
@@ -7968,10 +8050,8 @@ SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)
  * smaller than the kernel-space buffer. The usual case is that both
  * have the same size.
  */
-static int
-sched_attr_copy_to_user(struct sched_attr __user *uattr,
-			struct sched_attr *kattr,
-			unsigned int usize)
+static int sched_attr_copy_to_user(struct sched_attr __user *uattr,
+				   struct sched_attr *kattr, unsigned int usize)
 {
 	unsigned int ksize = sizeof(*kattr);
 
@@ -8009,7 +8089,7 @@ sched_attr_copy_to_user(struct sched_attr __user *uattr,
 SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr,
 		unsigned int, usize, unsigned int, flags)
 {
-	struct sched_attr kattr = { };
+	struct sched_attr kattr = {};
 	struct task_struct *p;
 	int retval;
 
@@ -8078,8 +8158,8 @@ int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)
 }
 #endif
 
-static int
-__sched_setaffinity(struct task_struct *p, const struct cpumask *mask)
+static int __sched_setaffinity(struct task_struct *p,
+			       const struct cpumask *mask)
 {
 	int retval;
 	cpumask_var_t cpus_allowed, new_mask;
@@ -8241,7 +8321,7 @@ SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,
 
 	if ((len * BITS_PER_BYTE) < nr_cpu_ids)
 		return -EINVAL;
-	if (len & (sizeof(unsigned long)-1))
+	if (len & (sizeof(unsigned long) - 1))
 		return -EINVAL;
 
 	if (!alloc_cpumask_var(&mask, GFP_KERNEL))
@@ -8320,13 +8400,13 @@ EXPORT_SYMBOL(__cond_resched);
 
 #ifdef CONFIG_PREEMPT_DYNAMIC
 #if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)
-#define cond_resched_dynamic_enabled	__cond_resched
-#define cond_resched_dynamic_disabled	((void *)&__static_call_return0)
+#define cond_resched_dynamic_enabled __cond_resched
+#define cond_resched_dynamic_disabled ((void *)&__static_call_return0)
 DEFINE_STATIC_CALL_RET0(cond_resched, __cond_resched);
 EXPORT_STATIC_CALL_TRAMP(cond_resched);
 
-#define might_resched_dynamic_enabled	__cond_resched
-#define might_resched_dynamic_disabled	((void *)&__static_call_return0)
+#define might_resched_dynamic_enabled __cond_resched
+#define might_resched_dynamic_disabled ((void *)&__static_call_return0)
 DEFINE_STATIC_CALL_RET0(might_resched, __cond_resched);
 EXPORT_STATIC_CALL_TRAMP(might_resched);
 #elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)
@@ -8472,11 +8552,11 @@ int sched_dynamic_mode(const char *str)
 }
 
 #if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)
-#define preempt_dynamic_enable(f)	static_call_update(f, f##_dynamic_enabled)
-#define preempt_dynamic_disable(f)	static_call_update(f, f##_dynamic_disabled)
+#define preempt_dynamic_enable(f) static_call_update(f, f##_dynamic_enabled)
+#define preempt_dynamic_disable(f) static_call_update(f, f##_dynamic_disabled)
 #elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)
-#define preempt_dynamic_enable(f)	static_key_enable(&sk_dynamic_##f.key)
-#define preempt_dynamic_disable(f)	static_key_disable(&sk_dynamic_##f.key)
+#define preempt_dynamic_enable(f) static_key_enable(&sk_dynamic_##f.key)
+#define preempt_dynamic_disable(f) static_key_disable(&sk_dynamic_##f.key)
 #else
 #error "Unsupported PREEMPT_DYNAMIC mechanism"
 #endif
@@ -8554,12 +8634,13 @@ static void __init preempt_dynamic_init(void)
 	}
 }
 
-#define PREEMPT_MODEL_ACCESSOR(mode) \
-	bool preempt_model_##mode(void)						 \
-	{									 \
-		WARN_ON_ONCE(preempt_dynamic_mode == preempt_dynamic_undefined); \
-		return preempt_dynamic_mode == preempt_dynamic_##mode;		 \
-	}									 \
+#define PREEMPT_MODEL_ACCESSOR(mode)                                   \
+	bool preempt_model_##mode(void)                                \
+	{                                                              \
+		WARN_ON_ONCE(preempt_dynamic_mode ==                   \
+			     preempt_dynamic_undefined);               \
+		return preempt_dynamic_mode == preempt_dynamic_##mode; \
+	}                                                              \
 	EXPORT_SYMBOL_GPL(preempt_model_##mode)
 
 PREEMPT_MODEL_ACCESSOR(none);
@@ -8568,7 +8649,9 @@ PREEMPT_MODEL_ACCESSOR(full);
 
 #else /* !CONFIG_PREEMPT_DYNAMIC */
 
-static inline void preempt_dynamic_init(void) { }
+static inline void preempt_dynamic_init(void)
+{
+}
 
 #endif /* #ifdef CONFIG_PREEMPT_DYNAMIC */
 
@@ -8731,7 +8814,7 @@ SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 	switch (policy) {
 	case SCHED_FIFO:
 	case SCHED_RR:
-		ret = MAX_RT_PRIO-1;
+		ret = MAX_RT_PRIO - 1;
 		break;
 	case SCHED_DEADLINE:
 	case SCHED_NORMAL:
@@ -8861,9 +8944,8 @@ void sched_show_task(struct task_struct *p)
 	if (pid_alive(p))
 		ppid = task_pid_nr(rcu_dereference(p->real_parent));
 	rcu_read_unlock();
-	pr_cont(" stack:%5lu pid:%5d ppid:%6d flags:0x%08lx\n",
-		free, task_pid_nr(p), ppid,
-		read_task_thread_flags(p));
+	pr_cont(" stack:%5lu pid:%5d ppid:%6d flags:0x%08lx\n", free,
+		task_pid_nr(p), ppid, read_task_thread_flags(p));
 
 	print_worker_info(KERN_INFO, p);
 	print_stop_info(KERN_INFO, p);
@@ -8872,8 +8954,8 @@ void sched_show_task(struct task_struct *p)
 }
 EXPORT_SYMBOL_GPL(sched_show_task);
 
-static inline bool
-state_filter_match(unsigned long state_filter, struct task_struct *p)
+static inline bool state_filter_match(unsigned long state_filter,
+				      struct task_struct *p)
 {
 	unsigned int state = READ_ONCE(p->__state);
 
@@ -8895,7 +8977,6 @@ state_filter_match(unsigned long state_filter, struct task_struct *p)
 	return true;
 }
 
-
 void show_state_filter(unsigned int state_filter)
 {
 	struct task_struct *g, *p;
@@ -9034,8 +9115,8 @@ int task_can_attach(struct task_struct *p,
 		goto out;
 	}
 
-	if (dl_task(p) && !cpumask_intersects(task_rq(p)->rd->span,
-					      cs_effective_cpus)) {
+	if (dl_task(p) &&
+	    !cpumask_intersects(task_rq(p)->rd->span, cs_effective_cpus)) {
 		int cpu = cpumask_any_and(cpu_active_mask, cs_effective_cpus);
 
 		if (unlikely(cpu >= nr_cpu_ids))
@@ -9172,9 +9253,7 @@ static void balance_push(struct rq *rq)
 	 * Both the cpu-hotplug and stop task are in this case and are
 	 * required to complete the hotplug process.
 	 */
-	if (kthread_is_per_cpu(push_task) ||
-	    is_migration_disabled(push_task)) {
-
+	if (kthread_is_per_cpu(push_task) || is_migration_disabled(push_task)) {
 		/*
 		 * If this is the idle task on the outgoing CPU try to wake
 		 * up the hotplug control thread which might wait for the
@@ -9265,7 +9344,8 @@ void set_rq_online(struct rq *rq)
 		cpumask_set_cpu(rq->cpu, rq->rd->online);
 		rq->online = 1;
 
-		for_each_class(class) {
+		for_each_class(class)
+		{
 			if (class->rq_online)
 				class->rq_online(rq);
 		}
@@ -9277,7 +9357,8 @@ void set_rq_offline(struct rq *rq)
 	if (rq->online) {
 		const struct sched_class *class;
 
-		for_each_class(class) {
+		for_each_class(class)
+		{
 			if (class->rq_offline)
 				class->rq_offline(rq);
 		}
@@ -9508,7 +9589,8 @@ static void dump_rq_tasks(struct rq *rq, const char *loglvl)
 
 	lockdep_assert_rq_held(rq);
 
-	printk("%sCPU%d enqueued tasks (%u total):\n", loglvl, cpu, rq->nr_running);
+	printk("%sCPU%d enqueued tasks (%u total):\n", loglvl, cpu,
+	       rq->nr_running);
 	for_each_process_thread(g, p) {
 		if (task_cpu(p) != cpu)
 			continue;
@@ -9557,7 +9639,8 @@ void __init sched_init_smp(void)
 	mutex_unlock(&sched_domains_mutex);
 
 	/* Move init over to a non-isolated CPU */
-	if (set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_DOMAIN)) < 0)
+	if (set_cpus_allowed_ptr(current,
+				 housekeeping_cpumask(HK_TYPE_DOMAIN)) < 0)
 		BUG();
 	current->flags &= ~PF_NO_SETAFFINITY;
 	sched_init_granularity();
@@ -9585,8 +9668,8 @@ void __init sched_init_smp(void)
 int in_sched_functions(unsigned long addr)
 {
 	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
+	       (addr >= (unsigned long)__sched_text_start &&
+		addr < (unsigned long)__sched_text_end);
 }
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -9611,8 +9694,9 @@ void __init sched_init(void)
 
 	/* Make sure the linker didn't screw up */
 	BUG_ON(&idle_sched_class != &fair_sched_class + 1 ||
-	       &fair_sched_class != &rt_sched_class + 1 ||
-	       &rt_sched_class   != &dl_sched_class + 1);
+	       &fair_sched_class != &rsdl_sched_class + 1||
+	       &rsdl_sched_class != &rt_sched_class + 1 ||
+	       &rt_sched_class != &dl_sched_class + 1);
 #ifdef CONFIG_SMP
 	BUG_ON(&dl_sched_class != &stop_sched_class + 1);
 #endif
@@ -9656,15 +9740,16 @@ void __init sched_init(void)
 	}
 #endif /* CONFIG_CPUMASK_OFFSTACK */
 
-	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());
+	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(),
+			  global_rt_runtime());
 
 #ifdef CONFIG_SMP
 	init_defrootdomain();
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	init_rt_bandwidth(&root_task_group.rt_bandwidth,
-			global_rt_period(), global_rt_runtime());
+	init_rt_bandwidth(&root_task_group.rt_bandwidth, global_rt_period(),
+			  global_rt_runtime());
 #endif /* CONFIG_RT_GROUP_SCHED */
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -9686,6 +9771,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt);
+		init_rsdl_rq(&rq->rsdl);
 		init_dl_rq(&rq->dl);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
@@ -9727,7 +9813,7 @@ void __init sched_init(void)
 		rq->cpu = i;
 		rq->online = 0;
 		rq->idle_stamp = 0;
-		rq->avg_idle = 2*sysctl_sched_migration_cost;
+		rq->avg_idle = 2 * sysctl_sched_migration_cost;
 		rq->wake_stamp = jiffies;
 		rq->wake_avg_idle = rq->avg_idle;
 		rq->max_idle_balance_cost = sysctl_sched_migration_cost;
@@ -9792,6 +9878,7 @@ void __init sched_init(void)
 	balance_push_set(smp_processor_id(), false);
 #endif
 	init_sched_fair_class();
+	init_sched_rsdl_class();
 
 	psi_init();
 
@@ -9813,10 +9900,10 @@ void __might_sleep(const char *file, int line)
 	 * otherwise we will destroy state.
 	 */
 	WARN_ONCE(state != TASK_RUNNING && current->task_state_change,
-			"do not call blocking ops when !TASK_RUNNING; "
-			"state=%x set at [<%p>] %pS\n", state,
-			(void *)current->task_state_change,
-			(void *)current->task_state_change);
+		  "do not call blocking ops when !TASK_RUNNING; "
+		  "state=%x set at [<%p>] %pS\n",
+		  state, (void *)current->task_state_change,
+		  (void *)current->task_state_change);
 
 	__might_resched(file, line, 0);
 }
@@ -9912,9 +9999,9 @@ void __cant_sleep(const char *file, int line, int preempt_offset)
 	prev_jiffy = jiffies;
 
 	printk(KERN_ERR "BUG: assuming atomic context at %s:%d\n", file, line);
-	printk(KERN_ERR "in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\n",
-			in_atomic(), irqs_disabled(),
-			current->pid, current->comm);
+	printk(KERN_ERR
+	       "in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\n",
+	       in_atomic(), irqs_disabled(), current->pid, current->comm);
 
 	debug_show_held_locks(current);
 	dump_stack();
@@ -9973,7 +10060,7 @@ void normalize_rt_tasks(void)
 			continue;
 
 		p->se.exec_start = 0;
-		schedstat_set(p->stats.wait_start,  0);
+		schedstat_set(p->stats.wait_start, 0);
 		schedstat_set(p->stats.sleep_start, 0);
 		schedstat_set(p->stats.block_start, 0);
 
@@ -10053,9 +10140,10 @@ static inline void alloc_uclamp_sched_group(struct task_group *tg,
 #ifdef CONFIG_UCLAMP_TASK_GROUP
 	enum uclamp_id clamp_id;
 
-	for_each_clamp_id(clamp_id) {
-		uclamp_se_set(&tg->uclamp_req[clamp_id],
-			      uclamp_none(clamp_id), false);
+	for_each_clamp_id(clamp_id)
+	{
+		uclamp_se_set(&tg->uclamp_req[clamp_id], uclamp_none(clamp_id),
+			      false);
 		tg->uclamp[clamp_id] = parent->uclamp[clamp_id];
 	}
 #endif
@@ -10194,8 +10282,8 @@ static void sched_change_group(struct task_struct *tsk, int type)
  */
 void sched_move_task(struct task_struct *tsk)
 {
-	int queued, running, queue_flags =
-		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
+	int queued, running,
+		queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 	struct rq_flags rf;
 	struct rq *rq;
 
@@ -10359,10 +10447,11 @@ static void cpu_util_update_eff(struct cgroup_subsys_state *css)
 	SCHED_WARN_ON(!rcu_read_lock_held());
 
 	css_for_each_descendant_pre(css, top_css) {
-		uc_parent = css_tg(css)->parent
-			? css_tg(css)->parent->uclamp : NULL;
+		uc_parent = css_tg(css)->parent ? css_tg(css)->parent->uclamp :
+						  NULL;
 
-		for_each_clamp_id(clamp_id) {
+		for_each_clamp_id(clamp_id)
+		{
 			/* Assume effective clamps matches requested clamps */
 			eff[clamp_id] = css_tg(css)->uclamp_req[clamp_id].value;
 			/* Cap effective clamps with parent's effective clamps */
@@ -10377,11 +10466,13 @@ static void cpu_util_update_eff(struct cgroup_subsys_state *css)
 		/* Propagate most restrictive effective clamps */
 		clamps = 0x0;
 		uc_se = css_tg(css)->uclamp;
-		for_each_clamp_id(clamp_id) {
+		for_each_clamp_id(clamp_id)
+		{
 			if (eff[clamp_id] == uc_se[clamp_id].value)
 				continue;
 			uc_se[clamp_id].value = eff[clamp_id];
-			uc_se[clamp_id].bucket_id = uclamp_bucket_id(eff[clamp_id]);
+			uc_se[clamp_id].bucket_id =
+				uclamp_bucket_id(eff[clamp_id]);
 			clamps |= (0x1 << clamp_id);
 		}
 		if (!clamps) {
@@ -10403,15 +10494,14 @@ static void cpu_util_update_eff(struct cgroup_subsys_state *css)
 #define POW10(exp) _POW10(exp)
 
 struct uclamp_request {
-#define UCLAMP_PERCENT_SHIFT	2
-#define UCLAMP_PERCENT_SCALE	(100 * POW10(UCLAMP_PERCENT_SHIFT))
+#define UCLAMP_PERCENT_SHIFT 2
+#define UCLAMP_PERCENT_SCALE (100 * POW10(UCLAMP_PERCENT_SHIFT))
 	s64 percent;
 	u64 util;
 	int ret;
 };
 
-static inline struct uclamp_request
-capacity_from_percent(char *buf)
+static inline struct uclamp_request capacity_from_percent(char *buf)
 {
 	struct uclamp_request req = {
 		.percent = UCLAMP_PERCENT_SCALE,
@@ -10431,7 +10521,8 @@ capacity_from_percent(char *buf)
 		}
 
 		req.util = req.percent << SCHED_CAPACITY_SHIFT;
-		req.util = DIV_ROUND_CLOSEST_ULL(req.util, UCLAMP_PERCENT_SCALE);
+		req.util =
+			DIV_ROUND_CLOSEST_ULL(req.util, UCLAMP_PERCENT_SCALE);
 	}
 
 	return req;
@@ -10472,16 +10563,14 @@ static ssize_t cpu_uclamp_write(struct kernfs_open_file *of, char *buf,
 	return nbytes;
 }
 
-static ssize_t cpu_uclamp_min_write(struct kernfs_open_file *of,
-				    char *buf, size_t nbytes,
-				    loff_t off)
+static ssize_t cpu_uclamp_min_write(struct kernfs_open_file *of, char *buf,
+				    size_t nbytes, loff_t off)
 {
 	return cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MIN);
 }
 
-static ssize_t cpu_uclamp_max_write(struct kernfs_open_file *of,
-				    char *buf, size_t nbytes,
-				    loff_t off)
+static ssize_t cpu_uclamp_max_write(struct kernfs_open_file *of, char *buf,
+				    size_t nbytes, loff_t off)
 {
 	return cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MAX);
 }
@@ -10536,7 +10625,7 @@ static u64 cpu_shares_read_u64(struct cgroup_subsys_state *css,
 {
 	struct task_group *tg = css_tg(css);
 
-	return (u64) scale_load_down(tg->shares);
+	return (u64)scale_load_down(tg->shares);
 }
 
 #ifdef CONFIG_CFS_BANDWIDTH
@@ -10580,8 +10669,8 @@ static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota,
 	if (quota != RUNTIME_INF && quota > max_cfs_runtime)
 		return -EINVAL;
 
-	if (quota != RUNTIME_INF && (burst > quota ||
-				     burst + quota > max_cfs_runtime))
+	if (quota != RUNTIME_INF &&
+	    (burst > quota || burst + quota > max_cfs_runtime))
 		return -EINVAL;
 
 	/*
@@ -10803,7 +10892,8 @@ static int tg_cfs_schedulable_down(struct task_group *tg, void *data)
 		} else {
 			if (quota == RUNTIME_INF)
 				quota = parent_quota;
-			else if (parent_quota != RUNTIME_INF && quota > parent_quota)
+			else if (parent_quota != RUNTIME_INF &&
+				 quota > parent_quota)
 				return -EINVAL;
 		}
 	}
@@ -10891,13 +10981,13 @@ static u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css,
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 static s64 cpu_idle_read_s64(struct cgroup_subsys_state *css,
-			       struct cftype *cft)
+			     struct cftype *cft)
 {
 	return css_tg(css)->idle;
 }
 
 static int cpu_idle_write_s64(struct cgroup_subsys_state *css,
-				struct cftype *cft, s64 idle)
+			      struct cftype *cft, s64 idle)
 {
 	return sched_group_set_idle(css_tg(css), idle);
 }
@@ -10963,7 +11053,7 @@ static struct cftype cpu_legacy_files[] = {
 		.write = cpu_uclamp_max_write,
 	},
 #endif
-	{ }	/* Terminate */
+	{} /* Terminate */
 };
 
 static int cpu_extra_stat_show(struct seq_file *sf,
@@ -10980,7 +11070,8 @@ static int cpu_extra_stat_show(struct seq_file *sf,
 		burst_usec = cfs_b->burst_time;
 		do_div(burst_usec, NSEC_PER_USEC);
 
-		seq_printf(sf, "nr_periods %d\n"
+		seq_printf(sf,
+			   "nr_periods %d\n"
 			   "nr_throttled %d\n"
 			   "throttled_usec %llu\n"
 			   "nr_bursts %d\n"
@@ -11067,10 +11158,10 @@ static void __maybe_unused cpu_period_quota_print(struct seq_file *sf,
 }
 
 /* caller should put the current value in *@periodp before calling */
-static int __maybe_unused cpu_period_quota_parse(char *buf,
-						 u64 *periodp, u64 *quotap)
+static int __maybe_unused cpu_period_quota_parse(char *buf, u64 *periodp,
+						 u64 *quotap)
 {
-	char tok[21];	/* U64_MAX */
+	char tok[21]; /* U64_MAX */
 
 	if (sscanf(buf, "%20s %llu", tok, periodp) < 1)
 		return -EINVAL;
@@ -11096,8 +11187,8 @@ static int cpu_max_show(struct seq_file *sf, void *v)
 	return 0;
 }
 
-static ssize_t cpu_max_write(struct kernfs_open_file *of,
-			     char *buf, size_t nbytes, loff_t off)
+static ssize_t cpu_max_write(struct kernfs_open_file *of, char *buf,
+			     size_t nbytes, loff_t off)
 {
 	struct task_group *tg = css_tg(of_css(of));
 	u64 period = tg_get_cfs_period(tg);
@@ -11161,25 +11252,25 @@ static struct cftype cpu_files[] = {
 		.write = cpu_uclamp_max_write,
 	},
 #endif
-	{ }	/* terminate */
+	{} /* terminate */
 };
 
 struct cgroup_subsys cpu_cgrp_subsys = {
-	.css_alloc	= cpu_cgroup_css_alloc,
-	.css_online	= cpu_cgroup_css_online,
-	.css_released	= cpu_cgroup_css_released,
-	.css_free	= cpu_cgroup_css_free,
+	.css_alloc = cpu_cgroup_css_alloc,
+	.css_online = cpu_cgroup_css_online,
+	.css_released = cpu_cgroup_css_released,
+	.css_free = cpu_cgroup_css_free,
 	.css_extra_stat_show = cpu_extra_stat_show,
-	.fork		= cpu_cgroup_fork,
-	.can_attach	= cpu_cgroup_can_attach,
-	.attach		= cpu_cgroup_attach,
-	.legacy_cftypes	= cpu_legacy_files,
-	.dfl_cftypes	= cpu_files,
-	.early_init	= true,
-	.threaded	= true,
+	.fork = cpu_cgroup_fork,
+	.can_attach = cpu_cgroup_can_attach,
+	.attach = cpu_cgroup_attach,
+	.legacy_cftypes = cpu_legacy_files,
+	.dfl_cftypes = cpu_files,
+	.early_init = true,
+	.threaded = true,
 };
 
-#endif	/* CONFIG_CGROUP_SCHED */
+#endif /* CONFIG_CGROUP_SCHED */
 
 void dump_cpu_task(int cpu)
 {
@@ -11200,14 +11291,14 @@ void dump_cpu_task(int cpu)
  * the relative distance between them is ~25%.)
  */
 const int sched_prio_to_weight[40] = {
- /* -20 */     88761,     71755,     56483,     46273,     36291,
- /* -15 */     29154,     23254,     18705,     14949,     11916,
- /* -10 */      9548,      7620,      6100,      4904,      3906,
- /*  -5 */      3121,      2501,      1991,      1586,      1277,
- /*   0 */      1024,       820,       655,       526,       423,
- /*   5 */       335,       272,       215,       172,       137,
- /*  10 */       110,        87,        70,        56,        45,
- /*  15 */        36,        29,        23,        18,        15,
+	/* -20 */ 88761, 71755, 56483, 46273, 36291,
+	/* -15 */ 29154, 23254, 18705, 14949, 11916,
+	/* -10 */ 9548,	 7620,	6100,  4904,  3906,
+	/*  -5 */ 3121,	 2501,	1991,  1586,  1277,
+	/*   0 */ 1024,	 820,	655,   526,   423,
+	/*   5 */ 335,	 272,	215,   172,   137,
+	/*  10 */ 110,	 87,	70,    56,    45,
+	/*  15 */ 36,	 29,	23,    18,    15,
 };
 
 /*
@@ -11218,17 +11309,49 @@ const int sched_prio_to_weight[40] = {
  * into multiplications:
  */
 const u32 sched_prio_to_wmult[40] = {
- /* -20 */     48388,     59856,     76040,     92818,    118348,
- /* -15 */    147320,    184698,    229616,    287308,    360437,
- /* -10 */    449829,    563644,    704093,    875809,   1099582,
- /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
- /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
- /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
- /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
- /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
+	/* -20 */ 48388,
+	59856,
+	76040,
+	92818,
+	118348,
+	/* -15 */ 147320,
+	184698,
+	229616,
+	287308,
+	360437,
+	/* -10 */ 449829,
+	563644,
+	704093,
+	875809,
+	1099582,
+	/*  -5 */ 1376151,
+	1717300,
+	2157191,
+	2708050,
+	3363326,
+	/*   0 */ 4194304,
+	5237765,
+	6557202,
+	8165337,
+	10153587,
+	/*   5 */ 12820798,
+	15790321,
+	19976592,
+	24970740,
+	31350126,
+	/*  10 */ 39045157,
+	49367440,
+	61356676,
+	76695844,
+	95443717,
+	/*  15 */ 119304647,
+	148102320,
+	186737708,
+	238609294,
+	286331153,
 };
 
 void call_trace_sched_update_nr_running(struct rq *rq, int count)
 {
-        trace_sched_update_nr_running_tp(rq, count);
+	trace_sched_update_nr_running_tp(rq, count);
 }
diff --git a/kernel/sched/rsdl.c b/kernel/sched/rsdl.c
new file mode 100644
index 000000000..10eb816c8
--- /dev/null
+++ b/kernel/sched/rsdl.c
@@ -0,0 +1,214 @@
+////////////////// Initialize sched rsdl class //////////////////
+
+void __init init_sched_rsdl_class(void)
+{
+	unsigned int i;
+
+	printk("***********************************************************************************************\n");
+	printk("INITIATING RSDL SCHED CLASS\n");
+	printk("***********************************************************************************************\n");
+
+	for_each_possible_cpu(i) {
+		zalloc_cpumask_var_node(&per_cpu(local_cpu_mask, i),
+					GFP_KERNEL, cpu_to_node(i));
+	}
+}
+
+////////////////// Initialize sched rsdl runqueue //////////////////
+
+void init_rsdl_rq(struct rsdl_rq *rsdl_rq)
+{
+    struct rsdl_prio_array *array1;
+    struct rsdl_prio_array *array2;
+
+	int i;
+
+	printk("Init RSDL runqueue function called\n");
+
+	array1 = &rsdl_rq->active;
+	for (i = 0; i < MAX_RSDL_PRIO; i++) {
+		INIT_LIST_HEAD(array1->queue + i);
+		printk("RSDL active runqueue function initialized\n");
+		__clear_bit(i, array1->bitmap);
+	}
+	/* delimiter for bitsearch: */
+	__set_bit(MAX_RSDL_PRIO, array1->bitmap);
+
+
+	array2 = &rsdl_rq->expired;
+	for (i = 0; i < MAX_RSDL_PRIO; i++) {
+		INIT_LIST_HEAD(array2->queue + i);
+		printk("RSDL expired runqueue function initialized\n");
+		__clear_bit(i, array2->bitmap);
+	}
+	/* delimiter for bitsearch: */
+	__set_bit(MAX_RSDL_PRIO, array2->bitmap);
+
+    rsdl_rq->rsdl_nr_running = 0;
+
+}
+
+
+////////////////// Basic hook function definations //////////////////
+static void
+enqueue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{
+	printk("Enqueue Task Called\n");
+}
+
+
+static void dequeue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{
+	printk("Dequeue Task Called\n");
+}
+
+
+static void yield_task_rsdl(struct rq *rq)
+{
+	printk("Yield Task Called\n");
+}
+
+
+static void check_preempt_curr_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{
+    printk("Check_preempt_curr_rsdl Task Called\n");
+}
+
+
+static struct task_struct *pick_next_task_rsdl(struct rq *rq)
+{
+    struct task_struct *p = NULL;
+    printk("pick_next_task_rsdl Task Called\n");
+    return p;
+}
+
+
+static void put_prev_task_rsdl(struct rq *rq, struct task_struct *p)
+{
+	printk("put_prev_task_rsdl Task Called\n");
+}
+
+
+static inline void set_next_task_rsdl(struct rq *rq, struct task_struct *p, bool first)
+{
+	printk("set_next_task_rsdl Task Called\n");
+}
+
+
+static void task_tick_rsdl(struct rq *rq, struct task_struct *p, int queued)
+{
+	printk("task_tick_rsdl Task Called\n");
+}
+
+static unsigned int get_rr_interval_rsdl(struct rq *rq, struct task_struct *task)
+{
+	printk("get_rr_interval_rsdl Task Called\n");
+    return 0;
+}
+
+static void
+prio_changed_rsdl(struct rq *rq, struct task_struct *p, int oldprio)
+{
+    printk("prio_changed_rsdl Task Called\n");
+}
+
+
+static void switched_to_rsdl(struct rq *rq, struct task_struct *p)
+{
+    printk("switched_to_rsdl Task Called\n");
+}
+
+
+static void update_curr_rsdl(struct rq *rq)
+{
+    printk("update_curr_rsdl Task Called\n");
+}
+
+static int balance_rsdl(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
+{
+    printk("balance_rsdl Task Called\n");
+    return 0;
+}
+
+static struct task_struct *pick_task_rsdl(struct rq *rq)
+{   
+	struct task_struct *p = NULL;
+//     if (!sched_rsdl_runnable(rq))
+// 		return NULL;
+//     printk("pick_task_rsdl Task Called\n");
+//     return p;
+// 
+	return p;
+}
+
+static int
+select_task_rq_rsdl(struct task_struct *p, int cpu, int flags)
+{
+    printk("select_task_rq_rsdl Task Called\n");
+    return 0;
+}
+
+static void rq_online_rsdl(struct rq *rq)
+{
+    printk("rq_online_rsdl Task Called\n");
+
+}
+
+static void rq_offline_rsdl(struct rq *rq)
+{
+    printk("rq_offline_rsdl Task Called\n");
+
+}
+
+static void task_woken_rsdl(struct rq *rq, struct task_struct *p)
+{
+    printk("task_woken_rsdl Task Called\n");
+}
+
+static void switched_from_rsdl(struct rq *rq, struct task_struct *p)
+{
+    printk("switched_from_rsdl Task Called\n");
+
+}
+
+
+static struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq);
+
+
+DEFINE_SCHED_CLASS(rsdl) = {
+
+	.enqueue_task		= enqueue_task_rsdl,
+	.dequeue_task		= dequeue_task_rsdl,
+	.yield_task		= yield_task_rsdl,
+
+	.check_preempt_curr	= check_preempt_curr_rsdl,
+
+	.pick_next_task		= pick_next_task_rsdl,
+	.put_prev_task		= put_prev_task_rsdl,
+	.set_next_task          = set_next_task_rsdl,
+
+#ifdef CONFIG_SMP
+	.balance		= balance_rsdl,
+	.pick_task		= pick_task_rsdl,
+	.select_task_rq		= select_task_rq_rsdl,
+	.set_cpus_allowed       = set_cpus_allowed_common,
+	.rq_online              = rq_online_rsdl,
+	.rq_offline             = rq_offline_rsdl,
+	.task_woken		= task_woken_rsdl,
+	.switched_from		= switched_from_rsdl,
+	.find_lock_rq		= find_lock_lowest_rq,
+#endif
+
+	.task_tick		= task_tick_rsdl,
+
+	.get_rr_interval	= get_rr_interval_rsdl,
+
+	.prio_changed		= prio_changed_rsdl,
+	.switched_to		= switched_to_rsdl,
+
+	.update_curr		= update_curr_rsdl,
+
+#ifdef CONFIG_UCLAMP_TASK
+	.uclamp_enabled		= 1,
+#endif
+};
\ No newline at end of file
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2fcb7eb56..2c70ddf62 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -80,29 +80,29 @@
 #endif
 
 #ifdef CONFIG_SCHED_DEBUG
-# include <linux/static_key.h>
+#include <linux/static_key.h>
 #endif
 
 #ifdef CONFIG_PARAVIRT
-# include <asm/paravirt.h>
-# include <asm/paravirt_api_clock.h>
+#include <asm/paravirt.h>
+#include <asm/paravirt_api_clock.h>
 #endif
 
 #include "cpupri.h"
 #include "cpudeadline.h"
 
 #ifdef CONFIG_SCHED_DEBUG
-# define SCHED_WARN_ON(x)      WARN_ONCE(x, #x)
+#define SCHED_WARN_ON(x) WARN_ONCE(x, #x)
 #else
-# define SCHED_WARN_ON(x)      ({ (void)(x), 0; })
+#define SCHED_WARN_ON(x) ({ (void)(x), 0; })
 #endif
 
 struct rq;
 struct cpuidle_state;
 
 /* task_struct::on_rq states: */
-#define TASK_ON_RQ_QUEUED	1
-#define TASK_ON_RQ_MIGRATING	2
+#define TASK_ON_RQ_QUEUED 1
+#define TASK_ON_RQ_MIGRATING 2
 
 extern __read_mostly int scheduler_running;
 
@@ -123,7 +123,7 @@ extern int sched_rr_timeslice;
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
  */
-#define NS_TO_JIFFIES(TIME)	((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))
+#define NS_TO_JIFFIES(TIME) ((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))
 
 /*
  * Increase resolution of nice-level calculations for 64-bit architectures.
@@ -140,19 +140,19 @@ extern int sched_rr_timeslice;
  * increase coverage and consistency always enable it on 64-bit platforms.
  */
 #ifdef CONFIG_64BIT
-# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
-# define scale_load(w)		((w) << SCHED_FIXEDPOINT_SHIFT)
-# define scale_load_down(w) \
-({ \
-	unsigned long __w = (w); \
-	if (__w) \
-		__w = max(2UL, __w >> SCHED_FIXEDPOINT_SHIFT); \
-	__w; \
-})
+#define NICE_0_LOAD_SHIFT (SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
+#define scale_load(w) ((w) << SCHED_FIXEDPOINT_SHIFT)
+#define scale_load_down(w)                                             \
+	({                                                             \
+		unsigned long __w = (w);                               \
+		if (__w)                                               \
+			__w = max(2UL, __w >> SCHED_FIXEDPOINT_SHIFT); \
+		__w;                                                   \
+	})
 #else
-# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)
-# define scale_load(w)		(w)
-# define scale_load_down(w)	(w)
+#define NICE_0_LOAD_SHIFT (SCHED_FIXEDPOINT_SHIFT)
+#define scale_load(w) (w)
+#define scale_load_down(w) (w)
 #endif
 
 /*
@@ -164,19 +164,19 @@ extern int sched_rr_timeslice;
  *  scale_load(sched_prio_to_weight[NICE_TO_PRIO(0)-MAX_RT_PRIO]) == NICE_0_LOAD
  *
  */
-#define NICE_0_LOAD		(1L << NICE_0_LOAD_SHIFT)
+#define NICE_0_LOAD (1L << NICE_0_LOAD_SHIFT)
 
 /*
  * Single value that decides SCHED_DEADLINE internal math precision.
  * 10 -> just above 1us
  * 9  -> just above 0.5us
  */
-#define DL_SCALE		10
+#define DL_SCALE 10
 
 /*
  * Single value that denotes runtime == period, ie unlimited time.
  */
-#define RUNTIME_INF		((u64)~0ULL)
+#define RUNTIME_INF ((u64)~0ULL)
 
 static inline int idle_policy(int policy)
 {
@@ -192,6 +192,12 @@ static inline int rt_policy(int policy)
 	return policy == SCHED_FIFO || policy == SCHED_RR;
 }
 
+//////////////////////// function to test rsdl_policy added
+static inline int rsdl_policy(int policy)
+{
+	return policy == SCHED_RSDL;
+}
+
 static inline int dl_policy(int policy)
 {
 	return policy == SCHED_DEADLINE;
@@ -199,7 +205,7 @@ static inline int dl_policy(int policy)
 static inline bool valid_policy(int policy)
 {
 	return idle_policy(policy) || fair_policy(policy) ||
-		rt_policy(policy) || dl_policy(policy);
+	       rt_policy(policy) || dl_policy(policy) || rsdl_policy(policy);
 }
 
 static inline int task_has_idle_policy(struct task_struct *p)
@@ -217,7 +223,12 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
-#define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)
+static inline int task_has_rsdl_policy(struct task_struct *p)
+{
+	return rsdl_policy(p->policy);
+}
+
+#define cap_scale(v, s) ((v) * (s) >> SCHED_CAPACITY_SHIFT)
 
 static inline void update_avg(u64 *avg, u64 sample)
 {
@@ -229,7 +240,7 @@ static inline void update_avg(u64 *avg, u64 sample)
  * Shifting a value by an exponent greater *or equal* to the size of said value
  * is UB; cap at size-1.
  */
-#define shr_bound(val, shift)							\
+#define shr_bound(val, shift) \
 	(val >> min_t(typeof(shift), shift, BITS_PER_TYPE(typeof(val)) - 1))
 
 /*
@@ -244,9 +255,10 @@ static inline void update_avg(u64 *avg, u64 sample)
  *
  * SUGOV stands for SchedUtil GOVernor.
  */
-#define SCHED_FLAG_SUGOV	0x10000000
+#define SCHED_FLAG_SUGOV 0x10000000
 
-#define SCHED_DL_FLAGS (SCHED_FLAG_RECLAIM | SCHED_FLAG_DL_OVERRUN | SCHED_FLAG_SUGOV)
+#define SCHED_DL_FLAGS \
+	(SCHED_FLAG_RECLAIM | SCHED_FLAG_DL_OVERRUN | SCHED_FLAG_SUGOV)
 
 static inline bool dl_entity_is_special(struct sched_dl_entity *dl_se)
 {
@@ -260,8 +272,8 @@ static inline bool dl_entity_is_special(struct sched_dl_entity *dl_se)
 /*
  * Tells if entity @a should preempt entity @b.
  */
-static inline bool
-dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
+static inline bool dl_entity_preempt(struct sched_dl_entity *a,
+				     struct sched_dl_entity *b)
 {
 	return dl_entity_is_special(a) ||
 	       dl_time_before(a->deadline, b->deadline);
@@ -271,25 +283,35 @@ dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
  * This is the priority-queue data structure of the RT scheduling class:
  */
 struct rt_prio_array {
-	DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */
+	DECLARE_BITMAP(bitmap,
+		       MAX_RT_PRIO + 1); /* include 1 bit for delimiter */
 	struct list_head queue[MAX_RT_PRIO];
 };
 
+/*
+ * This is the priority-queue data structure of the RSDL scheduling class:
+ */
+struct rsdl_prio_array {
+	DECLARE_BITMAP(bitmap,
+		       MAX_RSDL_PRIO + 1); /* include 1 bit for delimiter */
+	struct list_head queue[MAX_RSDL_PRIO];
+};
+
 struct rt_bandwidth {
 	/* nests inside the rq lock: */
-	raw_spinlock_t		rt_runtime_lock;
-	ktime_t			rt_period;
-	u64			rt_runtime;
-	struct hrtimer		rt_period_timer;
-	unsigned int		rt_period_active;
+	raw_spinlock_t rt_runtime_lock;
+	ktime_t rt_period;
+	u64 rt_runtime;
+	struct hrtimer rt_period_timer;
+	unsigned int rt_period_active;
 };
 
 void __dl_clear_params(struct task_struct *p);
 
 struct dl_bandwidth {
-	raw_spinlock_t		dl_runtime_lock;
-	u64			dl_runtime;
-	u64			dl_period;
+	raw_spinlock_t dl_runtime_lock;
+	u64 dl_runtime;
+	u64 dl_period;
 };
 
 static inline int dl_bandwidth_enabled(void)
@@ -316,9 +338,9 @@ static inline int dl_bandwidth_enabled(void)
  *  - total_bw is the currently allocated bandwidth in each root domain;
  */
 struct dl_bw {
-	raw_spinlock_t		lock;
-	u64			bw;
-	u64			total_bw;
+	raw_spinlock_t lock;
+	u64 bw;
+	u64 total_bw;
 };
 
 /*
@@ -337,46 +359,50 @@ static inline bool dl_task_fits_capacity(struct task_struct *p, int cpu)
 }
 
 extern void init_dl_bw(struct dl_bw *dl_b);
-extern int  sched_dl_global_validate(void);
+extern int sched_dl_global_validate(void);
 extern void sched_dl_do_global(void);
-extern int  sched_dl_overflow(struct task_struct *p, int policy, const struct sched_attr *attr);
+extern int sched_dl_overflow(struct task_struct *p, int policy,
+			     const struct sched_attr *attr);
 extern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);
 extern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);
 extern bool __checkparam_dl(const struct sched_attr *attr);
-extern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);
-extern int  dl_cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
-extern int  dl_cpu_busy(int cpu, struct task_struct *p);
+extern bool dl_param_changed(struct task_struct *p,
+			     const struct sched_attr *attr);
+extern int dl_cpuset_cpumask_can_shrink(const struct cpumask *cur,
+					const struct cpumask *trial);
+extern int dl_cpu_busy(int cpu, struct task_struct *p);
 
 #ifdef CONFIG_CGROUP_SCHED
 
 struct cfs_rq;
 struct rt_rq;
+struct rsdl_rq;
 
 extern struct list_head task_groups;
 
 struct cfs_bandwidth {
 #ifdef CONFIG_CFS_BANDWIDTH
-	raw_spinlock_t		lock;
-	ktime_t			period;
-	u64			quota;
-	u64			runtime;
-	u64			burst;
-	u64			runtime_snap;
-	s64			hierarchical_quota;
-
-	u8			idle;
-	u8			period_active;
-	u8			slack_started;
-	struct hrtimer		period_timer;
-	struct hrtimer		slack_timer;
-	struct list_head	throttled_cfs_rq;
+	raw_spinlock_t lock;
+	ktime_t period;
+	u64 quota;
+	u64 runtime;
+	u64 burst;
+	u64 runtime_snap;
+	s64 hierarchical_quota;
+
+	u8 idle;
+	u8 period_active;
+	u8 slack_started;
+	struct hrtimer period_timer;
+	struct hrtimer slack_timer;
+	struct list_head throttled_cfs_rq;
 
 	/* Statistics: */
-	int			nr_periods;
-	int			nr_throttled;
-	int			nr_burst;
-	u64			throttled_time;
-	u64			burst_time;
+	int nr_periods;
+	int nr_throttled;
+	int nr_burst;
+	u64 throttled_time;
+	u64 burst_time;
 #endif
 };
 
@@ -386,57 +412,56 @@ struct task_group {
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* schedulable entities of this group on each CPU */
-	struct sched_entity	**se;
+	struct sched_entity **se;
 	/* runqueue "owned" by this group on each CPU */
-	struct cfs_rq		**cfs_rq;
-	unsigned long		shares;
+	struct cfs_rq **cfs_rq;
+	unsigned long shares;
 
 	/* A positive value indicates that this is a SCHED_IDLE group. */
-	int			idle;
+	int idle;
 
-#ifdef	CONFIG_SMP
+#ifdef CONFIG_SMP
 	/*
 	 * load_avg can be heavily contended at clock tick time, so put
 	 * it in its own cacheline separated from the fields above which
 	 * will also be accessed at each tick.
 	 */
-	atomic_long_t		load_avg ____cacheline_aligned;
+	atomic_long_t load_avg ____cacheline_aligned;
 #endif
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	struct sched_rt_entity	**rt_se;
-	struct rt_rq		**rt_rq;
+	struct sched_rt_entity **rt_se;
+	struct rt_rq **rt_rq;
 
-	struct rt_bandwidth	rt_bandwidth;
+	struct rt_bandwidth rt_bandwidth;
 #endif
 
-	struct rcu_head		rcu;
-	struct list_head	list;
+	struct rcu_head rcu;
+	struct list_head list;
 
-	struct task_group	*parent;
-	struct list_head	siblings;
-	struct list_head	children;
+	struct task_group *parent;
+	struct list_head siblings;
+	struct list_head children;
 
 #ifdef CONFIG_SCHED_AUTOGROUP
-	struct autogroup	*autogroup;
+	struct autogroup *autogroup;
 #endif
 
-	struct cfs_bandwidth	cfs_bandwidth;
+	struct cfs_bandwidth cfs_bandwidth;
 
 #ifdef CONFIG_UCLAMP_TASK_GROUP
 	/* The two decimal precision [%] value requested from user-space */
-	unsigned int		uclamp_pct[UCLAMP_CNT];
+	unsigned int uclamp_pct[UCLAMP_CNT];
 	/* Clamp values requested for a task group */
-	struct uclamp_se	uclamp_req[UCLAMP_CNT];
+	struct uclamp_se uclamp_req[UCLAMP_CNT];
 	/* Effective clamp values used for a task group */
-	struct uclamp_se	uclamp[UCLAMP_CNT];
+	struct uclamp_se uclamp[UCLAMP_CNT];
 #endif
-
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-#define ROOT_TASK_GROUP_LOAD	NICE_0_LOAD
+#define ROOT_TASK_GROUP_LOAD NICE_0_LOAD
 
 /*
  * A weight of 0 or 1 can cause arithmetics problems.
@@ -446,14 +471,14 @@ struct task_group {
  * (The default weight is 1024 - so there's no practical
  *  limitation from this.)
  */
-#define MIN_SHARES		(1UL <<  1)
-#define MAX_SHARES		(1UL << 18)
+#define MIN_SHARES (1UL << 1)
+#define MAX_SHARES (1UL << 18)
 #endif
 
 typedef int (*tg_visitor)(struct task_group *, void *);
 
-extern int walk_tg_tree_from(struct task_group *from,
-			     tg_visitor down, tg_visitor up, void *data);
+extern int walk_tg_tree_from(struct task_group *from, tg_visitor down,
+			     tg_visitor up, void *data);
 
 /*
  * Iterate the full tree, calling @down when first entering a node and @up when
@@ -469,12 +494,13 @@ static inline int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)
 extern int tg_nop(struct task_group *tg, void *data);
 
 extern void free_fair_sched_group(struct task_group *tg);
-extern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);
+extern int alloc_fair_sched_group(struct task_group *tg,
+				  struct task_group *parent);
 extern void online_fair_sched_group(struct task_group *tg);
 extern void unregister_fair_sched_group(struct task_group *tg);
 extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
-			struct sched_entity *se, int cpu,
-			struct sched_entity *parent);
+			      struct sched_entity *se, int cpu,
+			      struct sched_entity *parent);
 extern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 
 extern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);
@@ -482,9 +508,10 @@ extern void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 extern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);
 
 extern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,
-		struct sched_rt_entity *rt_se, int cpu,
-		struct sched_rt_entity *parent);
-extern int sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us);
+			     struct sched_rt_entity *rt_se, int cpu,
+			     struct sched_rt_entity *parent);
+extern int sched_group_set_rt_runtime(struct task_group *tg,
+				      long rt_runtime_us);
 extern int sched_group_set_rt_period(struct task_group *tg, u64 rt_period_us);
 extern long sched_group_rt_runtime(struct task_group *tg);
 extern long sched_group_rt_period(struct task_group *tg);
@@ -504,23 +531,26 @@ extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 extern int sched_group_set_idle(struct task_group *tg, long idle);
 
 #ifdef CONFIG_SMP
-extern void set_task_rq_fair(struct sched_entity *se,
-			     struct cfs_rq *prev, struct cfs_rq *next);
+extern void set_task_rq_fair(struct sched_entity *se, struct cfs_rq *prev,
+			     struct cfs_rq *next);
 #else /* !CONFIG_SMP */
 static inline void set_task_rq_fair(struct sched_entity *se,
-			     struct cfs_rq *prev, struct cfs_rq *next) { }
+				    struct cfs_rq *prev, struct cfs_rq *next)
+{
+}
 #endif /* CONFIG_SMP */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 #else /* CONFIG_CGROUP_SCHED */
 
-struct cfs_bandwidth { };
+struct cfs_bandwidth {};
 
-#endif	/* CONFIG_CGROUP_SCHED */
+#endif /* CONFIG_CGROUP_SCHED */
 
 extern void unregister_rt_sched_group(struct task_group *tg);
 extern void free_rt_sched_group(struct task_group *tg);
-extern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent);
+extern int alloc_rt_sched_group(struct task_group *tg,
+				struct task_group *parent);
 
 /*
  * u64_u32_load/u64_u32_store
@@ -529,92 +559,92 @@ extern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent
  * applicable for 32-bits architectures.
  */
 #ifdef CONFIG_64BIT
-# define u64_u32_load_copy(var, copy)       var
-# define u64_u32_store_copy(var, copy, val) (var = val)
+#define u64_u32_load_copy(var, copy) var
+#define u64_u32_store_copy(var, copy, val) (var = val)
 #else
-# define u64_u32_load_copy(var, copy)					\
-({									\
-	u64 __val, __val_copy;						\
-	do {								\
-		__val_copy = copy;					\
-		/*							\
+#define u64_u32_load_copy(var, copy)           \
+	({                                     \
+		u64 __val, __val_copy;         \
+		do {                           \
+			__val_copy = copy;     \
+			/*							\
 		 * paired with u64_u32_store_copy(), ordering access	\
 		 * to var and copy.					\
-		 */							\
-		smp_rmb();						\
-		__val = var;						\
-	} while (__val != __val_copy);					\
-	__val;								\
-})
-# define u64_u32_store_copy(var, copy, val)				\
-do {									\
-	typeof(val) __val = (val);					\
-	var = __val;							\
-	/*								\
+		 */             \
+			smp_rmb();             \
+			__val = var;           \
+		} while (__val != __val_copy); \
+		__val;                         \
+	})
+#define u64_u32_store_copy(var, copy, val) \
+	do {                               \
+		typeof(val) __val = (val); \
+		var = __val;               \
+		/*								\
 	 * paired with u64_u32_load_copy(), ordering access to var and	\
 	 * copy.							\
-	 */								\
-	smp_wmb();							\
-	copy = __val;							\
-} while (0)
+	 */                \
+		smp_wmb();                 \
+		copy = __val;              \
+	} while (0)
 #endif
-# define u64_u32_load(var)      u64_u32_load_copy(var, var##_copy)
-# define u64_u32_store(var, val) u64_u32_store_copy(var, var##_copy, val)
+#define u64_u32_load(var) u64_u32_load_copy(var, var##_copy)
+#define u64_u32_store(var, val) u64_u32_store_copy(var, var##_copy, val)
 
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
-	struct load_weight	load;
-	unsigned int		nr_running;
-	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
-	unsigned int		idle_nr_running;   /* SCHED_IDLE */
-	unsigned int		idle_h_nr_running; /* SCHED_IDLE */
-
-	u64			exec_clock;
-	u64			min_vruntime;
+	struct load_weight load;
+	unsigned int nr_running;
+	unsigned int h_nr_running; /* SCHED_{NORMAL,BATCH,IDLE} */
+	unsigned int idle_nr_running; /* SCHED_IDLE */
+	unsigned int idle_h_nr_running; /* SCHED_IDLE */
+
+	u64 exec_clock;
+	u64 min_vruntime;
 #ifdef CONFIG_SCHED_CORE
-	unsigned int		forceidle_seq;
-	u64			min_vruntime_fi;
+	unsigned int forceidle_seq;
+	u64 min_vruntime_fi;
 #endif
 
 #ifndef CONFIG_64BIT
-	u64			min_vruntime_copy;
+	u64 min_vruntime_copy;
 #endif
 
-	struct rb_root_cached	tasks_timeline;
+	struct rb_root_cached tasks_timeline;
 
 	/*
 	 * 'curr' points to currently running entity on this cfs_rq.
 	 * It is set to NULL otherwise (i.e when none are currently running).
 	 */
-	struct sched_entity	*curr;
-	struct sched_entity	*next;
-	struct sched_entity	*last;
-	struct sched_entity	*skip;
+	struct sched_entity *curr;
+	struct sched_entity *next;
+	struct sched_entity *last;
+	struct sched_entity *skip;
 
-#ifdef	CONFIG_SCHED_DEBUG
-	unsigned int		nr_spread_over;
+#ifdef CONFIG_SCHED_DEBUG
+	unsigned int nr_spread_over;
 #endif
 
 #ifdef CONFIG_SMP
 	/*
 	 * CFS load tracking
 	 */
-	struct sched_avg	avg;
+	struct sched_avg avg;
 #ifndef CONFIG_64BIT
-	u64			last_update_time_copy;
+	u64 last_update_time_copy;
 #endif
 	struct {
-		raw_spinlock_t	lock ____cacheline_aligned;
-		int		nr;
-		unsigned long	load_avg;
-		unsigned long	util_avg;
-		unsigned long	runnable_avg;
+		raw_spinlock_t lock ____cacheline_aligned;
+		int nr;
+		unsigned long load_avg;
+		unsigned long util_avg;
+		unsigned long runnable_avg;
 	} removed;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	unsigned long		tg_load_avg_contrib;
-	long			propagate;
-	long			prop_runnable_sum;
+	unsigned long tg_load_avg_contrib;
+	long propagate;
+	long prop_runnable_sum;
 
 	/*
 	 *   h_load = weight * f(tg)
@@ -622,14 +652,14 @@ struct cfs_rq {
 	 * Where f(tg) is the recursive weight fraction assigned to
 	 * this group.
 	 */
-	unsigned long		h_load;
-	u64			last_h_load_update;
-	struct sched_entity	*h_load_next;
+	unsigned long h_load;
+	u64 last_h_load_update;
+	struct sched_entity *h_load_next;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */
+	struct rq *rq; /* CPU runqueue to which this cfs_rq is attached */
 
 	/*
 	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
@@ -639,27 +669,27 @@ struct cfs_rq {
 	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
 	 * This list is used during load balance.
 	 */
-	int			on_list;
-	struct list_head	leaf_cfs_rq_list;
-	struct task_group	*tg;	/* group that "owns" this runqueue */
+	int on_list;
+	struct list_head leaf_cfs_rq_list;
+	struct task_group *tg; /* group that "owns" this runqueue */
 
 	/* Locally cached copy of our task_group's idle value */
-	int			idle;
+	int idle;
 
 #ifdef CONFIG_CFS_BANDWIDTH
-	int			runtime_enabled;
-	s64			runtime_remaining;
+	int runtime_enabled;
+	s64 runtime_remaining;
 
-	u64			throttled_pelt_idle;
+	u64 throttled_pelt_idle;
 #ifndef CONFIG_64BIT
-	u64                     throttled_pelt_idle_copy;
-#endif
-	u64			throttled_clock;
-	u64			throttled_clock_pelt;
-	u64			throttled_clock_pelt_time;
-	int			throttled;
-	int			throttle_count;
-	struct list_head	throttled_list;
+	u64 throttled_pelt_idle_copy;
+#endif
+	u64 throttled_clock;
+	u64 throttled_clock_pelt;
+	u64 throttled_clock_pelt_time;
+	int throttled;
+	int throttle_count;
+	struct list_head throttled_list;
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
@@ -671,42 +701,42 @@ static inline int rt_bandwidth_enabled(void)
 
 /* RT IPI pull logic requires IRQ_WORK */
 #if defined(CONFIG_IRQ_WORK) && defined(CONFIG_SMP)
-# define HAVE_RT_PUSH_IPI
+#define HAVE_RT_PUSH_IPI
 #endif
 
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
-	struct rt_prio_array	active;
-	unsigned int		rt_nr_running;
-	unsigned int		rr_nr_running;
+	struct rt_prio_array active;
+	unsigned int rt_nr_running;
+	unsigned int rr_nr_running;
 #if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
 	struct {
-		int		curr; /* highest queued rt task prio */
+		int curr; /* highest queued rt task prio */
 #ifdef CONFIG_SMP
-		int		next; /* next highest */
+		int next; /* next highest */
 #endif
 	} highest_prio;
 #endif
 #ifdef CONFIG_SMP
-	unsigned int		rt_nr_migratory;
-	unsigned int		rt_nr_total;
-	int			overloaded;
-	struct plist_head	pushable_tasks;
+	unsigned int rt_nr_migratory;
+	unsigned int rt_nr_total;
+	int overloaded;
+	struct plist_head pushable_tasks;
 
 #endif /* CONFIG_SMP */
-	int			rt_queued;
+	int rt_queued;
 
-	int			rt_throttled;
-	u64			rt_time;
-	u64			rt_runtime;
+	int rt_throttled;
+	u64 rt_time;
+	u64 rt_runtime;
 	/* Nests inside the rq lock: */
-	raw_spinlock_t		rt_runtime_lock;
+	raw_spinlock_t rt_runtime_lock;
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	unsigned int		rt_nr_boosted;
+	unsigned int rt_nr_boosted;
 
-	struct rq		*rq;
-	struct task_group	*tg;
+	struct rq *rq;
+	struct task_group *tg;
 #endif
 };
 
@@ -715,12 +745,25 @@ static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
 	return rt_rq->rt_queued && rt_rq->rt_nr_running;
 }
 
+/* RSDL classes' related field in a runqueue: */
+struct rsdl_rq {
+	struct rsdl_prio_array active;
+	struct rsdl_prio_array expired;
+
+	unsigned int rsdl_nr_running;
+};
+
+static inline bool rsdl_rq_is_runnable(struct rsdl_rq *rsdl_rq)
+{
+	return rsdl_rq->rsdl_nr_running;
+}
+
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */
-	struct rb_root_cached	root;
+	struct rb_root_cached root;
 
-	unsigned int		dl_nr_running;
+	unsigned int dl_nr_running;
 
 #ifdef CONFIG_SMP
 	/*
@@ -730,28 +773,28 @@ struct dl_rq {
 	 * should migrate somewhere else.
 	 */
 	struct {
-		u64		curr;
-		u64		next;
+		u64 curr;
+		u64 next;
 	} earliest_dl;
 
-	unsigned int		dl_nr_migratory;
-	int			overloaded;
+	unsigned int dl_nr_migratory;
+	int overloaded;
 
 	/*
 	 * Tasks on this rq that can be pushed away. They are kept in
 	 * an rb-tree, ordered by tasks' deadlines, with caching
 	 * of the leftmost (earliest deadline) element.
 	 */
-	struct rb_root_cached	pushable_dl_tasks_root;
+	struct rb_root_cached pushable_dl_tasks_root;
 #else
-	struct dl_bw		dl_bw;
+	struct dl_bw dl_bw;
 #endif
 	/*
 	 * "Active utilization" for this runqueue: increased when a
 	 * task wakes up (becomes TASK_RUNNING) and decreased when a
 	 * task blocks
 	 */
-	u64			running_bw;
+	u64 running_bw;
 
 	/*
 	 * Utilization of the tasks "assigned" to this runqueue (including
@@ -762,19 +805,19 @@ struct dl_rq {
 	 * This is needed to compute the "inactive utilization" for the
 	 * runqueue (inactive utilization = this_bw - running_bw).
 	 */
-	u64			this_bw;
-	u64			extra_bw;
+	u64 this_bw;
+	u64 extra_bw;
 
 	/*
 	 * Inverse of the fraction of CPU utilization that can be reclaimed
 	 * by the GRUB algorithm.
 	 */
-	u64			bw_ratio;
+	u64 bw_ratio;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /* An entity is a task if it doesn't "own" a runqueue */
-#define entity_is_task(se)	(!se->my_q)
+#define entity_is_task(se) (!se->my_q)
 
 static inline void se_update_runnable(struct sched_entity *se)
 {
@@ -791,9 +834,11 @@ static inline long se_runnable(struct sched_entity *se)
 }
 
 #else
-#define entity_is_task(se)	1
+#define entity_is_task(se) 1
 
-static inline void se_update_runnable(struct sched_entity *se) {}
+static inline void se_update_runnable(struct sched_entity *se)
+{
+}
 
 static inline long se_runnable(struct sched_entity *se)
 {
@@ -810,7 +855,6 @@ static inline long se_weight(struct sched_entity *se)
 	return scale_load_down(se->load.weight);
 }
 
-
 static inline bool sched_asym_prefer(int a, int b)
 {
 	return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
@@ -823,8 +867,8 @@ struct perf_domain {
 };
 
 /* Scheduling group status flags */
-#define SG_OVERLOAD		0x1 /* More than one runnable task on a CPU. */
-#define SG_OVERUTILIZED		0x2 /* One or more CPUs are over-utilized. */
+#define SG_OVERLOAD 0x1 /* More than one runnable task on a CPU. */
+#define SG_OVERUTILIZED 0x2 /* One or more CPUs are over-utilized. */
 
 /*
  * We add the notion of a root-domain which will be used to define per-domain
@@ -835,30 +879,30 @@ struct perf_domain {
  *
  */
 struct root_domain {
-	atomic_t		refcount;
-	atomic_t		rto_count;
-	struct rcu_head		rcu;
-	cpumask_var_t		span;
-	cpumask_var_t		online;
+	atomic_t refcount;
+	atomic_t rto_count;
+	struct rcu_head rcu;
+	cpumask_var_t span;
+	cpumask_var_t online;
 
 	/*
 	 * Indicate pullable load on at least one CPU, e.g:
 	 * - More than one runnable task
 	 * - Running task is misfit
 	 */
-	int			overload;
+	int overload;
 
 	/* Indicate one or more cpus over-utilized (tipping point) */
-	int			overutilized;
+	int overutilized;
 
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
 	 * than one runnable -deadline task (as it is below for RT tasks).
 	 */
-	cpumask_var_t		dlo_mask;
-	atomic_t		dlo_count;
-	struct dl_bw		dl_bw;
-	struct cpudl		cpudl;
+	cpumask_var_t dlo_mask;
+	atomic_t dlo_count;
+	struct dl_bw dl_bw;
+	struct cpudl cpudl;
 
 	/*
 	 * Indicate whether a root_domain's dl_bw has been checked or
@@ -873,23 +917,23 @@ struct root_domain {
 	/*
 	 * For IPI pull requests, loop across the rto_mask.
 	 */
-	struct irq_work		rto_push_work;
-	raw_spinlock_t		rto_lock;
+	struct irq_work rto_push_work;
+	raw_spinlock_t rto_lock;
 	/* These are only updated and read within rto_lock */
-	int			rto_loop;
-	int			rto_cpu;
+	int rto_loop;
+	int rto_cpu;
 	/* These atomics are updated outside of a lock */
-	atomic_t		rto_loop_next;
-	atomic_t		rto_loop_start;
+	atomic_t rto_loop_next;
+	atomic_t rto_loop_start;
 #endif
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
 	 * one runnable RT task.
 	 */
-	cpumask_var_t		rto_mask;
-	struct cpupri		cpupri;
+	cpumask_var_t rto_mask;
+	struct cpupri cpupri;
 
-	unsigned long		max_cpu_capacity;
+	unsigned long max_cpu_capacity;
 
 	/*
 	 * NULL-terminated list of performance domains intersecting with the
@@ -962,48 +1006,49 @@ DECLARE_STATIC_KEY_FALSE(sched_uclamp_used);
  */
 struct rq {
 	/* runqueue lock: */
-	raw_spinlock_t		__lock;
+	raw_spinlock_t __lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
 	 * remote CPUs use both these fields when doing load calculation.
 	 */
-	unsigned int		nr_running;
+	unsigned int nr_running;
 #ifdef CONFIG_NUMA_BALANCING
-	unsigned int		nr_numa_running;
-	unsigned int		nr_preferred_running;
-	unsigned int		numa_migrate_on;
+	unsigned int nr_numa_running;
+	unsigned int nr_preferred_running;
+	unsigned int numa_migrate_on;
 #endif
 #ifdef CONFIG_NO_HZ_COMMON
 #ifdef CONFIG_SMP
-	unsigned long		last_blocked_load_update_tick;
-	unsigned int		has_blocked_load;
-	call_single_data_t	nohz_csd;
+	unsigned long last_blocked_load_update_tick;
+	unsigned int has_blocked_load;
+	call_single_data_t nohz_csd;
 #endif /* CONFIG_SMP */
-	unsigned int		nohz_tick_stopped;
-	atomic_t		nohz_flags;
+	unsigned int nohz_tick_stopped;
+	atomic_t nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_SMP
-	unsigned int		ttwu_pending;
+	unsigned int ttwu_pending;
 #endif
-	u64			nr_switches;
+	u64 nr_switches;
 
 #ifdef CONFIG_UCLAMP_TASK
 	/* Utilization clamp values based on CPU's RUNNABLE tasks */
-	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
-	unsigned int		uclamp_flags;
+	struct uclamp_rq uclamp[UCLAMP_CNT] ____cacheline_aligned;
+	unsigned int uclamp_flags;
 #define UCLAMP_FLAG_IDLE 0x01
 #endif
 
-	struct cfs_rq		cfs;
-	struct rt_rq		rt;
-	struct dl_rq		dl;
+	struct cfs_rq cfs;
+	struct rt_rq rt;
+	struct dl_rq dl;
+	struct rsdl_rq rsdl;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this CPU: */
-	struct list_head	leaf_cfs_rq_list;
-	struct list_head	*tmp_alone_branch;
+	struct list_head leaf_cfs_rq_list;
+	struct list_head *tmp_alone_branch;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*
@@ -1012,28 +1057,28 @@ struct rq {
 	 * one CPU and if it got migrated afterwards it may decrease
 	 * it on another CPU. Always updated under the runqueue lock:
 	 */
-	unsigned int		nr_uninterruptible;
+	unsigned int nr_uninterruptible;
 
-	struct task_struct __rcu	*curr;
-	struct task_struct	*idle;
-	struct task_struct	*stop;
-	unsigned long		next_balance;
-	struct mm_struct	*prev_mm;
+	struct task_struct __rcu *curr;
+	struct task_struct *idle;
+	struct task_struct *stop;
+	unsigned long next_balance;
+	struct mm_struct *prev_mm;
 
-	unsigned int		clock_update_flags;
-	u64			clock;
+	unsigned int clock_update_flags;
+	u64 clock;
 	/* Ensure that all clocks are in the same cache line */
-	u64			clock_task ____cacheline_aligned;
-	u64			clock_pelt;
-	unsigned long		lost_idle_time;
-	u64			clock_pelt_idle;
-	u64			clock_idle;
+	u64 clock_task ____cacheline_aligned;
+	u64 clock_pelt;
+	unsigned long lost_idle_time;
+	u64 clock_pelt_idle;
+	u64 clock_idle;
 #ifndef CONFIG_64BIT
-	u64			clock_pelt_idle_copy;
-	u64			clock_idle_copy;
+	u64 clock_pelt_idle_copy;
+	u64 clock_idle_copy;
 #endif
 
-	atomic_t		nr_iowait;
+	atomic_t nr_iowait;
 
 #ifdef CONFIG_SCHED_DEBUG
 	u64 last_seen_need_resched_ns;
@@ -1045,119 +1090,119 @@ struct rq {
 #endif
 
 #ifdef CONFIG_SMP
-	struct root_domain		*rd;
-	struct sched_domain __rcu	*sd;
+	struct root_domain *rd;
+	struct sched_domain __rcu *sd;
 
-	unsigned long		cpu_capacity;
-	unsigned long		cpu_capacity_orig;
+	unsigned long cpu_capacity;
+	unsigned long cpu_capacity_orig;
 
-	struct callback_head	*balance_callback;
+	struct callback_head *balance_callback;
 
-	unsigned char		nohz_idle_balance;
-	unsigned char		idle_balance;
+	unsigned char nohz_idle_balance;
+	unsigned char idle_balance;
 
-	unsigned long		misfit_task_load;
+	unsigned long misfit_task_load;
 
 	/* For active balancing */
-	int			active_balance;
-	int			push_cpu;
-	struct cpu_stop_work	active_balance_work;
+	int active_balance;
+	int push_cpu;
+	struct cpu_stop_work active_balance_work;
 
 	/* CPU of this runqueue: */
-	int			cpu;
-	int			online;
+	int cpu;
+	int online;
 
 	struct list_head cfs_tasks;
 
-	struct sched_avg	avg_rt;
-	struct sched_avg	avg_dl;
+	struct sched_avg avg_rt;
+	struct sched_avg avg_dl;
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
-	struct sched_avg	avg_irq;
+	struct sched_avg avg_irq;
 #endif
 #ifdef CONFIG_SCHED_THERMAL_PRESSURE
-	struct sched_avg	avg_thermal;
+	struct sched_avg avg_thermal;
 #endif
-	u64			idle_stamp;
-	u64			avg_idle;
+	u64 idle_stamp;
+	u64 avg_idle;
 
-	unsigned long		wake_stamp;
-	u64			wake_avg_idle;
+	unsigned long wake_stamp;
+	u64 wake_avg_idle;
 
 	/* This is used to determine avg_idle's max value */
-	u64			max_idle_balance_cost;
+	u64 max_idle_balance_cost;
 
 #ifdef CONFIG_HOTPLUG_CPU
-	struct rcuwait		hotplug_wait;
+	struct rcuwait hotplug_wait;
 #endif
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
-	u64			prev_irq_time;
+	u64 prev_irq_time;
 #endif
 #ifdef CONFIG_PARAVIRT
-	u64			prev_steal_time;
+	u64 prev_steal_time;
 #endif
 #ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
-	u64			prev_steal_time_rq;
+	u64 prev_steal_time_rq;
 #endif
 
 	/* calc_load related fields */
-	unsigned long		calc_load_update;
-	long			calc_load_active;
+	unsigned long calc_load_update;
+	long calc_load_active;
 
 #ifdef CONFIG_SCHED_HRTICK
 #ifdef CONFIG_SMP
-	call_single_data_t	hrtick_csd;
+	call_single_data_t hrtick_csd;
 #endif
-	struct hrtimer		hrtick_timer;
-	ktime_t 		hrtick_time;
+	struct hrtimer hrtick_timer;
+	ktime_t hrtick_time;
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
 	/* latency stats */
-	struct sched_info	rq_sched_info;
-	unsigned long long	rq_cpu_time;
+	struct sched_info rq_sched_info;
+	unsigned long long rq_cpu_time;
 	/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */
 
 	/* sys_sched_yield() stats */
-	unsigned int		yld_count;
+	unsigned int yld_count;
 
 	/* schedule() stats */
-	unsigned int		sched_count;
-	unsigned int		sched_goidle;
+	unsigned int sched_count;
+	unsigned int sched_goidle;
 
 	/* try_to_wake_up() stats */
-	unsigned int		ttwu_count;
-	unsigned int		ttwu_local;
+	unsigned int ttwu_count;
+	unsigned int ttwu_local;
 #endif
 
 #ifdef CONFIG_CPU_IDLE
 	/* Must be inspected within a rcu lock section */
-	struct cpuidle_state	*idle_state;
+	struct cpuidle_state *idle_state;
 #endif
 
 #ifdef CONFIG_SMP
-	unsigned int		nr_pinned;
+	unsigned int nr_pinned;
 #endif
-	unsigned int		push_busy;
-	struct cpu_stop_work	push_work;
+	unsigned int push_busy;
+	struct cpu_stop_work push_work;
 
 #ifdef CONFIG_SCHED_CORE
 	/* per rq */
-	struct rq		*core;
-	struct task_struct	*core_pick;
-	unsigned int		core_enabled;
-	unsigned int		core_sched_seq;
-	struct rb_root		core_tree;
+	struct rq *core;
+	struct task_struct *core_pick;
+	unsigned int core_enabled;
+	unsigned int core_sched_seq;
+	struct rb_root core_tree;
 
 	/* shared state -- careful with sched_core_cpu_deactivate() */
-	unsigned int		core_task_seq;
-	unsigned int		core_pick_seq;
-	unsigned long		core_cookie;
-	unsigned int		core_forceidle_count;
-	unsigned int		core_forceidle_seq;
-	unsigned int		core_forceidle_occupation;
-	u64			core_forceidle_start;
+	unsigned int core_task_seq;
+	unsigned int core_pick_seq;
+	unsigned long core_cookie;
+	unsigned int core_forceidle_count;
+	unsigned int core_forceidle_seq;
+	unsigned int core_forceidle_occupation;
+	u64 core_forceidle_start;
 #endif
 };
 
@@ -1186,7 +1231,7 @@ static inline int cpu_of(struct rq *rq)
 #endif
 }
 
-#define MDF_PUSH	0x01
+#define MDF_PUSH 0x01
 
 static inline bool is_migration_disabled(struct task_struct *p)
 {
@@ -1199,11 +1244,11 @@ static inline bool is_migration_disabled(struct task_struct *p)
 
 DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
-#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
-#define this_rq()		this_cpu_ptr(&runqueues)
-#define task_rq(p)		cpu_rq(task_cpu(p))
-#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
-#define raw_rq()		raw_cpu_ptr(&runqueues)
+#define cpu_rq(cpu) (&per_cpu(runqueues, (cpu)))
+#define this_rq() this_cpu_ptr(&runqueues)
+#define task_rq(p) cpu_rq(task_cpu(p))
+#define cpu_curr(cpu) (cpu_rq(cpu)->curr)
+#define raw_rq() raw_cpu_ptr(&runqueues)
 
 struct sched_group;
 #ifdef CONFIG_SCHED_CORE
@@ -1213,7 +1258,8 @@ DECLARE_STATIC_KEY_FALSE(__sched_core_enabled);
 
 static inline bool sched_core_enabled(struct rq *rq)
 {
-	return static_branch_unlikely(&__sched_core_enabled) && rq->core_enabled;
+	return static_branch_unlikely(&__sched_core_enabled) &&
+	       rq->core_enabled;
 }
 
 static inline bool sched_core_disabled(void)
@@ -1383,16 +1429,17 @@ static inline unsigned long _raw_spin_rq_lock_irqsave(struct rq *rq)
 	return flags;
 }
 
-static inline void raw_spin_rq_unlock_irqrestore(struct rq *rq, unsigned long flags)
+static inline void raw_spin_rq_unlock_irqrestore(struct rq *rq,
+						 unsigned long flags)
 {
 	raw_spin_rq_unlock(rq);
 	local_irq_restore(flags);
 }
 
-#define raw_spin_rq_lock_irqsave(rq, flags)	\
-do {						\
-	flags = _raw_spin_rq_lock_irqsave(rq);	\
-} while (0)
+#define raw_spin_rq_lock_irqsave(rq, flags)            \
+	do {                                           \
+		flags = _raw_spin_rq_lock_irqsave(rq); \
+	} while (0)
 
 #ifdef CONFIG_SCHED_SMT
 extern void __update_idle_core(struct rq *rq);
@@ -1404,7 +1451,9 @@ static inline void update_idle_core(struct rq *rq)
 }
 
 #else
-static inline void update_idle_core(struct rq *rq) { }
+static inline void update_idle_core(struct rq *rq)
+{
+}
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1483,9 +1532,9 @@ extern void update_rq_clock(struct rq *rq);
  * one position though, because the next rq_unpin_lock() will shift it
  * back.
  */
-#define RQCF_REQ_SKIP		0x01
-#define RQCF_ACT_SKIP		0x02
-#define RQCF_UPDATED		0x04
+#define RQCF_REQ_SKIP 0x01
+#define RQCF_ACT_SKIP 0x02
+#define RQCF_UPDATED 0x04
 
 static inline void assert_clock_updated(struct rq *rq)
 {
@@ -1576,10 +1625,11 @@ static inline void rq_pin_lock(struct rq *rq, struct rq_flags *rf)
 	rf->cookie = lockdep_pin_lock(__rq_lockp(rq));
 
 #ifdef CONFIG_SCHED_DEBUG
-	rq->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
+	rq->clock_update_flags &= (RQCF_REQ_SKIP | RQCF_ACT_SKIP);
 	rf->clock_update_flags = 0;
 #ifdef CONFIG_SMP
-	SCHED_WARN_ON(rq->balance_callback && rq->balance_callback != &balance_push_callback);
+	SCHED_WARN_ON(rq->balance_callback &&
+		      rq->balance_callback != &balance_push_callback);
 #endif
 #endif
 }
@@ -1610,8 +1660,7 @@ struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(rq->lock);
 
 struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
-	__acquires(p->pi_lock)
-	__acquires(rq->lock);
+	__acquires(p->pi_lock) __acquires(rq->lock);
 
 static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
@@ -1620,9 +1669,8 @@ static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
 	raw_spin_rq_unlock(rq);
 }
 
-static inline void
-task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
-	__releases(rq->lock)
+static inline void task_rq_unlock(struct rq *rq, struct task_struct *p,
+				  struct rq_flags *rf) __releases(rq->lock)
 	__releases(p->pi_lock)
 {
 	rq_unpin_lock(rq, rf);
@@ -1630,56 +1678,49 @@ task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
 }
 
-static inline void
-rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
+static inline void rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
 	__acquires(rq->lock)
 {
 	raw_spin_rq_lock_irqsave(rq, rf->flags);
 	rq_pin_lock(rq, rf);
 }
 
-static inline void
-rq_lock_irq(struct rq *rq, struct rq_flags *rf)
+static inline void rq_lock_irq(struct rq *rq, struct rq_flags *rf)
 	__acquires(rq->lock)
 {
 	raw_spin_rq_lock_irq(rq);
 	rq_pin_lock(rq, rf);
 }
 
-static inline void
-rq_lock(struct rq *rq, struct rq_flags *rf)
+static inline void rq_lock(struct rq *rq, struct rq_flags *rf)
 	__acquires(rq->lock)
 {
 	raw_spin_rq_lock(rq);
 	rq_pin_lock(rq, rf);
 }
 
-static inline void
-rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
+static inline void rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
 	rq_unpin_lock(rq, rf);
 	raw_spin_rq_unlock_irqrestore(rq, rf->flags);
 }
 
-static inline void
-rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
+static inline void rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
 	rq_unpin_lock(rq, rf);
 	raw_spin_rq_unlock_irq(rq);
 }
 
-static inline void
-rq_unlock(struct rq *rq, struct rq_flags *rf)
+static inline void rq_unlock(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
 	rq_unpin_lock(rq, rf);
 	raw_spin_rq_unlock(rq);
 }
 
-static inline struct rq *
-this_rq_lock_irq(struct rq_flags *rf)
+static inline struct rq *this_rq_lock_irq(struct rq_flags *rf)
 	__acquires(rq->lock)
 {
 	struct rq *rq;
@@ -1705,10 +1746,18 @@ extern void sched_domains_numa_masks_set(unsigned int cpu);
 extern void sched_domains_numa_masks_clear(unsigned int cpu);
 extern int sched_numa_find_closest(const struct cpumask *cpus, int cpu);
 #else
-static inline void sched_init_numa(int offline_node) { }
-static inline void sched_update_numa(int cpu, bool online) { }
-static inline void sched_domains_numa_masks_set(unsigned int cpu) { }
-static inline void sched_domains_numa_masks_clear(unsigned int cpu) { }
+static inline void sched_init_numa(int offline_node)
+{
+}
+static inline void sched_update_numa(int cpu, bool online)
+{
+}
+static inline void sched_domains_numa_masks_set(unsigned int cpu)
+{
+}
+static inline void sched_domains_numa_masks_clear(unsigned int cpu)
+{
+}
 static inline int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 {
 	return nr_cpu_ids;
@@ -1717,30 +1766,25 @@ static inline int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 
 #ifdef CONFIG_NUMA_BALANCING
 /* The regions in numa_faults array from task_struct */
-enum numa_faults_stats {
-	NUMA_MEM = 0,
-	NUMA_CPU,
-	NUMA_MEMBUF,
-	NUMA_CPUBUF
-};
+enum numa_faults_stats { NUMA_MEM = 0, NUMA_CPU, NUMA_MEMBUF, NUMA_CPUBUF };
 extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);
-extern int migrate_swap(struct task_struct *p, struct task_struct *t,
-			int cpu, int scpu);
-extern void init_numa_balancing(unsigned long clone_flags, struct task_struct *p);
+extern int migrate_swap(struct task_struct *p, struct task_struct *t, int cpu,
+			int scpu);
+extern void init_numa_balancing(unsigned long clone_flags,
+				struct task_struct *p);
 #else
-static inline void
-init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
+static inline void init_numa_balancing(unsigned long clone_flags,
+				       struct task_struct *p)
 {
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_SMP
 
-static inline void
-queue_balance_callback(struct rq *rq,
-		       struct callback_head *head,
-		       void (*func)(struct rq *rq))
+static inline void queue_balance_callback(struct rq *rq,
+					  struct callback_head *head,
+					  void (*func)(struct rq *rq))
 {
 	lockdep_assert_rq_held(rq);
 
@@ -1749,7 +1793,8 @@ queue_balance_callback(struct rq *rq,
 	 * balance_push() is active, see the comment with
 	 * balance_push_callback.
 	 */
-	if (unlikely(head->next || rq->balance_callback == &balance_push_callback))
+	if (unlikely(head->next ||
+		     rq->balance_callback == &balance_push_callback))
 		return;
 
 	head->func = (void (*)(struct callback_head *))func;
@@ -1758,8 +1803,7 @@ queue_balance_callback(struct rq *rq,
 }
 
 #define rcu_dereference_check_sched_domain(p) \
-	rcu_dereference_check((p), \
-			      lockdep_is_held(&sched_domains_mutex))
+	rcu_dereference_check((p), lockdep_is_held(&sched_domains_mutex))
 
 /*
  * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
@@ -1768,9 +1812,9 @@ queue_balance_callback(struct rq *rq,
  * The domain tree of any CPU may only be accessed from within
  * preempt-disabled sections.
  */
-#define for_each_domain(cpu, __sd) \
-	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); \
-			__sd; __sd = __sd->parent)
+#define for_each_domain(cpu, __sd)                                             \
+	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; \
+	     __sd = __sd->parent)
 
 /**
  * highest_flag_domain - Return highest sched_domain containing flag.
@@ -1785,7 +1829,8 @@ static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 {
 	struct sched_domain *sd, *hsd = NULL;
 
-	for_each_domain(cpu, sd) {
+	for_each_domain(cpu, sd)
+	{
 		if (!(sd->flags & flag))
 			break;
 		hsd = sd;
@@ -1798,7 +1843,8 @@ static inline struct sched_domain *lowest_flag_domain(int cpu, int flag)
 {
 	struct sched_domain *sd;
 
-	for_each_domain(cpu, sd) {
+	for_each_domain(cpu, sd)
+	{
 		if (sd->flags & flag)
 			break;
 	}
@@ -1821,32 +1867,32 @@ static __always_inline bool sched_asym_cpucap_active(void)
 }
 
 struct sched_group_capacity {
-	atomic_t		ref;
+	atomic_t ref;
 	/*
 	 * CPU capacity of this group, SCHED_CAPACITY_SCALE being max capacity
 	 * for a single CPU.
 	 */
-	unsigned long		capacity;
-	unsigned long		min_capacity;		/* Min per-CPU capacity in group */
-	unsigned long		max_capacity;		/* Max per-CPU capacity in group */
-	unsigned long		next_update;
-	int			imbalance;		/* XXX unrelated to capacity but shared group state */
+	unsigned long capacity;
+	unsigned long min_capacity; /* Min per-CPU capacity in group */
+	unsigned long max_capacity; /* Max per-CPU capacity in group */
+	unsigned long next_update;
+	int imbalance; /* XXX unrelated to capacity but shared group state */
 
 #ifdef CONFIG_SCHED_DEBUG
-	int			id;
+	int id;
 #endif
 
-	unsigned long		cpumask[];		/* Balance mask */
+	unsigned long cpumask[]; /* Balance mask */
 };
 
 struct sched_group {
-	struct sched_group	*next;			/* Must be a circular list */
-	atomic_t		ref;
+	struct sched_group *next; /* Must be a circular list */
+	atomic_t ref;
 
-	unsigned int		group_weight;
+	unsigned int group_weight;
 	struct sched_group_capacity *sgc;
-	int			asym_prefer_cpu;	/* CPU of highest priority in group */
-	int			flags;
+	int asym_prefer_cpu; /* CPU of highest priority in group */
+	int flags;
 
 	/*
 	 * The CPUs this group covers.
@@ -1855,7 +1901,7 @@ struct sched_group {
 	 * by attaching extra space to the end of the structure,
 	 * depending on how many CPUs the kernel has booted up with)
 	 */
-	unsigned long		cpumask[];
+	unsigned long cpumask[];
 };
 
 static inline struct cpumask *sched_group_span(struct sched_group *sg)
@@ -1910,9 +1956,13 @@ static inline void sched_core_tick(struct rq *rq)
 
 #else
 
-static inline void sched_core_account_forceidle(struct rq *rq) {}
+static inline void sched_core_account_forceidle(struct rq *rq)
+{
+}
 
-static inline void sched_core_tick(struct rq *rq) {}
+static inline void sched_core_tick(struct rq *rq)
+{
+}
 
 #endif /* CONFIG_SCHED_CORE && CONFIG_SCHEDSTATS */
 
@@ -1950,14 +2000,16 @@ static inline void set_task_rq(struct task_struct *p, unsigned int cpu)
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	p->rt.rt_rq  = tg->rt_rq[cpu];
+	p->rt.rt_rq = tg->rt_rq[cpu];
 	p->rt.parent = tg->rt_se[cpu];
 #endif
 }
 
 #else /* CONFIG_CGROUP_SCHED */
 
-static inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }
+static inline void set_task_rq(struct task_struct *p, unsigned int cpu)
+{
+}
 static inline struct task_group *task_group(struct task_struct *p)
 {
 	return NULL;
@@ -1984,13 +2036,12 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
 #ifdef CONFIG_SCHED_DEBUG
-# define const_debug __read_mostly
+#define const_debug __read_mostly
 #else
-# define const_debug const
+#define const_debug const
 #endif
 
-#define SCHED_FEAT(name, enabled)	\
-	__SCHED_FEAT_##name ,
+#define SCHED_FEAT(name, enabled) __SCHED_FEAT_##name,
 
 enum {
 #include "features.h"
@@ -2008,11 +2059,12 @@ enum {
 extern const_debug unsigned int sysctl_sched_features;
 
 #ifdef CONFIG_JUMP_LABEL
-#define SCHED_FEAT(name, enabled)					\
-static __always_inline bool static_branch_##name(struct static_key *key) \
-{									\
-	return static_key_##enabled(key);				\
-}
+#define SCHED_FEAT(name, enabled)                         \
+	static __always_inline bool static_branch_##name( \
+		struct static_key *key)                   \
+	{                                                 \
+		return static_key_##enabled(key);         \
+	}
 
 #include "features.h"
 #undef SCHED_FEAT
@@ -2033,8 +2085,7 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
  * constants propagation at compile time and compiler optimization based on
  * features default.
  */
-#define SCHED_FEAT(name, enabled)	\
-	(1UL << __SCHED_FEAT_##name) * enabled |
+#define SCHED_FEAT(name, enabled) (1UL << __SCHED_FEAT_##name) * enabled |
 static const_debug __maybe_unused unsigned int sysctl_sched_features =
 #include "features.h"
 	0;
@@ -2085,11 +2136,11 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 }
 
 /* Wake flags. The first three directly map to some SD flag value */
-#define WF_EXEC     0x02 /* Wakeup after exec; maps to SD_BALANCE_EXEC */
-#define WF_FORK     0x04 /* Wakeup after fork; maps to SD_BALANCE_FORK */
-#define WF_TTWU     0x08 /* Wakeup;            maps to SD_BALANCE_WAKE */
+#define WF_EXEC 0x02 /* Wakeup after exec; maps to SD_BALANCE_EXEC */
+#define WF_FORK 0x04 /* Wakeup after fork; maps to SD_BALANCE_FORK */
+#define WF_TTWU 0x08 /* Wakeup;            maps to SD_BALANCE_WAKE */
 
-#define WF_SYNC     0x10 /* Waker goes to sleep after wakeup */
+#define WF_SYNC 0x10 /* Waker goes to sleep after wakeup */
 #define WF_MIGRATED 0x20 /* Internal use, task got migrated */
 
 #ifdef CONFIG_SMP
@@ -2107,11 +2158,11 @@ static_assert(WF_TTWU == SD_BALANCE_WAKE);
  * slice expiry etc.
  */
 
-#define WEIGHT_IDLEPRIO		3
-#define WMULT_IDLEPRIO		1431655765
+#define WEIGHT_IDLEPRIO 3
+#define WMULT_IDLEPRIO 1431655765
 
-extern const int		sched_prio_to_weight[40];
-extern const u32		sched_prio_to_wmult[40];
+extern const int sched_prio_to_weight[40];
+extern const u32 sched_prio_to_wmult[40];
 
 /*
  * {de,en}queue flags:
@@ -2132,38 +2183,38 @@ extern const u32		sched_prio_to_wmult[40];
  *
  */
 
-#define DEQUEUE_SLEEP		0x01
-#define DEQUEUE_SAVE		0x02 /* Matches ENQUEUE_RESTORE */
-#define DEQUEUE_MOVE		0x04 /* Matches ENQUEUE_MOVE */
-#define DEQUEUE_NOCLOCK		0x08 /* Matches ENQUEUE_NOCLOCK */
+#define DEQUEUE_SLEEP 0x01
+#define DEQUEUE_SAVE 0x02 /* Matches ENQUEUE_RESTORE */
+#define DEQUEUE_MOVE 0x04 /* Matches ENQUEUE_MOVE */
+#define DEQUEUE_NOCLOCK 0x08 /* Matches ENQUEUE_NOCLOCK */
 
-#define ENQUEUE_WAKEUP		0x01
-#define ENQUEUE_RESTORE		0x02
-#define ENQUEUE_MOVE		0x04
-#define ENQUEUE_NOCLOCK		0x08
+#define ENQUEUE_WAKEUP 0x01
+#define ENQUEUE_RESTORE 0x02
+#define ENQUEUE_MOVE 0x04
+#define ENQUEUE_NOCLOCK 0x08
 
-#define ENQUEUE_HEAD		0x10
-#define ENQUEUE_REPLENISH	0x20
+#define ENQUEUE_HEAD 0x10
+#define ENQUEUE_REPLENISH 0x20
 #ifdef CONFIG_SMP
-#define ENQUEUE_MIGRATED	0x40
+#define ENQUEUE_MIGRATED 0x40
 #else
-#define ENQUEUE_MIGRATED	0x00
+#define ENQUEUE_MIGRATED 0x00
 #endif
 
-#define RETRY_TASK		((void *)-1UL)
+#define RETRY_TASK ((void *)-1UL)
 
 struct sched_class {
-
 #ifdef CONFIG_UCLAMP_TASK
 	int uclamp_enabled;
 #endif
 
-	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
-	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
-	void (*yield_task)   (struct rq *rq);
+	void (*enqueue_task)(struct rq *rq, struct task_struct *p, int flags);
+	void (*dequeue_task)(struct rq *rq, struct task_struct *p, int flags);
+	void (*yield_task)(struct rq *rq);
 	bool (*yield_to_task)(struct rq *rq, struct task_struct *p);
 
-	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);
+	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p,
+				   int flags);
 
 	struct task_struct *(*pick_next_task)(struct rq *rq);
 
@@ -2171,18 +2222,18 @@ struct sched_class {
 	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);
 
 #ifdef CONFIG_SMP
-	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
-	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);
+	int (*balance)(struct rq *rq, struct task_struct *prev,
+		       struct rq_flags *rf);
+	int (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);
 
-	struct task_struct * (*pick_task)(struct rq *rq);
+	struct task_struct *(*pick_task)(struct rq *rq);
 
 	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);
 
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
-				 const struct cpumask *newmask,
-				 u32 flags);
+				 const struct cpumask *newmask, u32 flags);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
@@ -2200,17 +2251,17 @@ struct sched_class {
 	 * rq->lock. They are however serialized by p->pi_lock.
 	 */
 	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
-	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
-	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
-			      int oldprio);
+	void (*switched_to)(struct rq *this_rq, struct task_struct *task);
+	void (*prio_changed)(struct rq *this_rq, struct task_struct *task,
+			     int oldprio);
 
 	unsigned int (*get_rr_interval)(struct rq *rq,
 					struct task_struct *task);
 
 	void (*update_curr)(struct rq *rq);
 
-#define TASK_SET_GROUP		0
-#define TASK_MOVE_GROUP		1
+#define TASK_SET_GROUP 0
+#define TASK_MOVE_GROUP 1
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	void (*task_change_group)(struct task_struct *p, int type);
@@ -2228,7 +2279,6 @@ static inline void set_next_task(struct rq *rq, struct task_struct *next)
 	next->sched_class->set_next_task(rq, next, false);
 }
 
-
 /*
  * Helper to define a sched_class instance; each one is placed in a separate
  * section which is ordered by the linker script:
@@ -2239,26 +2289,26 @@ static inline void set_next_task(struct rq *rq, struct task_struct *next)
  *
  * Also enforce alignment on the instance, not the type, to guarantee layout.
  */
-#define DEFINE_SCHED_CLASS(name) \
-const struct sched_class name##_sched_class \
-	__aligned(__alignof__(struct sched_class)) \
-	__section("__" #name "_sched_class")
+#define DEFINE_SCHED_CLASS(name)                                           \
+	const struct sched_class name##_sched_class __aligned(__alignof__( \
+		struct sched_class)) __section("__" #name "_sched_class")
 
 /* Defined in include/asm-generic/vmlinux.lds.h */
 extern struct sched_class __sched_class_highest[];
 extern struct sched_class __sched_class_lowest[];
 
 #define for_class_range(class, _from, _to) \
-	for (class = (_from); class < (_to); class++)
+	for (class = (_from); class < (_to); class ++)
 
 #define for_each_class(class) \
 	for_class_range(class, __sched_class_highest, __sched_class_lowest)
 
-#define sched_class_above(_a, _b)	((_a) < (_b))
+#define sched_class_above(_a, _b) ((_a) < (_b))
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
+extern const struct sched_class rsdl_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
 
@@ -2277,18 +2327,25 @@ static inline bool sched_rt_runnable(struct rq *rq)
 	return rq->rt.rt_queued > 0;
 }
 
+static inline bool sched_rsdl_runnable(struct rq *rq)
+{
+	return rq->rsdl.rsdl_nr_running > 0;
+}
+
 static inline bool sched_fair_runnable(struct rq *rq)
 {
 	return rq->cfs.nr_running > 0;
 }
 
-extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
+extern struct task_struct *pick_next_task_fair(struct rq *rq,
+					       struct task_struct *prev,
+					       struct rq_flags *rf);
 extern struct task_struct *pick_next_task_idle(struct rq *rq);
 
-#define SCA_CHECK		0x01
-#define SCA_MIGRATE_DISABLE	0x02
-#define SCA_MIGRATE_ENABLE	0x04
-#define SCA_USER		0x08
+#define SCA_CHECK 0x01
+#define SCA_MIGRATE_DISABLE 0x02
+#define SCA_MIGRATE_ENABLE 0x04
+#define SCA_USER 0x08
 
 #ifdef CONFIG_SMP
 
@@ -2296,7 +2353,8 @@ extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 
-extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask, u32 flags);
+extern void set_cpus_allowed_common(struct task_struct *p,
+				    const struct cpumask *new_mask, u32 flags);
 
 static inline struct task_struct *get_push_task(struct rq *rq)
 {
@@ -2355,6 +2413,7 @@ extern void update_max_interval(void);
 extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
+extern void init_sched_rsdl_class(void);
 
 extern void reweight_task(struct task_struct *p, int prio);
 
@@ -2362,18 +2421,20 @@ extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
 extern struct rt_bandwidth def_rt_bandwidth;
-extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
+extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period,
+			      u64 runtime);
 extern bool sched_rt_bandwidth_account(struct rt_rq *rt_rq);
 
-extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
+extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period,
+			      u64 runtime);
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
 
-#define BW_SHIFT		20
-#define BW_UNIT			(1 << BW_SHIFT)
-#define RATIO_SHIFT		8
-#define MAX_BW_BITS		(64 - BW_SHIFT)
-#define MAX_BW			((1ULL << MAX_BW_BITS) - 1)
+#define BW_SHIFT 20
+#define BW_UNIT (1 << BW_SHIFT)
+#define RATIO_SHIFT 8
+#define MAX_BW_BITS (64 - BW_SHIFT)
+#define MAX_BW ((1ULL << MAX_BW_BITS) - 1)
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
@@ -2401,8 +2462,13 @@ static inline void sched_update_tick_dependency(struct rq *rq)
 		tick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);
 }
 #else
-static inline int sched_tick_offload_init(void) { return 0; }
-static inline void sched_update_tick_dependency(struct rq *rq) { }
+static inline int sched_tick_offload_init(void)
+{
+	return 0;
+}
+static inline void sched_update_tick_dependency(struct rq *rq)
+{
+}
 #endif
 
 static inline void add_nr_running(struct rq *rq, unsigned count)
@@ -2509,8 +2575,7 @@ static inline int hrtick_enabled(struct rq *rq)
 #endif /* CONFIG_SCHED_HRTICK */
 
 #ifndef arch_scale_freq_tick
-static __always_inline
-void arch_scale_freq_tick(void)
+static __always_inline void arch_scale_freq_tick(void)
 {
 }
 #endif
@@ -2526,8 +2591,7 @@ void arch_scale_freq_tick(void)
  *     ------ * SCHED_CAPACITY_SCALE
  *     f_max
  */
-static __always_inline
-unsigned long arch_scale_freq_capacity(int cpu)
+static __always_inline unsigned long arch_scale_freq_capacity(int cpu)
 {
 	return SCHED_CAPACITY_SCALE;
 }
@@ -2542,14 +2606,16 @@ unsigned long arch_scale_freq_capacity(int cpu)
  */
 static inline void double_rq_clock_clear_update(struct rq *rq1, struct rq *rq2)
 {
-	rq1->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
+	rq1->clock_update_flags &= (RQCF_REQ_SKIP | RQCF_ACT_SKIP);
 	/* rq1 == rq2 for !CONFIG_SMP, so just clear RQCF_UPDATED once. */
 #ifdef CONFIG_SMP
-	rq2->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
+	rq2->clock_update_flags &= (RQCF_REQ_SKIP | RQCF_ACT_SKIP);
 #endif
 }
 #else
-static inline void double_rq_clock_clear_update(struct rq *rq1, struct rq *rq2) {}
+static inline void double_rq_clock_clear_update(struct rq *rq1, struct rq *rq2)
+{
+}
 #endif
 
 #ifdef CONFIG_SMP
@@ -2573,7 +2639,7 @@ static inline bool rq_order_less(struct rq *rq1, struct rq *rq2)
 	if (rq1->core->cpu > rq2->core->cpu)
 		return false;
 
-	/*
+		/*
 	 * __sched_core_flip() relies on SMT having cpu-id lock order.
 	 */
 #endif
@@ -2593,9 +2659,8 @@ extern void double_rq_lock(struct rq *rq1, struct rq *rq2);
  * also adds more overhead and therefore may reduce throughput.
  */
 static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
-	__releases(this_rq->lock)
-	__acquires(busiest->lock)
-	__acquires(this_rq->lock)
+	__releases(this_rq->lock) __acquires(busiest->lock)
+		__acquires(this_rq->lock)
 {
 	raw_spin_rq_unlock(this_rq);
 	double_rq_lock(this_rq, busiest);
@@ -2612,9 +2677,8 @@ static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
  * regardless of entry order into the function.
  */
 static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
-	__releases(this_rq->lock)
-	__acquires(busiest->lock)
-	__acquires(this_rq->lock)
+	__releases(this_rq->lock) __acquires(busiest->lock)
+		__acquires(this_rq->lock)
 {
 	if (__rq_lockp(this_rq) == __rq_lockp(busiest) ||
 	    likely(raw_spin_rq_trylock(busiest))) {
@@ -2688,8 +2752,7 @@ static inline void double_raw_lock(raw_spinlock_t *l1, raw_spinlock_t *l2)
  * you need to do so manually after calling.
  */
 static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
-	__releases(rq1->lock)
-	__releases(rq2->lock)
+	__releases(rq1->lock) __releases(rq2->lock)
 {
 	if (__rq_lockp(rq1) != __rq_lockp(rq2))
 		raw_spin_rq_unlock(rq2);
@@ -2698,7 +2761,7 @@ static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
 	raw_spin_rq_unlock(rq1);
 }
 
-extern void set_rq_online (struct rq *rq);
+extern void set_rq_online(struct rq *rq);
 extern void set_rq_offline(struct rq *rq);
 extern bool sched_smp_initialized;
 
@@ -2711,13 +2774,12 @@ extern bool sched_smp_initialized;
  * you need to do so manually before calling.
  */
 static inline void double_rq_lock(struct rq *rq1, struct rq *rq2)
-	__acquires(rq1->lock)
-	__acquires(rq2->lock)
+	__acquires(rq1->lock) __acquires(rq2->lock)
 {
 	BUG_ON(!irqs_disabled());
 	BUG_ON(rq1 != rq2);
 	raw_spin_rq_lock(rq1);
-	__acquire(rq2->lock);	/* Fake it out ;) */
+	__acquire(rq2->lock); /* Fake it out ;) */
 	double_rq_clock_clear_update(rq1, rq2);
 }
 
@@ -2728,8 +2790,7 @@ static inline void double_rq_lock(struct rq *rq1, struct rq *rq2)
  * you need to do so manually after calling.
  */
 static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
-	__releases(rq1->lock)
-	__releases(rq2->lock)
+	__releases(rq1->lock) __releases(rq2->lock)
 {
 	BUG_ON(rq1 != rq2);
 	raw_spin_rq_unlock(rq1);
@@ -2741,7 +2802,7 @@ static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
 extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
 extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
 
-#ifdef	CONFIG_SCHED_DEBUG
+#ifdef CONFIG_SCHED_DEBUG
 extern bool sched_debug_verbose;
 
 extern void print_cfs_stats(struct seq_file *m, int cpu);
@@ -2753,59 +2814,65 @@ extern void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq);
 
 extern void resched_latency_warn(int cpu, u64 latency);
 #ifdef CONFIG_NUMA_BALANCING
-extern void
-show_numa_stats(struct task_struct *p, struct seq_file *m);
-extern void
-print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
-	unsigned long tpf, unsigned long gsf, unsigned long gpf);
+extern void show_numa_stats(struct task_struct *p, struct seq_file *m);
+extern void print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
+			     unsigned long tpf, unsigned long gsf,
+			     unsigned long gpf);
 #endif /* CONFIG_NUMA_BALANCING */
 #else
-static inline void resched_latency_warn(int cpu, u64 latency) {}
+static inline void resched_latency_warn(int cpu, u64 latency)
+{
+}
 #endif /* CONFIG_SCHED_DEBUG */
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);
+extern void init_rsdl_rq(struct rsdl_rq *rsdl_rq);
 extern void init_dl_rq(struct dl_rq *dl_rq);
 
 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
 
 #ifdef CONFIG_NO_HZ_COMMON
-#define NOHZ_BALANCE_KICK_BIT	0
-#define NOHZ_STATS_KICK_BIT	1
-#define NOHZ_NEWILB_KICK_BIT	2
-#define NOHZ_NEXT_KICK_BIT	3
+#define NOHZ_BALANCE_KICK_BIT 0
+#define NOHZ_STATS_KICK_BIT 1
+#define NOHZ_NEWILB_KICK_BIT 2
+#define NOHZ_NEXT_KICK_BIT 3
 
 /* Run rebalance_domains() */
-#define NOHZ_BALANCE_KICK	BIT(NOHZ_BALANCE_KICK_BIT)
+#define NOHZ_BALANCE_KICK BIT(NOHZ_BALANCE_KICK_BIT)
 /* Update blocked load */
-#define NOHZ_STATS_KICK		BIT(NOHZ_STATS_KICK_BIT)
+#define NOHZ_STATS_KICK BIT(NOHZ_STATS_KICK_BIT)
 /* Update blocked load when entering idle */
-#define NOHZ_NEWILB_KICK	BIT(NOHZ_NEWILB_KICK_BIT)
+#define NOHZ_NEWILB_KICK BIT(NOHZ_NEWILB_KICK_BIT)
 /* Update nohz.next_balance */
-#define NOHZ_NEXT_KICK		BIT(NOHZ_NEXT_KICK_BIT)
+#define NOHZ_NEXT_KICK BIT(NOHZ_NEXT_KICK_BIT)
 
-#define NOHZ_KICK_MASK	(NOHZ_BALANCE_KICK | NOHZ_STATS_KICK | NOHZ_NEXT_KICK)
+#define NOHZ_KICK_MASK (NOHZ_BALANCE_KICK | NOHZ_STATS_KICK | NOHZ_NEXT_KICK)
 
-#define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
+#define nohz_flags(cpu) (&cpu_rq(cpu)->nohz_flags)
 
 extern void nohz_balance_exit_idle(struct rq *rq);
 #else
-static inline void nohz_balance_exit_idle(struct rq *rq) { }
+static inline void nohz_balance_exit_idle(struct rq *rq)
+{
+}
 #endif
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void nohz_run_idle_balance(int cpu);
 #else
-static inline void nohz_run_idle_balance(int cpu) { }
+static inline void nohz_run_idle_balance(int cpu)
+{
+}
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 struct irqtime {
-	u64			total;
-	u64			tick_delta;
-	u64			irq_start_time;
-	struct u64_stats_sync	sync;
+	u64 total;
+	u64 tick_delta;
+	u64 irq_start_time;
+	struct u64_stats_sync sync;
 };
 
 DECLARE_PER_CPU(struct irqtime, cpu_irqtime);
@@ -2859,21 +2926,23 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
 {
 	struct update_util_data *data;
 
-	data = rcu_dereference_sched(*per_cpu_ptr(&cpufreq_update_util_data,
-						  cpu_of(rq)));
+	data = rcu_dereference_sched(
+		*per_cpu_ptr(&cpufreq_update_util_data, cpu_of(rq)));
 	if (data)
 		data->func(data, rq_clock(rq), flags);
 }
 #else
-static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
+static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
+{
+}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef arch_scale_freq_capacity
-# ifndef arch_scale_freq_invariant
-#  define arch_scale_freq_invariant()	true
-# endif
+#ifndef arch_scale_freq_invariant
+#define arch_scale_freq_invariant() true
+#endif
 #else
-# define arch_scale_freq_invariant()	false
+#define arch_scale_freq_invariant() false
 #endif
 
 #ifdef CONFIG_SMP
@@ -3002,9 +3071,8 @@ static inline bool uclamp_rq_is_idle(struct rq *rq)
  * will return the correct effective uclamp value of the task even if the
  * static key is disabled.
  */
-static __always_inline
-unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
-				  struct task_struct *p)
+static __always_inline unsigned long
+uclamp_rq_util_with(struct rq *rq, unsigned long util, struct task_struct *p)
 {
 	unsigned long min_util = 0;
 	unsigned long max_util = 0;
@@ -3024,8 +3092,10 @@ unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
 			goto out;
 	}
 
-	min_util = max_t(unsigned long, min_util, uclamp_rq_get(rq, UCLAMP_MIN));
-	max_util = max_t(unsigned long, max_util, uclamp_rq_get(rq, UCLAMP_MAX));
+	min_util =
+		max_t(unsigned long, min_util, uclamp_rq_get(rq, UCLAMP_MIN));
+	max_util =
+		max_t(unsigned long, max_util, uclamp_rq_get(rq, UCLAMP_MAX));
 out:
 	/*
 	 * Since CPU's {min,max}_util clamps are MAX aggregated considering
@@ -3075,14 +3145,16 @@ static inline unsigned long uclamp_eff_value(struct task_struct *p,
 	return SCHED_CAPACITY_SCALE;
 }
 
-static inline
-unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
-				  struct task_struct *p)
+static inline unsigned long
+uclamp_rq_util_with(struct rq *rq, unsigned long util, struct task_struct *p)
 {
 	return util;
 }
 
-static inline bool uclamp_rq_is_capped(struct rq *rq) { return false; }
+static inline bool uclamp_rq_is_capped(struct rq *rq)
+{
+	return false;
+}
 
 static inline bool uclamp_is_used(void)
 {
@@ -3115,14 +3187,13 @@ static inline unsigned long cpu_util_irq(struct rq *rq)
 	return rq->avg_irq.util_avg;
 }
 
-static inline
-unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
+static inline unsigned long
+scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
 {
 	util *= (max - irq);
 	util /= max;
 
 	return util;
-
 }
 #else
 static inline unsigned long cpu_util_irq(struct rq *rq)
@@ -3130,8 +3201,8 @@ static inline unsigned long cpu_util_irq(struct rq *rq)
 	return 0;
 }
 
-static inline
-unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
+static inline unsigned long
+scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
 {
 	return util;
 }
@@ -3151,7 +3222,10 @@ static inline bool sched_energy_enabled(void)
 #else /* ! (CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL) */
 
 #define perf_domain_span(pd) NULL
-static inline bool sched_energy_enabled(void) { return false; }
+static inline bool sched_energy_enabled(void)
+{
+	return false;
+}
 
 #endif /* CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL */
 
@@ -3199,7 +3273,8 @@ static inline bool is_per_cpu_kthread(struct task_struct *p)
 #endif
 
 extern void swake_up_all_locked(struct swait_queue_head *q);
-extern void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);
+extern void __prepare_to_swait(struct swait_queue_head *q,
+			       struct swait_queue *wait);
 
 #ifdef CONFIG_PREEMPT_DYNAMIC
 extern int preempt_dynamic_mode;
diff --git a/scripts/dtc/include-prefixes/arc b/scripts/dtc/include-prefixes/arc
deleted file mode 120000
index 5d21b5a69..000000000
--- a/scripts/dtc/include-prefixes/arc
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/arc/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/arm b/scripts/dtc/include-prefixes/arm
deleted file mode 120000
index eb14d4515..000000000
--- a/scripts/dtc/include-prefixes/arm
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/arm/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/arm64 b/scripts/dtc/include-prefixes/arm64
deleted file mode 120000
index 275c42c21..000000000
--- a/scripts/dtc/include-prefixes/arm64
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/arm64/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/dt-bindings b/scripts/dtc/include-prefixes/dt-bindings
deleted file mode 120000
index 04fdbb3af..000000000
--- a/scripts/dtc/include-prefixes/dt-bindings
+++ /dev/null
@@ -1 +0,0 @@
-../../../include/dt-bindings
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/microblaze b/scripts/dtc/include-prefixes/microblaze
deleted file mode 120000
index d9830330a..000000000
--- a/scripts/dtc/include-prefixes/microblaze
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/microblaze/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/mips b/scripts/dtc/include-prefixes/mips
deleted file mode 120000
index ae8d4948d..000000000
--- a/scripts/dtc/include-prefixes/mips
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/mips/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/nios2 b/scripts/dtc/include-prefixes/nios2
deleted file mode 120000
index 51772336d..000000000
--- a/scripts/dtc/include-prefixes/nios2
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/nios2/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/openrisc b/scripts/dtc/include-prefixes/openrisc
deleted file mode 120000
index 71c3bc75c..000000000
--- a/scripts/dtc/include-prefixes/openrisc
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/openrisc/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/powerpc b/scripts/dtc/include-prefixes/powerpc
deleted file mode 120000
index 7cd6ec16e..000000000
--- a/scripts/dtc/include-prefixes/powerpc
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/powerpc/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/sh b/scripts/dtc/include-prefixes/sh
deleted file mode 120000
index 67d37808c..000000000
--- a/scripts/dtc/include-prefixes/sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/sh/boot/dts
\ No newline at end of file
diff --git a/scripts/dtc/include-prefixes/xtensa b/scripts/dtc/include-prefixes/xtensa
deleted file mode 120000
index d1eaf6ec7..000000000
--- a/scripts/dtc/include-prefixes/xtensa
+++ /dev/null
@@ -1 +0,0 @@
-../../../arch/xtensa/boot/dts
\ No newline at end of file
diff --git a/scripts/dummy-tools/nm b/scripts/dummy-tools/nm
deleted file mode 120000
index c0648b38d..000000000
--- a/scripts/dummy-tools/nm
+++ /dev/null
@@ -1 +0,0 @@
-ld
\ No newline at end of file
diff --git a/scripts/dummy-tools/objcopy b/scripts/dummy-tools/objcopy
deleted file mode 120000
index c0648b38d..000000000
--- a/scripts/dummy-tools/objcopy
+++ /dev/null
@@ -1 +0,0 @@
-ld
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/bonding/net_forwarding_lib.sh b/tools/testing/selftests/drivers/net/bonding/net_forwarding_lib.sh
deleted file mode 120000
index 39c96828c..000000000
--- a/tools/testing/selftests/drivers/net/bonding/net_forwarding_lib.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/lib.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_locked_port.sh b/tools/testing/selftests/drivers/net/dsa/bridge_locked_port.sh
deleted file mode 120000
index f5eb940c4..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_locked_port.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_locked_port.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_mdb.sh b/tools/testing/selftests/drivers/net/dsa/bridge_mdb.sh
deleted file mode 120000
index 76492da52..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_mdb.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_mdb.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_mld.sh b/tools/testing/selftests/drivers/net/dsa/bridge_mld.sh
deleted file mode 120000
index 81a7e0df0..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_mld.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_mld.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_aware.sh b/tools/testing/selftests/drivers/net/dsa/bridge_vlan_aware.sh
deleted file mode 120000
index 9831ed743..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_aware.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_vlan_aware.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_mcast.sh b/tools/testing/selftests/drivers/net/dsa/bridge_vlan_mcast.sh
deleted file mode 120000
index 7f3c3f0bf..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_mcast.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_vlan_mcast.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_unaware.sh b/tools/testing/selftests/drivers/net/dsa/bridge_vlan_unaware.sh
deleted file mode 120000
index bf1a57e6b..000000000
--- a/tools/testing/selftests/drivers/net/dsa/bridge_vlan_unaware.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/bridge_vlan_unaware.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/lib.sh b/tools/testing/selftests/drivers/net/dsa/lib.sh
deleted file mode 120000
index 39c96828c..000000000
--- a/tools/testing/selftests/drivers/net/dsa/lib.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/lib.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/local_termination.sh b/tools/testing/selftests/drivers/net/dsa/local_termination.sh
deleted file mode 120000
index c08166f84..000000000
--- a/tools/testing/selftests/drivers/net/dsa/local_termination.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/local_termination.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/dsa/no_forwarding.sh b/tools/testing/selftests/drivers/net/dsa/no_forwarding.sh
deleted file mode 120000
index b9757466b..000000000
--- a/tools/testing/selftests/drivers/net/dsa/no_forwarding.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/no_forwarding.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/mlxsw/spectrum-2/rif_counter_scale.sh b/tools/testing/selftests/drivers/net/mlxsw/spectrum-2/rif_counter_scale.sh
deleted file mode 120000
index 1f5752e8f..000000000
--- a/tools/testing/selftests/drivers/net/mlxsw/spectrum-2/rif_counter_scale.sh
+++ /dev/null
@@ -1 +0,0 @@
-../spectrum/rif_counter_scale.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/team/lag_lib.sh b/tools/testing/selftests/drivers/net/team/lag_lib.sh
deleted file mode 120000
index e1347a10a..000000000
--- a/tools/testing/selftests/drivers/net/team/lag_lib.sh
+++ /dev/null
@@ -1 +0,0 @@
-../bonding/lag_lib.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/drivers/net/team/net_forwarding_lib.sh b/tools/testing/selftests/drivers/net/team/net_forwarding_lib.sh
deleted file mode 120000
index 39c96828c..000000000
--- a/tools/testing/selftests/drivers/net/team/net_forwarding_lib.sh
+++ /dev/null
@@ -1 +0,0 @@
-../../../net/forwarding/lib.sh
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/copy_mc_64.S b/tools/testing/selftests/powerpc/copyloops/copy_mc_64.S
deleted file mode 120000
index dcbe06d50..000000000
--- a/tools/testing/selftests/powerpc/copyloops/copy_mc_64.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/copy_mc_64.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/copyuser_64.S b/tools/testing/selftests/powerpc/copyloops/copyuser_64.S
deleted file mode 120000
index f1c418a25..000000000
--- a/tools/testing/selftests/powerpc/copyloops/copyuser_64.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/copyuser_64.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/copyuser_power7.S b/tools/testing/selftests/powerpc/copyloops/copyuser_power7.S
deleted file mode 120000
index 478689598..000000000
--- a/tools/testing/selftests/powerpc/copyloops/copyuser_power7.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/copyuser_power7.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/mem_64.S b/tools/testing/selftests/powerpc/copyloops/mem_64.S
deleted file mode 120000
index db254c9a5..000000000
--- a/tools/testing/selftests/powerpc/copyloops/mem_64.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/mem_64.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/memcpy_64.S b/tools/testing/selftests/powerpc/copyloops/memcpy_64.S
deleted file mode 120000
index cce33fb6f..000000000
--- a/tools/testing/selftests/powerpc/copyloops/memcpy_64.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/memcpy_64.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/copyloops/memcpy_power7.S b/tools/testing/selftests/powerpc/copyloops/memcpy_power7.S
deleted file mode 120000
index 0d6fbfaf3..000000000
--- a/tools/testing/selftests/powerpc/copyloops/memcpy_power7.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/memcpy_power7.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/mce/vas-api.h b/tools/testing/selftests/powerpc/mce/vas-api.h
deleted file mode 120000
index 1455c1bcd..000000000
--- a/tools/testing/selftests/powerpc/mce/vas-api.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/include/uapi/asm/vas-api.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/nx-gzip/include/vas-api.h b/tools/testing/selftests/powerpc/nx-gzip/include/vas-api.h
deleted file mode 120000
index 77fb4c723..000000000
--- a/tools/testing/selftests/powerpc/nx-gzip/include/vas-api.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/uapi/asm/vas-api.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/asm/asm-compat.h b/tools/testing/selftests/powerpc/primitives/asm/asm-compat.h
deleted file mode 120000
index b14255e15..000000000
--- a/tools/testing/selftests/powerpc/primitives/asm/asm-compat.h
+++ /dev/null
@@ -1 +0,0 @@
-../.././../../../../arch/powerpc/include/asm/asm-compat.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/asm/asm-const.h b/tools/testing/selftests/powerpc/primitives/asm/asm-const.h
deleted file mode 120000
index 18d8be13e..000000000
--- a/tools/testing/selftests/powerpc/primitives/asm/asm-const.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/asm/asm-const.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/asm/extable.h b/tools/testing/selftests/powerpc/primitives/asm/extable.h
deleted file mode 120000
index 6385f059a..000000000
--- a/tools/testing/selftests/powerpc/primitives/asm/extable.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/asm/extable.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/asm/feature-fixups.h b/tools/testing/selftests/powerpc/primitives/asm/feature-fixups.h
deleted file mode 120000
index 8dc6d4d46..000000000
--- a/tools/testing/selftests/powerpc/primitives/asm/feature-fixups.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/asm/feature-fixups.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/asm/ppc_asm.h b/tools/testing/selftests/powerpc/primitives/asm/ppc_asm.h
deleted file mode 120000
index 66c819322..000000000
--- a/tools/testing/selftests/powerpc/primitives/asm/ppc_asm.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/asm/ppc_asm.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/primitives/word-at-a-time.h b/tools/testing/selftests/powerpc/primitives/word-at-a-time.h
deleted file mode 120000
index eb74401b5..000000000
--- a/tools/testing/selftests/powerpc/primitives/word-at-a-time.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/include/asm/word-at-a-time.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/stringloops/memcmp_32.S b/tools/testing/selftests/powerpc/stringloops/memcmp_32.S
deleted file mode 120000
index 056f2b3af..000000000
--- a/tools/testing/selftests/powerpc/stringloops/memcmp_32.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/memcmp_32.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/stringloops/memcmp_64.S b/tools/testing/selftests/powerpc/stringloops/memcmp_64.S
deleted file mode 120000
index 9bc87e438..000000000
--- a/tools/testing/selftests/powerpc/stringloops/memcmp_64.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/memcmp_64.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/stringloops/strlen_32.S b/tools/testing/selftests/powerpc/stringloops/strlen_32.S
deleted file mode 120000
index 72b13731b..000000000
--- a/tools/testing/selftests/powerpc/stringloops/strlen_32.S
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/lib/strlen_32.S
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/vphn/asm/lppaca.h b/tools/testing/selftests/powerpc/vphn/asm/lppaca.h
deleted file mode 120000
index 942b1d009..000000000
--- a/tools/testing/selftests/powerpc/vphn/asm/lppaca.h
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../../arch/powerpc/include/asm/lppaca.h
\ No newline at end of file
diff --git a/tools/testing/selftests/powerpc/vphn/vphn.c b/tools/testing/selftests/powerpc/vphn/vphn.c
deleted file mode 120000
index 5b5fbddcc..000000000
--- a/tools/testing/selftests/powerpc/vphn/vphn.c
+++ /dev/null
@@ -1 +0,0 @@
-../../../../../arch/powerpc/platforms/pseries/vphn.c
\ No newline at end of file
-- 
2.34.1


From 086bb17b3cac48d7ee33004b103566691eda198f Mon Sep 17 00:00:00 2001
From: Milin <milinbhade@iisc.ac.in>
Date: Tue, 28 Feb 2023 22:18:39 +0530
Subject: [PATCH 3/3] OS Assignment-1

Signed-off-by: Milin <milinbhade@iisc.ac.in>
---
 include/linux/list.h       |   3 +-
 include/linux/sched.h      |   5 +-
 include/linux/sched/prio.h |   4 +
 include/linux/sched/rsdl.h |   2 +-
 kernel/sched/core.c        |  16 +-
 kernel/sched/rsdl.c        | 593 +++++++++++++++++++++++++++++++++----
 kernel/sched/rt.c          |   1 -
 kernel/sched/sched.h       |  25 +-
 8 files changed, 585 insertions(+), 64 deletions(-)

diff --git a/include/linux/list.h b/include/linux/list.h
index 61762054b..409599736 100644
--- a/include/linux/list.h
+++ b/include/linux/list.h
@@ -97,8 +97,9 @@ static inline void list_add(struct list_head *new, struct list_head *head)
  * Insert a new entry before the specified head.
  * This is useful for implementing queues.
  */
+
 static inline void list_add_tail(struct list_head *new, struct list_head *head)
-{
+{	
 	__list_add(new, head->prev, head);
 }
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 820f1a4ad..753f4f7a4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -592,11 +592,14 @@ struct sched_rt_entity {
 ////////// Sched_rsdl_entity added /////////
 
 struct sched_rsdl_entity {
+	unsigned int			quota;
 	struct list_head		run_list;
+	unsigned int			time_slice;
 	unsigned short			on_rq;
 	unsigned short			on_list;
+	unsigned int			new;
 
-} __randomize_layout;
+};
 
 
 
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index a16996aa7..40a11aad0 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -15,6 +15,7 @@
 
 #define MAX_RT_PRIO		100
 #define MAX_RSDL_PRIO		40
+#define MAX_RQ_QUOTA		20
 
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
@@ -27,6 +28,9 @@
 #define NICE_TO_PRIO(nice)	((nice) + DEFAULT_PRIO)
 #define PRIO_TO_NICE(prio)	((prio) - DEFAULT_PRIO)
 
+
+#define PRIO_TO_IDX(prio)  ((prio) - MAX_RT_PRIO)
+#define IDX_TO_PRIO(idx)  ((idx) + MAX_RT_PRIO)
 /*
  * Convert nice value [19,-20] to rlimit style value [1,40].
  */
diff --git a/include/linux/sched/rsdl.h b/include/linux/sched/rsdl.h
index 837f11870..3dfbe77db 100644
--- a/include/linux/sched/rsdl.h
+++ b/include/linux/sched/rsdl.h
@@ -26,6 +26,6 @@ static inline bool task_is_rsdl(struct task_struct *tsk)
 		return true;
 	return false;
 }
-
+#define RSDL_TIMESLICE		5
 
 #endif /* _LINUX_SCHED_RSDL_H */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b382acf23..7a5aae2b8 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4386,8 +4386,16 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->rt.on_list = 0;
 
 	//////// Add initialization related to rsdl entity p->rsdl
-
+	// printk("Initializing parameters \t");
+	INIT_LIST_HEAD(&p->rsdl.run_list);
+	// p->.timeout = 0;
+	p->rsdl.quota = 5;
+	p->rsdl.time_slice = 5;
+	p->rsdl.on_rq = 0;
+	p->rsdl.on_list = 0;
+	p->rsdl.new = 1;
 	///////////////////////////////////////////////////////////
+	// printk("Completed Initializing parameters \t");
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
@@ -4578,7 +4586,6 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
 	p->prio = current->normal_prio;
-
 	uclamp_fork(p);
 
 	/*
@@ -4606,8 +4613,8 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		return -EAGAIN;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
-	// else if (rsdl_prio(p->prio) && rsdl_policy(p->policy))
-	// 	p->sched_class = &rsdl_sched_class;
+	else if (rsdl_prio(p->prio) && rsdl_policy(p->policy))
+		p->sched_class = &rsdl_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 
@@ -6571,6 +6578,7 @@ void __noreturn do_task_dead(void)
 	current->flags |= PF_NOFREEZE;
 
 	__schedule(SM_NONE);
+	//pr_crit("XXX: %u %d\n", current->__state, current->on_rq);
 	BUG();
 
 	/* Avoid "noreturn function does return" - but don't continue if BUG() is a NOP: */
diff --git a/kernel/sched/rsdl.c b/kernel/sched/rsdl.c
index 10eb816c8..a6984f387 100644
--- a/kernel/sched/rsdl.c
+++ b/kernel/sched/rsdl.c
@@ -1,5 +1,6 @@
 ////////////////// Initialize sched rsdl class //////////////////
-
+// int sched_rsdl_timeslice = RSDL_TIMESLICE;
+int sched_rsdl_timeslice = 5;
 void __init init_sched_rsdl_class(void)
 {
 	unsigned int i;
@@ -22,12 +23,16 @@ void init_rsdl_rq(struct rsdl_rq *rsdl_rq)
     struct rsdl_prio_array *array2;
 
 	int i;
-
+	rsdl_rq->curr = NULL;
 	printk("Init RSDL runqueue function called\n");
 
-	array1 = &rsdl_rq->active;
+	array1 = &rsdl_rq->activeA;
 	for (i = 0; i < MAX_RSDL_PRIO; i++) {
 		INIT_LIST_HEAD(array1->queue + i);
+		// INIT_LIST_HEAD(&((array1->queue)[i].head));
+		// array1->queue[i].quota = 20;
+		rsdl_rq->prio_quota[i] = 20;
+		rsdl_rq->prio_queued[i] = 0;
 		printk("RSDL active runqueue function initialized\n");
 		__clear_bit(i, array1->bitmap);
 	}
@@ -35,148 +40,630 @@ void init_rsdl_rq(struct rsdl_rq *rsdl_rq)
 	__set_bit(MAX_RSDL_PRIO, array1->bitmap);
 
 
-	array2 = &rsdl_rq->expired;
+	array2 = &rsdl_rq->expiredA;
 	for (i = 0; i < MAX_RSDL_PRIO; i++) {
 		INIT_LIST_HEAD(array2->queue + i);
+		// INIT_LIST_HEAD(&((array2->queue)[i].head));
+		// array1->queue[i].quota = 20;
+
 		printk("RSDL expired runqueue function initialized\n");
 		__clear_bit(i, array2->bitmap);
 	}
 	/* delimiter for bitsearch: */
 	__set_bit(MAX_RSDL_PRIO, array2->bitmap);
 
+	rsdl_rq->active = &rsdl_rq->activeA;
+	rsdl_rq->expired = &rsdl_rq->expiredA;
+
     rsdl_rq->rsdl_nr_running = 0;
+	rsdl_rq->curr_rq = 0;
+	spin_lock_init(&rsdl_rq->rsdl_runtime_lock);
 
 }
+////////////////// Basic hook function definations //////////////////
 
 
-////////////////// Basic hook function definations //////////////////
-static void
-enqueue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+static inline int on_rsdl_rq(struct sched_rsdl_entity *rsdl_se)
 {
-	printk("Enqueue Task Called\n");
+	return rsdl_se->on_rq;
 }
 
+static inline int task_queued(struct task_struct *task)
+{
+	return task->rsdl.on_list;
+}
 
-static void dequeue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+
+#define rsdl_rq_quota(rsdl_rq, prio)	((rsdl_rq)->prio_quota[prio])
+
+static void update_curr_rsdl(struct rq *rq)
 {
-	printk("Dequeue Task Called\n");
+
+	struct task_struct *curr = rq->curr;
+	//struct sched_rsdl_entity *rsdl_se = &curr->rsdl;
+	u64 delta_exec;
+	u64 now;
+
+	if (curr->sched_class != &rsdl_sched_class)
+		return;
+
+	now = rq_clock_task(rq);
+	delta_exec = now - curr->se.exec_start;
+	if (unlikely((s64)delta_exec <= 0))
+		return;
+
+	schedstat_set(curr->stats.exec_max,
+		      max(curr->stats.exec_max, delta_exec));
+
+	trace_sched_stat_runtime(curr, delta_exec, 0);
+
+	curr->se.sum_exec_runtime += delta_exec;
+	account_group_exec_runtime(curr, delta_exec);
+
+	curr->se.exec_start = now;
+	cgroup_account_cputime(curr, delta_exec);
+
+	//printk("Entered update curr rsdl\n");
+
 }
 
+///////////////////////////////////////////Enqueue//////////////////////////////////////////////////////////////////////
+static inline struct task_struct *rsdl_task_of(struct sched_rsdl_entity *rsdl_se)
+{	
+	if(rsdl_se == NULL){
+		return NULL;
+	}
+	return container_of(rsdl_se, struct task_struct, rsdl);
+}
 
-static void yield_task_rsdl(struct rq *rq)
+static inline struct rq *rq_of_rsdl_se(struct sched_rsdl_entity *rsdl_se)
 {
-	printk("Yield Task Called\n");
+	struct task_struct *p = rsdl_task_of(rsdl_se);
+
+	return task_rq(p);
 }
+static inline struct rsdl_rq *rsdl_rq_of_se(struct sched_rsdl_entity *rsdl_se)
+{
+	struct rq *rq = rq_of_rsdl_se(rsdl_se);
 
+	return &rq->rsdl;
+}
 
-static void check_preempt_curr_rsdl(struct rq *rq, struct task_struct *p, int flags)
+static inline int rsdl_se_prio(struct sched_rsdl_entity *rsdl_se)
 {
-    printk("Check_preempt_curr_rsdl Task Called\n");
+	return rsdl_task_of(rsdl_se)->prio;
 }
 
+static inline
+void dec_rsdl_tasks(struct sched_rsdl_entity *rsdl_se, struct rsdl_rq *rsdl_rq, unsigned int level)
+{
+	printk("Task at current level :{%u} before incr: %ld", level, rsdl_rq->prio_queued[level]);
+	rsdl_rq->rsdl_nr_running -= 1;
+	rsdl_rq->prio_queued[level] -= 1;
+	printk("Decrement task called");
+	printk("Tasks Total : %d", rsdl_rq->rsdl_nr_running);
+	printk("Task at current level :{%u} after decr: %ld", level, rsdl_rq->prio_queued[level]);
+}
 
-static struct task_struct *pick_next_task_rsdl(struct rq *rq)
+static inline
+void inc_rsdl_tasks(struct sched_rsdl_entity *rsdl_se, struct rsdl_rq *rsdl_rq, unsigned int level)
 {
-    struct task_struct *p = NULL;
-    printk("pick_next_task_rsdl Task Called\n");
-    return p;
+	printk("Task at current level :{%u} before incr: %ld", level, rsdl_rq->prio_queued[level]);
+	rsdl_rq->rsdl_nr_running += 1;
+	rsdl_rq->prio_queued[level] += 1;
+	printk("Increment task called");
+	printk("Tasks Total : %d", rsdl_rq->rsdl_nr_running);
+	printk("Task at current level :{%u} after incr: %ld", level, rsdl_rq->prio_queued[level]);
 }
 
 
+static void __enqueue_rsdl_entity(struct sched_rsdl_entity *rsdl_se, unsigned int flags)
+{
+	struct rsdl_rq *rsdl_rq = rsdl_rq_of_se(rsdl_se);
+	struct task_struct *p = rsdl_task_of(rsdl_se);
+	struct rsdl_prio_array *array = rsdl_rq->active;
+	unsigned int pr = rsdl_se_prio(rsdl_se) - MAX_RT_PRIO;
+	unsigned int curr = rsdl_rq->curr_rq;
+	unsigned int idx = (pr >= curr) ? pr : curr;
+	struct list_head *queue = array->queue + idx;    /// Make changes here to add in current active list
+	printk("Priority : {%u}, Index chosen: {%u},  Current Runqueue: {%u}", pr, idx, curr);
+	p->prio = IDX_TO_PRIO(idx);
+
+	if(rsdl_se->new == 1){  // Start with timeslice of 5 
+		rsdl_se->new = 0;
+		// p->prio = idx;
+		rsdl_se->time_slice = sched_rsdl_timeslice;
+	}else{ // Start with timeslice of all accumulated timeslices
+		rsdl_se->time_slice = max((int)0, (int)(curr - pr)) * sched_rsdl_timeslice + rsdl_se->time_slice;    // reverify this logic
+	}
+
+	// printk("__enqueue_rsdl_entity called");
+	list_add_tail(&rsdl_se->run_list, queue);
+	// printk("added to list tail");
+	__set_bit(idx, array->bitmap);
+	// printk("set bit called");
+	rsdl_se->on_list = 1;
+	rsdl_se->on_rq = 1;
+	inc_rsdl_tasks(rsdl_se, rsdl_rq, idx);
+	printk("Enqueue success : pid{%d} at queue {%d}", p->pid, idx);
+}
+
+static void enqueue_rsdl_entity(struct sched_rsdl_entity *rsdl_se, unsigned int flags)
+{
+	// struct rq *rq = rq_of_rsdl_se(rsdl_se);
+	struct task_struct *p = rsdl_task_of(rsdl_se);
+	printk("enqueue_rsdl_entity called");
+	
+	__enqueue_rsdl_entity(rsdl_se, flags);
+	printk("Pid to be enqueued : %d", p->pid);
+}
+
+static void
+enqueue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{	
+	struct sched_rsdl_entity *rsdl_se = &p->rsdl;
+	
+	
+	update_curr_rsdl(rq);
+	//Check nr_running
+	printk("Currently Running CPU :  %d\n", rq->cpu);
+	printk("No of tasks running on this cpu before enqueue : %d\n", rq->nr_running);
+
+	enqueue_rsdl_entity(rsdl_se, flags);
+
+	//printk("Enqueue Task Called\n");
+
+	add_nr_running(rq, 1);
+	// 
+	set_tsk_need_resched(p);
+	printk("Currently Running CPU :  %d\n", rq->cpu);
+	printk("No of tasks running on this cpu after enqueue : %d\n", rq->nr_running);
+}
+
+///////////////////////////////////////////Dequeue//////////////////////////////////////////////////////////////////////
+
+
+static void dequeue_task_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{	
+	struct sched_rsdl_entity *rsdl_se = &p->rsdl;
+	struct rsdl_rq *rsdl_rq = rsdl_rq_of_se(rsdl_se);
+	//struct rsdl_prio_array *array = rsdl_rq->active;
+	// unsigned int pr = rsdl_se_prio(rsdl_se) - MAX_RT_PRIO;
+	unsigned int curr = rsdl_rq->curr_rq;
+	// struct list_head *queue = array->queue + idx;
+
+	update_curr_rsdl(rq);
+	list_del_init(&rsdl_se->run_list);
+	sub_nr_running(rq, 1);
+	// inc_rsdl_tasks(rsdl_se, rsdl_rq, idx);
+	dec_rsdl_tasks(rsdl_se, rsdl_rq, curr);
+	
+	printk("++++++++++++++++++++++++Dequeue Task Called on task : pid {%d}+++++++++++++++++++++++\n", p->pid);
+}
+
+
+///////////////////////////////////////////Dequeue//////////////////////////////////////////////////////////////////////
+
+
+
+
+static void check_preempt_curr_rsdl(struct rq *rq, struct task_struct *p, int flags)
+{
+	// printk("Entered check preempt curr rsdl\n");
+
+}
+
+/////////////////////////////////////////// Pick Next Task //////////////////////////////////////////////////////////////////////
+
+
 static void put_prev_task_rsdl(struct rq *rq, struct task_struct *p)
 {
-	printk("put_prev_task_rsdl Task Called\n");
+	// printk("Entered put previous task rsdl\n");
 }
 
 
 static inline void set_next_task_rsdl(struct rq *rq, struct task_struct *p, bool first)
 {
-	printk("set_next_task_rsdl Task Called\n");
+	// printk("Entered set next task from rsdl\n");
+
 }
 
 
-static void task_tick_rsdl(struct rq *rq, struct task_struct *p, int queued)
+/////////////////////////////////////////// Pick Next Task //////////////////////////////////////////////////////////////////////
+
+static inline void major_rotation(struct rsdl_rq *rsdl_rq)
+{	
+
+	struct rsdl_prio_array *new_array = rsdl_rq->expired;
+	printk("Major rotation Called\n");
+	rsdl_rq->expired = rsdl_rq->active;
+	printk("Major rotation Called step 1\n");
+	rsdl_rq->active = new_array;
+	printk("Major rotation Called step 2\n");
+	
+}
+static struct sched_rsdl_entity *pick_next_rsdl_entity(struct rsdl_rq *rsdl_rq)
+{	
+	struct rsdl_prio_array *array = rsdl_rq->active;
+	struct sched_rsdl_entity *next = NULL;
+	struct list_head *queue;
+	struct task_struct *p;
+	int idx;
+
+	idx = rsdl_rq->curr_rq;
+	// BUG_ON(idx >= MAX_RSDL_PRIO);
+	if(unlikely(idx >= MAX_RSDL_PRIO)){
+		printk("Major Rotation Initiated\n");
+		//as exhausted current active runqueue
+		major_rotation(rsdl_rq);
+		array = rsdl_rq->active;
+		idx -= MAX_RSDL_PRIO;
+		rsdl_rq->curr_rq = idx;
+	}
+	
+	queue = array->queue + idx;
+	while(list_empty(queue)) {
+		idx += 1;
+		rsdl_rq->curr_rq = idx;
+		//// Just a hack to make it work  -->>>>>>>>>>>>>>>>>Please change this code
+		if(rsdl_rq->curr_rq == MAX_RSDL_PRIO){
+			rsdl_rq->rsdl_nr_running = 0;
+			rsdl_rq->curr_rq = 0;
+			major_rotation(rsdl_rq);
+			return NULL;
+		}
+		//// Remove above code as soon as possible
+		
+		queue = array->queue + idx;
+		printk("Currently at index {%d}\n", idx);
+	}
+	//rsdl_rq->curr_rq = idx;
+	printk("Task Picked from queue : {%d}, Remaining in queue : {%ld}, Curr_rq: {%d}\n", idx, rsdl_rq->prio_queued[idx], rsdl_rq->curr_rq);
+	next = list_entry(queue->next, struct sched_rsdl_entity, run_list);
+
+	/*
+	* next needs to have its prio and array reset here in case values are wrong
+	* due to priority rotation 
+	*/
+	p = rsdl_task_of(next);
+	// if(PRIO_TO_IDX(p->prio) != rsdl_rq->curr_rq){  // due to rotation of 
+	// 	next->time_slice += sched_rsdl_timeslice;
+	// }
+	p->prio = IDX_TO_PRIO(idx);
+	printk("Task pid: {%d} , timeslice : {%u}", p->pid, next->time_slice);
+	return next;
+}
+static struct task_struct *_pick_next_task_rsdl(struct rq *rq)
+{	
+	struct sched_rsdl_entity *rsdl_se;
+	struct rsdl_rq *rsdl_rq  = &rq->rsdl;
+
+	rsdl_se = pick_next_rsdl_entity(rsdl_rq);
+	//BUG_ON(!rsdl_se);
+	if(rsdl_se == NULL){
+		return NULL;
+	}
+	return rsdl_task_of(rsdl_se);
+}
+static struct task_struct *pick_next_task_rsdl(struct rq *rq)
 {
-	printk("task_tick_rsdl Task Called\n");
+	
+    struct task_struct *next;
+	//printk("Pick Next Task Initiated\n");
+	//printk("Total Tasks in rsdl_rq: {%u}", rq->rsdl.rsdl_nr_running);
+	
+	
+	if (!rsdl_rq_is_runnable(&(rq->rsdl))){
+		//printk("Ended Here!!! Oops");
+		return NULL;
+	}
+	// printk("Reached Here");
+	next = _pick_next_task_rsdl(rq);
+
+	if(next){
+		set_next_task_rsdl(rq,next,1);
+	}
+
+	// printk("Pick Next Task Completed\n");
+	return next;
 }
 
-static unsigned int get_rr_interval_rsdl(struct rq *rq, struct task_struct *task)
+
+/////////////////////////////////////////// Task Tick //////////////////////////////////////////////////////////////////////
+
+static inline void init_rq_quotas(struct rsdl_rq *rsdl_rq){
+	
+	for(int i=0; i<MAX_RSDL_PRIO; i++){
+		rsdl_rq->prio_quota[i] = MAX_RQ_QUOTA; 
+	}
+	printk("Runqueue Quota Reinitialized\n");
+}
+
+static inline void clear_queued_array(struct rsdl_rq *rsdl_rq){
+	struct rsdl_prio_array *array = rsdl_rq->expired;
+	struct list_head *queue = array->queue;
+	for(int i=0; i<MAX_RSDL_PRIO; i++){
+		if(!list_empty(queue + i)){
+			rsdl_rq->prio_queued[i] += 1;
+		}else{
+		rsdl_rq->prio_queued[i] = 0;
+		}
+	}
+	printk("Queued Array Reinitialized\n");
+}
+
+/*
+ * This is the heart of the virtual deadline priority management.
+ *
+ * We have used up the quota allocated to this priority level so we rotate
+ * the prio_level of the runqueue to the next lowest priority. We merge any
+ * remaining tasks at this level current_queue with the next priority and
+ * reset this level's queue. MAX_PRIO - 1 is a special case where we perform
+ * a major rotation.
+ */
+static inline void rotate_runqueue_priority(struct rsdl_rq *rsdl_rq)
 {
-	printk("get_rr_interval_rsdl Task Called\n");
-    return 0;
+	struct rsdl_prio_array *array = rsdl_rq->active;
+	struct list_head *queue = array->queue + rsdl_rq->curr_rq;
+	unsigned int static_p;
+	struct sched_rsdl_entity *first;
+	struct task_struct * p;
+	unsigned int new_prio_level = 0;
+	printk("Minor Rotation Called on : {%u}", rsdl_rq->curr_rq);
+	if (rsdl_rq->curr_rq > MAX_RSDL_PRIO - 2) {
+		/* Major rotation required */
+		struct rsdl_prio_array *new_array = rsdl_rq->expired;
+		struct list_head *expired_queue = new_array->queue;
+		printk("May have error during Major Rotation\n");
+		// Transfer all tasks to expired array
+		clear_queued_array(rsdl_rq);      //============================================<<<<<<<<<<<<<<<<<<
+		printk("Queued Array Cleared!!!!\n");
+		// Clear all queued variables
+
+		while(!list_empty(array->queue + rsdl_rq->curr_rq)){
+			printk("Step1\n");
+			first = list_first_entry(queue, struct sched_rsdl_entity, run_list);
+			printk("Step2\n");	
+			p = rsdl_task_of(first);
+			printk("Step3\n");
+			list_del_init(&(first->run_list));
+			printk("Step4\n");
+			static_p = PRIO_TO_IDX(p->static_prio);
+			printk("Step5\n");
+			first->time_slice = first->quota;    // Initializing timeslice of task as its quota = 5
+			printk("Step6\n");
+			list_add_tail(&(first->run_list), expired_queue  + static_p);
+			rsdl_rq->prio_queued[static_p] += 1;                                                        //------------------------------------
+			////Add here Rq_queued to +1
+			printk("Task enqueued in Expired array in Level {%u}\n", static_p);
+		}
+		printk("Out of while Loop\n");
+		init_rq_quotas(rsdl_rq);
+		printk("Quota Initialization Complete\n");
+		major_rotation(rsdl_rq);
+		printk("Major Rotation Complete\n");
+	} else {
+		/* Minor rotation */
+		new_prio_level = rsdl_rq->curr_rq + 1;
+		// __clear_bit(rsdl_rq->prio_level, rsdl_rq->dyn_bitmap);
+		while(!list_empty(array->queue + rsdl_rq->curr_rq)) {
+			first = list_first_entry(queue, struct sched_rsdl_entity, run_list);
+			p = rsdl_task_of(first);
+			printk("Step---++++ Task thrown in next : Pid {%d}\n", p->pid);
+			list_del_init(&(first->run_list));
+			rsdl_rq->prio_queued[rsdl_rq->curr_rq] -= 1;
+			printk("Task Deleted from Current Runqueue: {%u}, Pid: {%d}\n", rsdl_rq->curr_rq, p->pid);
+			//static_p = PRIO_TO_IDX(p->static_prio);
+			//printk("Step---++++ Task thrown in next\n");
+			first->time_slice += first->quota;    // Initializing timeslice of task as its quota = 5
+			//printk("Step---++++ Task thrown in next\n");
+			list_add_tail(&(first->run_list),queue + 1);
+			printk("Task Added to Runqueue: {%u}, Pid: {%d}\n", new_prio_level, p->pid);
+			rsdl_rq->prio_queued[new_prio_level] += 1;
+			// list_splice_tail_init(array->queue + rsdl_rq->curr_rq,
+			// 		 array->queue + new_prio_level);
+			// printk("List Splice Called to move this tasks to end of next list\n");
+			// __set_bit(new_prio_level, rsdl_rq->dyn_bitmap);
+		}
+	}
+	rsdl_rq->curr_rq = new_prio_level;
+	printk("Current Runqueue: {%d} after rotation", rsdl_rq->curr_rq);
 }
 
 static void
-prio_changed_rsdl(struct rq *rq, struct task_struct *p, int oldprio)
+requeue_rsdl_entity(struct rsdl_rq *rsdl_rq, struct sched_rsdl_entity *rsdl_se, int head, int shift)
+{		
+	if (on_rsdl_rq(rsdl_se)) {
+		struct rsdl_prio_array *array = rsdl_rq->active;
+		struct rsdl_prio_array *expired_array = rsdl_rq->expired;
+		
+		struct list_head *queue = array->queue + rsdl_rq->curr_rq;
+		struct list_head *expired_queue = expired_array->queue;
+
+		struct task_struct *p = rsdl_task_of(rsdl_se);
+		int stat_p = PRIO_TO_IDX(p->static_prio);
+
+
+		if((rsdl_rq->curr_rq == MAX_RSDL_PRIO - 1) && (shift == 1)){
+			// rotate_runqueue_priority(rsdl_rq);
+			// enqueue task to expired array
+
+			list_move_tail(&rsdl_se->run_list, expired_queue + stat_p);
+			rsdl_rq->prio_queued[rsdl_rq->curr_rq] -= 1;
+			rsdl_rq->prio_queued[rsdl_rq->curr_rq + stat_p] += 1;   ////// This created problem if task has priority 
+			p->prio = p->static_prio;								////// 39 and completes and gets added to same place 			
+			return;
+		}
+
+		if (head)
+			list_move(&rsdl_se->run_list, queue);
+		else{
+			list_move_tail(&rsdl_se->run_list, queue + shift);
+			// p->prio = m;
+		}
+		rsdl_rq->prio_queued[rsdl_rq->curr_rq] -= 1;
+		rsdl_rq->prio_queued[rsdl_rq->curr_rq + shift] += 1;
+	}
+	// Add in queued array
+	
+
+	printk("Requeue at {%d}, Pid : {%d}, TimeSlice: {%u}\n", rsdl_rq->curr_rq + shift,rsdl_task_of(rsdl_se)->pid, rsdl_se->time_slice);
+}
+
+static void requeue_task_rsdl(struct rsdl_rq *rsdl_rq, struct task_struct *p, int head, int shift)
 {
-    printk("prio_changed_rsdl Task Called\n");
+	struct sched_rsdl_entity *rsdl_se = &p->rsdl;
+	//PENTER;
+	head = 0;  // to always add in tail
+	requeue_rsdl_entity(rsdl_rq, rsdl_se, head, shift);
+	
 }
 
+static void task_expired_entitlement(struct rsdl_rq *rsdl_rq, struct task_struct *p)
+{	
+	set_tsk_need_resched(p);
+	p->rsdl.time_slice = sched_rsdl_timeslice;
+	requeue_task_rsdl(rsdl_rq, p, 0, 1);
+}
 
-static void switched_to_rsdl(struct rq *rq, struct task_struct *p)
+
+static void task_tick_rsdl(struct rq *rq, struct task_struct *p, int queued)
 {
-    printk("switched_to_rsdl Task Called\n");
+	struct sched_rsdl_entity *rsdl_se = &p->rsdl;
+	struct rsdl_rq *rsdl_rq  = &(rq_of_rsdl_se(rsdl_se))->rsdl;
+	update_curr_rsdl(rq);							//--------------------------> need to implement
+	// update_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);
+
+	
+	//printk("Sum_exec_runtime : {%llu}, exec_start: {%llu}\n", p->se.sum_exec_runtime, p->se.exec_start);
+	watchdog(rq, p);
+	// printk("Task: {%d}, Timeslice: {%u} at start of taskTick", p->pid, rsdl_se->time_slice);
+	if (p->policy != SCHED_RSDL)
+		return;
+	if (unlikely(!task_queued(p))) {
+		/* Task has expired but was not scheduled yet */
+		set_tsk_need_resched(p);
+		return;
+	}
+
+	if(rsdl_rq->curr_rq != p->prio){
+		//rsdl_se->time_slice += sched_rsdl_timeslice;
+		p->prio = IDX_TO_PRIO(rsdl_rq->curr_rq);
+		//printk("Boosted Timeslice by 5: Pid: {%d}, Timeslice{%u}\n",p->pid, rsdl_se->time_slice);
+	}
+
+	// Maintain current pointer here
+
+
+
+	//p->prio = IDX_TO_PRIO(rsdl_rq->curr_rq);
+	// // If p->rsdl.time_slice == 1, only one timetick is remaining
+	// if (--p->rsdl.time_slice)
+	// 	return;
+
+	spin_lock(&rsdl_rq->rsdl_runtime_lock);
+	/*
+	 * Accounting is performed by both the task and the runqueue. This
+	 * allows frequently sleeping tasks to get their proper quota of
+	 * cpu as the runqueue will have their quota still available at
+	 * the appropriate priority level. It also means frequently waking
+	 * tasks that might miss the scheduler_tick() will get forced down
+	 * priority regardless.
+	 */
+	printk("Task: {%d}, Timeslice: {%u} TickEnds, Queue in Use : {%u}, quota: {%ld}", p->pid, rsdl_se->time_slice,rsdl_rq->curr_rq, rsdl_rq->prio_quota[rsdl_rq->curr_rq]);
+
+	rsdl_rq->prio_quota[rsdl_rq->curr_rq] -= 1;
+	if (!--rsdl_se->time_slice)
+		task_expired_entitlement(rsdl_rq, p);
+	/*
+	 * The rq quota can become negative due to a task being queued in
+	 * scheduler without any quota left at that priority level. It is
+	 * cheaper to allow it to run till this scheduler tick and then
+	 * subtract it from the quota of the merged queues.
+	 */
+	if (rsdl_rq_quota(rsdl_rq, rsdl_rq->curr_rq) <= 0) {
+		printk("Runqueue quota Ended. {%d}\n", rsdl_rq->curr_rq);
+		rotate_runqueue_priority(rsdl_rq);
+		set_tsk_need_resched(p);
+	}
+	spin_unlock(&rsdl_rq->rsdl_runtime_lock);
+	// resched_curr(rq);
+	return;
+	
 }
 
+///////////////////////////////////////////Yield//////////////////////////////////////////////////////////////////////
 
-static void update_curr_rsdl(struct rq *rq)
+static void yield_task_rsdl(struct rq *rq)
+{	
+	struct rsdl_rq *rsdl_rq = &(rq->rsdl);
+	printk("Entered yield task rsdl\n");
+	
+	requeue_task_rsdl(rsdl_rq, rq->curr, 0, 0);
+}
+
+
+
+
+
+
+/// This Function corresponds to SMP and are not defined
+
+static unsigned int get_rr_interval_rsdl(struct rq *rq, struct task_struct *task)
 {
-    printk("update_curr_rsdl Task Called\n");
+	// printk("Entered get rr interval rsdl\n");
+    return 0;
 }
+static void
+prio_changed_rsdl(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	// printk("Entered prio changed rsdl\n");
 
+}	
+static void switched_to_rsdl(struct rq *rq, struct task_struct *p)
+{
+	// printk("Entered switched to rsdl\n");
+}
 static int balance_rsdl(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 {
-    printk("balance_rsdl Task Called\n");
+	// printk("Entered balance rsdl\n");
     return 0;
 }
-
 static struct task_struct *pick_task_rsdl(struct rq *rq)
 {   
 	struct task_struct *p = NULL;
-//     if (!sched_rsdl_runnable(rq))
-// 		return NULL;
-//     printk("pick_task_rsdl Task Called\n");
-//     return p;
-// 
+	// printk("Entered pick task rsdl\n");
 	return p;
 }
-
 static int
 select_task_rq_rsdl(struct task_struct *p, int cpu, int flags)
 {
-    printk("select_task_rq_rsdl Task Called\n");
+	// printk("Entered select task rq rsdl\n");
     return 0;
 }
-
 static void rq_online_rsdl(struct rq *rq)
 {
-    printk("rq_online_rsdl Task Called\n");
-
+	// printk("Entered rq online rsdl\n");	
 }
-
 static void rq_offline_rsdl(struct rq *rq)
 {
-    printk("rq_offline_rsdl Task Called\n");
-
+	// printk("Entered rq offline rsdl\n");   
 }
-
 static void task_woken_rsdl(struct rq *rq, struct task_struct *p)
 {
-    printk("task_woken_rsdl Task Called\n");
+	// printk("Entered task woken rsdl\n");	
 }
-
 static void switched_from_rsdl(struct rq *rq, struct task_struct *p)
 {
-    printk("switched_from_rsdl Task Called\n");
-
+	// printk("Entered switched from rsdl\n");
 }
+static struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq);
 
 
-static struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq);
 
 
+/////// RSDL Class Defination 
 DEFINE_SCHED_CLASS(rsdl) = {
-
 	.enqueue_task		= enqueue_task_rsdl,
 	.dequeue_task		= dequeue_task_rsdl,
 	.yield_task		= yield_task_rsdl,
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 054b6711e..28e238759 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -149,7 +149,6 @@ void init_rt_rq(struct rt_rq *rt_rq)
 #endif /* CONFIG_SMP */
 	/* We start is dequeued state, because no RT tasks are queued */
 	rt_rq->rt_queued = 0;
-
 	rt_rq->rt_time = 0;
 	rt_rq->rt_throttled = 0;
 	rt_rq->rt_runtime = 0;
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2c70ddf62..6386b5d95 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -291,10 +291,16 @@ struct rt_prio_array {
 /*
  * This is the priority-queue data structure of the RSDL scheduling class:
  */
+
+// struct RSDLQueue{
+// 	int quota;
+// 	struct list_head head;		
+// };
 struct rsdl_prio_array {
 	DECLARE_BITMAP(bitmap,
 		       MAX_RSDL_PRIO + 1); /* include 1 bit for delimiter */
 	struct list_head queue[MAX_RSDL_PRIO];
+	// struct RSDLQueue queue[MAX_RSDL_PRIO];
 };
 
 struct rt_bandwidth {
@@ -747,10 +753,23 @@ static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
 
 /* RSDL classes' related field in a runqueue: */
 struct rsdl_rq {
-	struct rsdl_prio_array active;
-	struct rsdl_prio_array expired;
-
+	struct rsdl_prio_array activeA;
+	struct rsdl_prio_array expiredA;
+	// Maintain Pointer to track active and expired 
+	struct rsdl_prio_array *active;
+	struct rsdl_prio_array *expired;
+	
+	struct task_struct * curr;
 	unsigned int rsdl_nr_running;
+	unsigned int curr_rq;
+	spinlock_t	rsdl_runtime_lock;
+	
+	
+	// unsigned long prio_queued[MAX_PRIO];
+	/* The number of tasks at each static priority */
+
+	long prio_quota[MAX_RSDL_PRIO];   // To store quota of each runqueue
+	long prio_queued[MAX_RSDL_PRIO];  // To store no of tasks currently queued in each runqueue
 };
 
 static inline bool rsdl_rq_is_runnable(struct rsdl_rq *rsdl_rq)
-- 
2.34.1

